[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Topics In Statistics - Smart Data",
    "section": "",
    "text": "This refresher course is intended to help you brush up on essential linear algebra tools and some key statistical concepts.\nCourse Overview\nThis course is specifically tailored for those who are stepping into data science, it is intended to present or refresh two key tools in Data Science : Principal Component Analysis (PCA) and Regression analysis. These two mathematical conepts in Data science are built on basic Linear algebra ideas. While not designed as a math course, this course will provide crucial reminders of linear algebra indispensable for statistics and how they are used in PCA and Regression. We will also have labs to use this method on practical example.\nWhat You Will Learn\n\nLinear Surival Kit:\n\nVectors and matrices: Definitions, operations, and properties\nSystems of linear equations\nMatrix decomposition and eigenvalues\n\nPrincipal Component Analysis (PCA):\n\nUnderstanding the concept and purpose of PCA\nSteps involved in performing PCA\nApplications of PCA in data reduction and visualization\n\nLinear Regression:\n\nIntroduction to regression analysis\nBuilding and interpreting multiple regression models\nEvaluating model performance\n\n\nCourse Format\nThis course is conducted in English and combines lectures, practical examples, and hands-on exercises to reinforce learning. Whether you’re a beginner or someone looking to refresh these topics, the structured approach and close link made between the linear algebra concept and their use in statistics will help you grasp and apply these concepts effectively.\nHow to use this website\nYou can find the lecture notes on the top menu on the right, as well as the Rmarkdown and/or Jupyter notebook used during the labs. The website will be updated during the course.\nPlease, if you have any remarks regarding the material on this website (typos, lack of clarity or anything else) please contact me.\nEnjoy!"
  },
  {
    "objectID": "pca.html#the-example-of-doubs-river-characteristics",
    "href": "pca.html#the-example-of-doubs-river-characteristics",
    "title": "Principal Component Analysis",
    "section": "The Example of Doubs River Characteristics",
    "text": "The Example of Doubs River Characteristics\nWe measured the physico-chemical characteristics at 30 different sites along the Doubs River .\n\n\nThe first 4 rows (out of 30) of the doubs.env dataset\n\n\n# A tibble: 30 × 11\n    das   alt   pen   deb    pH   dur   pho   nit   amm   oxy   dbo\n  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0.3   934  48    0.84   7.9    45  0.01  0.2   0     12.2   2.7\n2   2.2   932   3    1      8      40  0.02  0.2   0.1   10.3   1.9\n3  10.2   914   3.7  1.8    8.3    52  0.05  0.22  0.05  10.5   3.5\n4  18.5   854   3.2  2.53   8      72  0.1   0.21  0     11     1.3\n# ℹ 26 more rows\n\n\nHow can we best visualize these data to reveal the relationships between variables and identify similarities between sites?\n\n\ndas: distance to the source (\\(km\\)),\nalt: altitude (\\(m\\)),\npen: slope (elevation change per 1000m),\ndeb: flow rate (\\(m^3.s^{-1}\\)),\npH: water pH,\ndur: calcium concentration (\\(mg.L^{-1}\\)),\npho: phosphate concentration (\\(mg.L^{-1}\\)),\nnit: nitrate concentration (\\(mg.L^{-1}\\)),\namn: ammonium concentration (\\(mg.L^{-1}\\)),\noxy: dissolved oxygen concentration (\\(mg.L^{-1}\\)),\ndbo: Biological Oxygen Demand (\\(mg.L^{-1}\\))."
  },
  {
    "objectID": "pca.html#the-example-of-penguin-morphology",
    "href": "pca.html#the-example-of-penguin-morphology",
    "title": "Principal Component Analysis",
    "section": "The Example of Penguin Morphology",
    "text": "The Example of Penguin Morphology\nWe measured the morphological characteristics of various penguins:\n\n\nThe first 6 rows (out of 333) of the penguins dataset\n\n\n# A tibble: 333 × 4\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1           39.1          18.7               181        3750\n2           39.5          17.4               186        3800\n3           40.3          18                 195        3250\n4           36.7          19.3               193        3450\n5           39.3          20.6               190        3650\n6           38.9          17.8               181        3625\n# ℹ 327 more rows\n\n\nHow can we best visualize these data to reveal the relationships between variables and identify similarities between individuals?\n\n\nbill_length_mm: bill length,\nbill_depth_mm: bill depth,\nflipper_length_mm: flipper length,\nbody_mass_g: body mass."
  },
  {
    "objectID": "pca.html#formalization",
    "href": "pca.html#formalization",
    "title": "Principal Component Analysis",
    "section": "Formalization",
    "text": "Formalization\n\nFor each individual \\(i\\), we measured \\(p\\) different variables.\nFor each variable \\(k\\), we measured \\(n\\) individuals.\n\nThe data are arranged in a table with \\(n\\) rows and \\(p\\) columns.\n\n\n\n\n\n\n\n\n\n\n\n\nWe denote \\(x_{ik}\\) as the value measured for variable \\(k\\) on individual \\(i\\),\nand\n\n\\(x_{\\bullet k} = \\frac{1}{n} \\sum_{i=1}^n x_{ik}\\) as the mean value of variable \\(k\\),\n\\(s_k  = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ik}-x_{\\bullet k})^2}\\) as the standard deviation of variable \\(k\\)."
  },
  {
    "objectID": "pca.html#the-same-question-in-various-fields",
    "href": "pca.html#the-same-question-in-various-fields",
    "title": "Principal Component Analysis",
    "section": "The Same Question in Various Fields",
    "text": "The Same Question in Various Fields\n\nSensory analysis: score of descriptor \\(k\\) for product \\(i\\)\nEconomics: value of indicator \\(k\\) for year \\(i\\)\nGenomics: gene expression \\(k\\) for patient/sample \\(i\\)\nMarketing: satisfaction index \\(k\\) for brand \\(i\\)\n\netc…\nWe have \\(p\\) variables measured on \\(n\\) individuals, and we want to visualize these data to understand the relationships between variables and the proximity between individuals."
  },
  {
    "objectID": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table",
    "href": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table",
    "title": "Principal Component Analysis",
    "section": "Seeing is Understanding: How to Represent the Information Contained in This Table?",
    "text": "Seeing is Understanding: How to Represent the Information Contained in This Table?\nIdea 1: Represent the relationships between variables 2 by 2"
  },
  {
    "objectID": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table-1",
    "href": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table-1",
    "title": "Principal Component Analysis",
    "section": "Seeing is Understanding: How to Represent the Information Contained in This Table?",
    "text": "Seeing is Understanding: How to Represent the Information Contained in This Table?\nIdea 1: Represent the relationships between variables 2 by 2\n\n\n\n\n\n\n\n\n\nWe lose information on the other axes."
  },
  {
    "objectID": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table-2",
    "href": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table-2",
    "title": "Principal Component Analysis",
    "section": "Seeing is Understanding: How to Represent the Information Contained in This Table?",
    "text": "Seeing is Understanding: How to Represent the Information Contained in This Table?\nIdea 1: Represent the relationships between variables 2 by 2"
  },
  {
    "objectID": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table-3",
    "href": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table-3",
    "title": "Principal Component Analysis",
    "section": "Seeing is Understanding: How to Represent the Information Contained in This Table?",
    "text": "Seeing is Understanding: How to Represent the Information Contained in This Table?\nObjective:\n\nRepresent without losing too much information\nIdeally, individuals far apart in the initial cloud remain far apart in the representation.\n\nWhat We Need:\n\nQuantify the information loss in the representation\nBuild the representation that loses the least amount of information\n\nPrecautions:\n\nPotentially, Make variables expressed in different units comparable"
  },
  {
    "objectID": "pca.html#distance-between-individuals",
    "href": "pca.html#distance-between-individuals",
    "title": "Principal Component Analysis",
    "section": "Distance between individuals",
    "text": "Distance between individuals\nLet \\(X_{i,.}^\\intercal \\in \\R^d\\) be the descriptions of individual \\(i\\). To quantify the distance between indivuals we might used the Euclidian distance in \\(\\R^d,\\)\n\\[d(i, j)^2 = \\sum_{k=1}^d (x_{ik} - x_{jk})^2 = \\norm{X_{i,.}^\\intercal - X_{i,.}^\\intercal}^2 = \\left({X_{i,.}^\\intercal - X_{i,.}^\\intercal}\\right)^\\intercal  \\left({X_{i,.}^\\intercal - X_{i,.}^\\intercal}\\right). \\]\nThis could be misleading when dealing with different variables with different scale.\nExample in the penguins dataset\nBody mass varies form 2700 to 6300, while flipper length varies from 172 to 231.\nVariable Body mass will contribute more to the distance between individuals."
  },
  {
    "objectID": "pca.html#specify-a-different-metric",
    "href": "pca.html#specify-a-different-metric",
    "title": "Principal Component Analysis",
    "section": "Specify a different metric",
    "text": "Specify a different metric\n\n\n\nDefinition\n\n\nA metric in \\(\\R^d\\), \\(M\\) is a definite positive symmetric \\(\\R^{d\\times d}\\) matrix which can be used to define distance between individuals\n\\[d_M(i, j)^2  = \\norm{X_{i,.}^\\intercal - X_{i,.}^\\intercal}^2_M = \\left({X_{i,.}^\\intercal - X_{i,.}^\\intercal}\\right)^\\intercal  M \\left({X_{i,.}^\\intercal - X_{i,.}^\\intercal}\\right).\\]\n\n\n\n\n\n\nIf \\(M=I_d\\),\n\n\\(d_M\\) is the classical Euclidian distance\n\nGet rid of the units used for the variables and attribute the same weight to all variables by choosing\n\n\\(M = \\begin{pmatrix}\n\\frac{1}{s_1^2} & &0 & \\cr\n& \\frac{1}{s_2^2}  & & \\cr\n&0 & \\ddots & \\cr\n&&& \\frac{1}{s_d^2} \\cr\n\\end{pmatrix} = D_{1/s^2}\\)\n\nwith \\(s_k^2=\\sum_{i=1}^n (x_{ik} - x_{.k})^2,\\) \\(x_{.k} =\\frac{1}{n}\\sum_{i=1}^n x_{ik}\\)\n\n\n\nRemarks\n\n\nThe distance defined with \\(D_{1/s^2}\\) is the same than the distance defined on centred and scaled variables with the identity matrix.\n\n\n\nIn the following, we will assume that \\(X\\) is the matrix of centred and scaled variables."
  },
  {
    "objectID": "pca.html#dispersion-measure-inertia-with-respect-to-a-point",
    "href": "pca.html#dispersion-measure-inertia-with-respect-to-a-point",
    "title": "Principal Component Analysis",
    "section": "Dispersion measure: Inertia with respect to a point",
    "text": "Dispersion measure: Inertia with respect to a point\n\n\n\nDefinition\n\n\nInertia (denomination derived from moments of inertia in Physics) with respect to a point \\(a \\in R^{d},\\) according to metric \\(M\\): \\[I_a = \\sum_{i=1}^n \\norm{X_{i,.}^\\intercal - a}_M^2\\]\n\n\n\n\nInertia around the centroïd \\(G\\) (center of gravity of the cloud) plays a central role in Principal Component Analysis:\n\n\\(G = (x_{.,1}, \\cdots, x_{.,d})^\\intercal\\) and \\(I_G = min_{a\\in \\R^d} I_a\\)\nRemarks\nAssume, we deal with centred sacled variables:\n\\(I_G = \\sum_{i=1}^n \\norm{X_{i,.}^\\intercal - G}^2 = \\sum_{i=1}^n \\class{rouge}{\\sum_{k=1}^d} (x_{ik} - x_{.,k})^2 = \\class{rouge}{\\sum_{k=1}^d} \\sum_{i=1}^n  (x_{ik} - x_{.,k})^2.\\)\nAs the variables are scaled, \\(\\sum_{i=1}^n  (x_{ik} - x_{.,k})^2 = n s^2_k = n.\\) and \\(I_G=nd.\\)\nTotal inertia with scaled centred variables is \\(nd\\)"
  },
  {
    "objectID": "pca.html#dispersion-measure-inertia-with-respect-to-a-affine-subspace",
    "href": "pca.html#dispersion-measure-inertia-with-respect-to-a-affine-subspace",
    "title": "Principal Component Analysis",
    "section": "Dispersion measure: Inertia with respect to a affine subspace",
    "text": "Dispersion measure: Inertia with respect to a affine subspace\n\n\n\nDefinition\n\n\nInertia with respect to an affine subspace \\(S\\) according to metric \\(M\\): \\(I_S = \\sum_{i=1}^n d_M(X_{i,.}, S)^2\\)\n\n\n\nHuygens theorem states that if \\(S^G\\) stands for the affine subspace containing \\(G\\) and parallel to \\(S\\) then \\[I_S = I_{S^G} + d_M^2(a, G),\\] where \\(a\\) is the orthogonal projection of \\(G\\) on \\(S\\).\nThe affine subspace \\(S\\) which minimizes inertia is \\(S^G\\)."
  },
  {
    "objectID": "pca.html#inertia-decomposition",
    "href": "pca.html#inertia-decomposition",
    "title": "Principal Component Analysis",
    "section": "Inertia Decomposition",
    "text": "Inertia Decomposition\nSince, variables are centred \\(G=\\mathbb{0}\\), \\(I=\\sum_{i=1}^n d(X_{i,.},0)^2.\\)\nLet \\(S\\) be an affine subspace and \\(U=S^\\intercal,\\) \\(X^S_{i,.}\\) (recip. \\(X^S_{i,.})\\)) the orthogonal projection on \\(S\\) (recip. on \\(U\\)).\nAs \\(d(X_{i,.},0)^2 = d(X^S_{i,.},0)^2 + d(X^U_{i,.},0)^2\\), \\(I=I_S + I_{S^\\intercal}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\\(I_S\\) is the dispersion of the dataset lost by projection on \\(S\\),\nwhile \\(I_{S^\\intercal}\\) is the dispersion of the dataset projected on \\(S\\).\nPCA\nIdentifying \\((U_1, U_d)\\) a sequence of orthogonal unitary vectors such that \\(I_{U_1}\\leq I_{U_2}\\leq \\cdots \\leq I_{U_d}\\).\nThe projection on \\(U_1\\) is the best projection of the dataset in one dimension, \\(U_1\\) define the first Principal Component."
  },
  {
    "objectID": "pca.html#inertia-useful-representation",
    "href": "pca.html#inertia-useful-representation",
    "title": "Principal Component Analysis",
    "section": "Inertia: useful representation",
    "text": "Inertia: useful representation\nLet \\(x_i= X_{i,.}^\\intercal\\), (recall that \\(X_{i,}\\) is a row vector and \\(x_i\\) is the corresponding column vector).\n\\[\\begin{align}\nI & = \\sum_{i}^n d^2(X_{i,.}, 0) = \\sum_{i}^n \\norm{X_{i,.}}\\\\\n& = \\sum_{i}^n x_i^\\intercal x_i \\cr\n& = tr(\\sum_{i}^n x_i^\\intercal x_i) \\cr\n& = tr(\\sum_{i}^n x_i x_i^\\intercal ) \\cr\n& = tr(X^\\intercal  X) \\cr\n\\end{align}\\]\nRemarks\n\n\\(X^\\intercal  X\\) is the covariance matrix of the \\(d\\) variables,\n\\(X^\\intercal  X\\) is a symmetric \\(\\R^{d\\times d}\\) matrix, and the corresponding SVD\n\n\\[X^\\intercal  X = \\left ( P D Q^\\intercal \\right )^\\intercal \\left ( P D Q^\\intercal \\right ) =  Q^\\intercal D  P^\\intercal  P D Q^\\intercal = Q D^2 Q^\\intercal.\\]\n\n\\(I= tr(X^\\intercal X) = tr(Q D^2 Q^\\intercal) = tr(P^\\intercal P D ) = tr( D ) = \\sum_{k=1}^d \\sigma^2_k,\\)\nwhere \\(\\sigma^2_k\\) stands for the k\\(^{th}\\) eigen value."
  },
  {
    "objectID": "pca.html#identifying-u_1",
    "href": "pca.html#identifying-u_1",
    "title": "Principal Component Analysis",
    "section": "Identifying \\(U_1\\)",
    "text": "Identifying \\(U_1\\)\nConsider \\(I_{\\Delta_{U}}\\) the inertia with respect to \\(\\Delta_{U}\\) affine subspace containing \\(G\\) with directed bu the unitary vector \\(U\\).\n\\(I_{\\Delta_{U}}\\) is the cost of the projection on \\(\\Delta_{U}\\), i.e the loss information.\nMinimizing \\(I_{\\Delta_{U}}\\) equals maximizing \\(I_{\\Delta_{U^T}}\\), i.e the dispersion of the projected set of data points.\nProjection on U\n\\(d(X_{i,}, U^\\intercal)^2= \\left ( x_i \\dot U^\\intercal \\right )^2 = \\left ( X_i^\\intercal U \\right )^2 = cos(\\theta)^2 \\norm{x_i}^2 ,\\)\nSo that \\[\\begin{align}\nI_{\\Delta_{U^\\intercal}} & = \\sum_{i=1}^n  d(X_{i,}, U^\\intercal)^2 \\cr\n                         & = \\norm{X U} \\cr\n                         & = U^\\intercal X^\\intercal X  U\\cr\n                         & = U^\\intercal Q D^2  Q^\\intercal   U\\cr\n\\end{align}\\]\nBut \\(U^\\intercal Q = \\begin{pmatrix} U^\\intercal Q_{.,1}, \\cdots, U^\\intercal Q_{.,d})\\) Is the coordinate of the unitary vector \\(U\\) on the basis defined by the eigen vector of \\(X\\): \\(\\omega_1 q_1 +  = \\omega_d q_d,\\) such that \\(\\sum_k \\omega_k^2=1\\)\n\\(I_{\\Delta_{U^\\intercal}} = \\sum_{k=1}^d \\omega_k \\sigma^2_k.\\)\nMaximizing \\(I_{\\Delta_{U^\\intercal}}\\), as \\(\\sigma^2_k\\) are in decreasing order, consists in choosing \\(\\omega_1=1\\)"
  },
  {
    "objectID": "linearalgebra4DS.html#references-on-linear-algebra-for-data-scientist.",
    "href": "linearalgebra4DS.html#references-on-linear-algebra-for-data-scientist.",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "References on Linear Algebra for Data Scientist.",
    "text": "References on Linear Algebra for Data Scientist.\n\n\nFor a general overview\n [Gen07]\n [Har98]\n\nWith R or Python applications\n [Yos21]\n [Coh22]\nMatrix computations reference book\n [PP08]"
  },
  {
    "objectID": "linearalgebra4DS.html#linear-algebra-what-and-why",
    "href": "linearalgebra4DS.html#linear-algebra-what-and-why",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Linear Algebra : What and why",
    "text": "Linear Algebra : What and why\n\n\n\nDefinition\n\n\nLinear algebra is the branch of mathematics concerning linear equations such as \\(a_1 x_1 + \\cdots  a_n x_n = b,\\) and linear maps such as \\((x_{1},\\ldots ,x_{n})\\mapsto a_{1}x_{1}+\\cdots +a_{n}x_{n},\\) and their representations in vector spaces and through matrices.\n\n\n\nLink between Data science and Linear Algebra\nData are organized in rectangular array, understood as matrices, and the mathematical concept associated with matrices are used to manipulate data."
  },
  {
    "objectID": "linearalgebra4DS.html#palmer-penguins-illustration",
    "href": "linearalgebra4DS.html#palmer-penguins-illustration",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Palmer penguins illustration",
    "text": "Palmer penguins illustration\n\n## install.packages('palmerpenguins\")\nlibrary(\"palmerpenguins\")\npenguins_nona &lt;- na.omit(penguins) ## remove_na for now\npenguins_nona |&gt; print(n=3)\n\n# A tibble: 333 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 330 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nOne row describes one specific penguin,\nOne column corresponds to one specific attribute of the penguin.\n\nFocusing on quantitative variables, we get rectangular array with n=333 rows, and d=5 columns.\nEach penguin might be seen as a vector in \\(\\mathbb{R}^d\\) and each variable might be seen as a vector in \\(\\mathbb{R}^n\\). The whole data set is a matrix of dimension \\((n,d)\\)."
  },
  {
    "objectID": "linearalgebra4DS.html#linear-algebra-what-and-why-1",
    "href": "linearalgebra4DS.html#linear-algebra-what-and-why-1",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Linear Algebra : What and why",
    "text": "Linear Algebra : What and why\nRegression Analysis\n\nto predict one variable, the response variable named \\(Y\\), given the others, the regressors\nor equivalently to identify a linear combination of the regressors which provide a good approximation of the responsbe variable,\nor equivalently to specify parameters \\(\\theta,\\) so that the prediction \\(\\hat{Y} = X \\theta,\\) linear combination of the regressors which is as close as possible from \\(Y\\).\n\nWe are dealing with linear equations, enter the world of Linear Algebra.\nPrincipal Component Analysis\n\nto explore relationship between variables,\nor equivalently to quantify proximity between vectors,\nbut also to small set set of new variables (vectors) which represent most of the initial variables,\n\nWe are dealing with vectors, vector basis, enter the world of Linear Algebra."
  },
  {
    "objectID": "linearalgebra4DS.html#vectors",
    "href": "linearalgebra4DS.html#vectors",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Vectors",
    "text": "Vectors\n\\(x\\in \\R^n\\) is a vector of \\(n\\) components.\nBy convention, \\(x\\) is a column vector \\[x=\\begin{pmatrix} x_1 \\cr \\vdots\\cr x_n \\end{pmatrix}.\\]\nWhen a row vector is needed we use operator \\(\\ ^\\intercal,\\)\n\\[x^\\intercal = \\begin{pmatrix} x_1, \\cdots,  x_n \\end{pmatrix}.\\] The null vector \\(\\mathbb{0}\\in\\R^d\\),\n\\[\\mathbb{0}^\\intercal = \\begin{pmatrix} 0, \\cdots 0\\end{pmatrix}.\\]"
  },
  {
    "objectID": "linearalgebra4DS.html#matrix",
    "href": "linearalgebra4DS.html#matrix",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Matrix",
    "text": "Matrix\nA matrix \\(A\\in \\R^{n\\times d}\\) has \\(n\\) rows and \\(d\\) columns:\n\\[A = \\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1d} \\cr\na_{21} & a_{22} & \\cdots & a_{2d} \\cr\n\\vdots & \\vdots &        & \\vdots \\cr\na_{n1} & a_{n2} & \\cdots & a_{nd}\n\\end{pmatrix}\\]\nThe term on row \\(i\\) and column \\(j\\) is refered to as \\(a_{ij}\\),\n\n\nThe column \\(j\\) is refer to as \\(A_{.,j}\\), \\(A_{.,j} = \\begin{pmatrix} a_{1j} \\cr \\vdots\\cr a_{nj} \\end{pmatrix}.\\)\n\nTo respect the column vector convention, the row \\(i\\) is refer to as \\(A_{i,.}^\\intercal\\), \\(A_{i,.}^\\intercal = \\begin{pmatrix} a_{i1}, & \\cdots &  , a_{id} \\end{pmatrix}\\)"
  },
  {
    "objectID": "linearalgebra4DS.html#simple-operations",
    "href": "linearalgebra4DS.html#simple-operations",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Simple operations",
    "text": "Simple operations\n\n\nMultiplication by a scalar\nlet’s \\(\\lambda\\in\\R\\),\n\\[\\lambda x = \\begin{pmatrix} \\lambda x_1 \\cr \\vdots\\cr \\lambda x_n \\end{pmatrix}.\\]\n\\[\\lambda A = \\begin{pmatrix}\n\\lambda  a_{11} & \\lambda a_{12} & \\cdots & \\lambda  a_{1d} \\cr\n\\lambda  a_{21} & \\lambda  a_{22} & \\cdots & \\lambda a_{2d} \\cr\n\\vdots & \\vdots &        & \\vdots \\cr\n\\lambda a_{n1} & \\lambda a_{n2} & \\cdots & \\lambda a_{nd}\n\\end{pmatrix}\\]\n\nSum\nIf \\(x\\in \\R^n\\) and \\(y\\in R^n\\), then \\(x+y\\) exists\n\\[x+ y =  \\begin{pmatrix} x_1 + y_1 \\cr \\vdots\\cr x_n + y_n\\end{pmatrix}.\\]\nIf \\(A\\) and \\(B\\) are two matrices with the same dimension, \\(A+B\\) exists\n\\[\nA+B = \\begin{pmatrix}\na_{11} + b_{11} &  \\cdots &  a_{1d} + b_{1d}\\cr\n\\vdots &  &         \\vdots \\cr\na_{n1} + b_{n1}& \\cdots & a_{nd} + b_{nd}\n\\end{pmatrix}.\n\\]"
  },
  {
    "objectID": "linearalgebra4DS.html#visualization",
    "href": "linearalgebra4DS.html#visualization",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Visualization",
    "text": "Visualization\nConsider \\(x=\\begin{pmatrix} 1 \\cr 0 \\end{pmatrix}\\) and \\(y= \\begin{pmatrix} 0 \\cr 1 \\end{pmatrix},\\) and \\(z=x+y.\\)"
  },
  {
    "objectID": "linearalgebra4DS.html#matrix-product",
    "href": "linearalgebra4DS.html#matrix-product",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Matrix product",
    "text": "Matrix product\n\nDefinitionSolutionRPython\n\n\nLet \\(A\\in \\R^{n \\times p}\\) and \\(B\\in\\R^{p \\times d}\\), then the product \\(AB\\) is well defined, it is a matrix \\(C\\) in $^{n,d}, so that\n\\[C_{ij} = \\sum_{k=1}^p A_{ik} B_{kj}, \\quad 1\\leq i\\leq n, 1\\leq j \\leq d.\\]\nExample\n\n\\(A\\in \\R^{3 \\times 2}\\), \\(B\\in\\R^{2 \\times 4}\\), with \\(A = \\begin{pmatrix}\n\\color{color\n}{1} & 2 \\cr\n1 & 3 \\cr\n-1 & -1 \\end{pmatrix}\\) and \\(B =\\begin{pmatrix}\n{-1} & {-2} & {0} & {1}\\cr\n{-2} & {1} & {1} & {0}\\end{pmatrix},\\)\n\n\n\n\\(A B\\) exists as the number of colums in \\(A\\) equals the number of rows in \\(B\\).\nA useful presentation: \\[\\begin{matrix}\n& & \\begin{pmatrix}\n\\class{bleuf}{-1} & \\class{bleu}{-2} & \\class{vert}{0} & \\class{clair}{1}\\cr\n\\class{bleuf}{-2} & \\class{bleu}{1} & \\class{vert}{1} & \\class{clair}{0}\\end{pmatrix}   \\cr\n& \\begin{pmatrix}\n\\class{jaune}{1} & \\class{jaune}{2} \\cr\n\\class{orange}{1} & \\class{orange}{3} \\cr\n\\class{rouge}{-1} & \\class{rouge}{-1} \\end{pmatrix} &\n\\begin{pmatrix}\n\\class{jaune}{\\bullet} \\class{bleuf}{\\bullet}   & \\class{jaune}{\\bullet} \\class{bleu}{\\bullet}  & \\class{jaune}{\\bullet} \\class{vert}{\\bullet} & \\class{jaune}{\\bullet} \\class{clair}{\\bullet} \\cr\n\\class{orange}{\\bullet} \\class{bleuf}{\\bullet}   & \\class{orange}{\\bullet} \\class{bleu}{\\bullet}  & \\class{orange}{\\bullet} \\class{vert}{\\bullet} & \\class{orange}{\\bullet} \\class{clair}{\\bullet} \\cr\n\\class{rouge}{\\bullet} \\class{bleuf}{\\bullet}   & \\class{rouge}{\\bullet} \\class{bleu}{\\bullet}  & \\class{rouge}{\\bullet} \\class{vert}{\\bullet} & \\class{rouge}{\\bullet} \\class{clair}{\\bullet} \\cr\n\\end{pmatrix},\n\\end{matrix}\n\\] * \\[AB =   \\begin{pmatrix}\n\\class{jaune}{1}\\times\\class{bleuf}{(-1)} +\\class{jaune}{2}\\times \\class{bleuf}{(-2)}    & \\class{jaune}{1}\\times \\class{bleu}{(-2)} + \\class{jaune}{2} \\times\\class{bleu}{1}  & \\class{jaune}{1}\\times \\class{vert}{0} + \\class{jaune}{2}\\times \\class{vert}{1}  & \\class{jaune}{1} \\times\\class{clair}{1} + \\class{jaune}{2}\\times \\class{clair}{0} \\cr\n\\class{orange}{1} \\times \\class{bleuf}{(-1)}  +  \\class{orange}{3} \\times \\class{bleuf}{(-2)}   & \\class{orange}{1} \\times  \\class{bleu}{(-2)} +  \\class{orange}{3} \\times  \\class{bleu}{1} & \\class{orange}{1} \\times \\class{vert}{0} + \\class{orange}{3} \\times \\class{vert}{1}  & \\class{orange}{1} \\times\\class{clair}{1} + \\class{orange}{3}\\times \\class{clair}{0}  \\cr\n\\class{rouge}{(-1)} \\times \\class{bleuf}{(-1)}  +  \\class{rouge}{(-1)} \\times \\class{bleuf}{(-2)}   & \\class{rouge}{(-1)} \\times  \\class{bleu}{(-2)} +  \\class{rouge}{(-1)} \\times  \\class{bleu}{1} & \\class{rouge}{(-1)} \\times \\class{vert}{0} + \\class{rouge}{(-1)} \\times \\class{vert}{1}  & \\class{rouge}{(-1)} \\times\\class{clair}{1} + \\class{rouge}{(-1)}\\times \\class{clair}{0}  \\cr\n\\end{pmatrix},\\]\n\\[AB = \\begin{pmatrix}\n-5   & 0  & 2  & 1 \\cr\n-7   & 1 & 3 & 1  \\cr\n3  & 1 &-1  & -1  \\cr\n\\end{pmatrix}. \\]\n\n\nA matrix is defined column by column\n\nA = matrix(c(1,1,-1,2,3,-1), ncol = 2)\nB = matrix(c(-1,-2, -2,1, 0, 1, 1, 0), ncol = 4)\n\nThe default operation \\(A*B\\) is not the matrix product but a product elment by element, the matrix product is obtained by\n\nA %*% B\n\n     [,1] [,2] [,3] [,4]\n[1,]   -5    0    2    1\n[2,]   -7    1    3    1\n[3,]    3    1   -1   -1\n\n\n\n\nMatrix product is available through the numpy library\n\nimport numpy as np\n\nA = np.array([[ 1, 2], [ 1, 3], [ -1, 1]]); A.view()\n\narray([[ 1,  2],\n       [ 1,  3],\n       [-1,  1]])\n\nB = np.array([[-1, -2, 0, 1], [ -2, 1, 1, 0]]); B.view()\n\narray([[-1, -2,  0,  1],\n       [-2,  1,  1,  0]])\n\nC = A.dot(B);  C.view()\n\narray([[-5,  0,  2,  1],\n       [-7,  1,  3,  1],\n       [-1,  3,  1, -1]])"
  },
  {
    "objectID": "linearalgebra4DS.html#matrix-vector-product",
    "href": "linearalgebra4DS.html#matrix-vector-product",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Matrix vector product",
    "text": "Matrix vector product\nAs a vector \\(x\\) in \\(\\R^n\\) is also a matrix in \\(\\R^{n\\times 1}\\), The product \\(A x\\) exist is the number of columns in \\(_A\\) equals the number of row for \\(x\\).\n\nExampleExercises\n\n\nA =\n\\[\\begin{pmatrix}\n1 & -1 \\cr\n1 & 0 \\cr\n-1 & 0.5\n\\end{pmatrix}\\]\n,\nLet \\(x\\in \\R^2\\), \\(Ax\\) exists\n\\[Ax = \\begin{pmatrix}\nx_1 - x_2 \\cr\nx_1 \\cr\n-x_1 + 0.5 x\n\\end{pmatrix},\\]\n\n\nLet \\(x\\in\\R^2\\), \\(y\\in\\R^3\\) and \\[A = \\begin{pmatrix}\n1 & -1 & 0 \\cr\n1 & 0 & -0.5 \\cr\n-1 & 0.5 & 2\n\\end{pmatrix}, B = \\begin{pmatrix}\n1 & 1 & 0 \\cr\n-1 & 0 & -0.5\n\\end{pmatrix},\\quad  C = \\begin{pmatrix}\n1 & -1  \\cr\n-1 & 2  \\cr\n1 & -0.5\n\\end{pmatrix},\\]\nWhen possible, compute the following quantites\n\\[ Ax; \\quad Ay; \\quad AB; \\quad BA; \\quad AC; \\quad CA; \\quad Bx; \\quad Cx. \\]"
  },
  {
    "objectID": "linearalgebra4DS.html#transposition",
    "href": "linearalgebra4DS.html#transposition",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Transposition",
    "text": "Transposition\nThe transpose, denoted by \\(\\ ^\\intercal\\), of a matrix results from switching the rows and columns. Let \\(A \\in \\R^{n\\times d}\\), \\[A^{\\intercal} \\in \\R^{d\\times n},\\  and (A^{\\intercal})_{ij}= A_{ji}.\\]\n\\[\nA =  \\begin{pmatrix}\n\\class{jaune}{\\bullet} & \\class{jaune}{\\bullet} \\cr\n\\class{orange}{\\bullet} & \\class{orange}{\\bullet} \\cr\n\\class{rouge}{\\bullet} & \\class{rouge}{\\bullet} \\cr\n\\end{pmatrix}, \\quad A^{\\intercal} = \\begin{pmatrix}\n\\class{jaune}{\\bullet} & \\class{orange}{\\bullet} & \\class{rouge}{\\bullet}\\cr\n\\class{jaune}{\\bullet} & \\class{orange}{\\bullet} & \\class{rouge}{\\bullet}\n\\end{pmatrix}\n\\]\n\nIllustrationExercise\n\n\nExample\n\\[A = \\begin{pmatrix}\n1 & -1 \\cr\n1 & 0 \\cr\n-1 & 0.5\n\\end{pmatrix}, \\quad A^\\intercal =  \\begin{pmatrix}\n1 &  1 & -1 \\cr\n-1 & 0  & 0.5\n\\end{pmatrix},\n\\]\nProperties\n\n\\((A^\\intercal)^\\intercal = A,\\)\n\\((AB)^\\intercal = (B)^\\intercal (A)^\\intercal,\\)\n\\((A+B)^\\intercal = (A)^\\intercal + (B)^\\intercal.\\)\n\n\n\n\\[A = \\begin{pmatrix}\n1 & -1 & 0 \\cr\n1 & 0 & -0.5 \\cr\n-1 & 0.5 & 2\n\\end{pmatrix}, B = \\begin{pmatrix}\n1 & 1 & 0 \\cr\n-1 & 0 & -0.5\n\\end{pmatrix}, C=\\begin{pmatrix}\n1 & 2 \\cr\n2 & 1\n\\end{pmatrix}, x \\in \\R^2, y\\in\\R^3.\\]\nCompute \\((C)^\\intercal, (BA)^\\intercal, x^\\intercal B, (Ay)^\\intercal, y^\\intercal A^\\intercal\\)"
  },
  {
    "objectID": "linearalgebra4DS.html#dot-product-and-norm",
    "href": "linearalgebra4DS.html#dot-product-and-norm",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Dot product and norm",
    "text": "Dot product and norm\nLet \\(x,y\\) be in \\((\\R^n)^2,\\) the dot product, also known as inner product or scalar product is defined as \\[x\\cdot y = y\\cdot x = \\sum_{i=1}^n x_i y_i = x^\\intercal y= y^\\intercal x.\\] The norm of a vector \\(x\\), symbolized with \\(\\norm{x},\\) is defined by \\[\\norm{x}^2 =\\sum_{i=1}^n x_i^2 = x^\\intercal x.\\]\n\nExampleGeometrical PropertiesUseful remark\n\n\n\\[x=\\begin{pmatrix}\n1\\cr\n0\n\\end{pmatrix},  y=\\begin{pmatrix}\n0\\cr\n1\n\\end{pmatrix},  z=\\begin{pmatrix}\n1\\cr\n2\n\\end{pmatrix} \\]\n\\[x\\cdot y = 0, x\\cdot z = 1, y \\cdot z = 2, \\quad \\norm{x}=\\norm{y}=1, \\quad \\norm{z} = \\sqrt{5}\\]\n\n\n\nLet \\((x,y)\\in(\\R^n)^2\\),\n\\(x\\cdot y = \\norm{x} \\norm{y} \\cos(\\theta)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\cos(\\theta)= \\frac{x\\cdot y}{\\norm{x} \\norm{y}}\\)"
  },
  {
    "objectID": "linearalgebra4DS.html#vector-space",
    "href": "linearalgebra4DS.html#vector-space",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Vector space",
    "text": "Vector space\n\n\n\nDefinition\n\n\nA real vector space \\((V, +, . \\R)\\) is a set \\(V\\) equipped with two operations satisfybing the following properties for all \\((x,y,z)\\in V^3\\) and all \\((\\lambda,\\mu)\\in\\R^2:\\)\n\n(close for addition) \\(x + y \\in V,\\)\n(close for scalar multiplaication) \\(\\lambda x  \\in V,\\)\n(communtativity) \\(x + y = y + x,\\)\n(associativity of +) \\((x + y) + z  = x + (y + z),\\)\n(Null element for +) \\(x + \\mathbb{0} = x,\\)\n(Existence of additive inverse +) There exists \\(-x \\in V\\), such that $ + (-x) = ,$\n(Associativity of scalar multiplication) \\(\\lambda (\\mu x)   = (\\lambda \\mu ) x,\\)\n(Distributivity of scalar sums) \\((\\lambda + \\mu ) x   = \\lambda x + \\mu x,\\)\n(Distributivity of vector sums) \\(\\lambda (x + y )   = \\lambda x + \\lambda y,\\)\n(Scalar multiplication identity) \\(1 x   = x.\\)\n\n\n\n\nIt is enough for the course to think to the set of vectors in \\(\\R^d\\)."
  },
  {
    "objectID": "linearalgebra4DS.html#linear-mapping",
    "href": "linearalgebra4DS.html#linear-mapping",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Linear mapping",
    "text": "Linear mapping\nLet \\(f\\) be a function from \\(\\R^d\\) to \\(\\R^n\\), \\(f\\) is linear if and only if\nFor all \\((\\lambda,\\mu) \\in \\R^2\\) and for all \\((x,y) (\\R^d)^2,\\) \\[f(\\lambda x + \\mu y ) = \\lambda  f(x) + \\mu f( y )\\]\nExample\n\\[\\begin{align}\nf : \\R^2 & \\to \\R\\cr\nx & \\mapsto x_1-x_2\n\\end{align}\\]\n\\[\\begin{align}\nf(\\lambda x + \\mu y) & = f \\left (\n\\begin{pmatrix}\n\\lambda x_1 + \\mu y_1\\cr\n\\lambda x_2 + \\mu y_2\\cr\n\\end{pmatrix}\n\\right) =  (\\lambda x_1 + \\mu y_1) - (\\lambda x_2 + \\mu y_2)=\\lambda (x_1 - x_2) + \\mu (y_1 -y_2)\\cr\n& = \\lambda f( x )  + \\mu f( y )\\cr\n\\end{align}\\]\n\\[f(x) = A x, \\quad A = \\begin{pmatrix}\n1 & -1\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "linearalgebra4DS.html#matrix-as-a-linear-map-on-vector-space",
    "href": "linearalgebra4DS.html#matrix-as-a-linear-map-on-vector-space",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Matrix as a linear map on vector space",
    "text": "Matrix as a linear map on vector space\nA real matrix \\(A\\in\\R^{n\\times d}\\) is a linear function operating from one real vector space of dimension \\(d\\) to real vector space of dimension \\(n\\).\n\\[\\begin{align}\nA: &\\R^d \\to  \\R^n\\\\\n  & x  \\mapsto  y = Ax\n\\end{align}\\]\n\nExamplesVisualisationExercicesSolutions\n\n\n\n\n\\[A = \\begin{pmatrix}\n\\color{color\n}{1} & 2 \\cr\n1 & 3 \\cr\n-1 & -1 \\end{pmatrix}\\]\n\\[\\begin{align}\nA: &\\R^2 \\to  \\R^3\\\\\n  & x  \\mapsto  y = \\begin{pmatrix}\n\\color{color\n}{1} & 2 \\cr\n1 & 3 \\cr\n-1 & -1 \\end{pmatrix} \\begin{pmatrix}\nx_1\\cr\nx_2 \\end{pmatrix} =\\begin{pmatrix}\n                      x_1 + 2 x_2 \\cr\n                      x_1 + 3 x_2 \\cr\n                      -x_1-x_2\n                    \\end{pmatrix}\n                    \n\\end{align}\\]\nIn particular \\(A \\begin{pmatrix}1\\cr 0\\end{pmatrix} = A_{.,1}\\) and \\(A \\begin{pmatrix} 0\\cr 1\\end{pmatrix} = A_{.,2}.\\)\n\n\n\n\\[M = \\begin{pmatrix}\n\\color{color\n}{1} & -1 \\cr\n1 & 2\n\\end{pmatrix}\\]\n\\[\\begin{align}\n  & x  \\mapsto  y = \\begin{pmatrix}\n                    1 & -1 \\cr\n                    1 & 2\n                    \\end{pmatrix} \\begin{pmatrix}\nx_1\\cr\nx_2 \\end{pmatrix} =\\begin{pmatrix}\n                      x_1 -  x_2 \\cr\n                      x_1 + 2 x_2\n                    \\end{pmatrix}\n\\end{align}\\]\nIn particular \\(M \\begin{pmatrix}1\\cr 0\\end{pmatrix} = M_{.,1}\\) and \\(M \\begin{pmatrix} 0\\cr 1\\end{pmatrix} = M_{.,2}.\\)\n\n\n\n\n\n\n\\[\\begin{align}\nM: &\\R^2 \\to  \\R^2\\\\\n  & x  \\mapsto  y = \\begin{pmatrix}\n  1 & -1 \\cr\n1 & 2\n\\end{pmatrix} x\n\\end{align}\\] is a transformation of \\(\\R^2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 1\nLet’s consider the application from \\(\\R^2\\) to \\(\\R^2\\) which projects on the \\(x\\) axis.\n\nIs it a linear transformation of \\(\\R^2\\) ?\nIf so, could you write the corresponding matrix \\(P\\) ?\n\n\nExercice 2\nLet’s consider the application from \\(\\R^2\\) to \\(\\R\\) which computes the square norm of the vector \\(|| x||^2 = x_1^2 + x_2^2\\)\n\nIs it a linear transformation of \\(\\R^2\\) ?\nIf so, could you write the corresponding matrix \\(N\\) ?\n\n\n\n\n\n\n\nExercice 1\n\nProjection on \\(x\\) axis consists in forgetting the \\(y\\) axis component. \\[\\begin{align}\nf(\\lambda_1 x + \\lambda_2 z) & = f\\left ( \\begin{pmatrix} \\lambda_1 x_1 + \\lambda_2 z_1 \\cr\n\\lambda_1 x_1 + \\lambda_2 z_2\\cr\n\\end{pmatrix} \\right) \\cr\n& =  \\begin{pmatrix} \\lambda_1 x_1 + \\lambda_2 z_1 \\cr\n0\\cr\n\\end{pmatrix}\\cr\n& = \\lambda_1 f(x) + \\lambda_2 f(z),\n\\end{align}\\]\n\\(f\\) is linear, its matrix form \\(P:\\)\n\n\\[P = \\begin{pmatrix} 1 & 0 \\cr 0  & 0 \\end{pmatrix},\\]\n\n\\[Px = \\begin{pmatrix} 1 & 0 \\cr 0  & 0 \\end{pmatrix} \\begin{pmatrix} x_1  \\cr x_2  \\end{pmatrix} =  \\begin{pmatrix} x_1  \\cr 0  \\end{pmatrix}\\]\nExercice 2\n\nSquare Norm \\(f\\) of \\(x\\) equals \\(x_1^2 + x_2^2,\\) As \\(f(\\lambda_1 x ) = \\lambda_1^2 f( x)\\), the square norm is not a linear function and could not be expressed as a matrix."
  },
  {
    "objectID": "linearalgebra4DS.html#linear-independance",
    "href": "linearalgebra4DS.html#linear-independance",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Linear independance",
    "text": "Linear independance\nA sequence of vectors \\(x_1, \\cdots, x_k\\) from \\(\\R^n\\) is said to be linearly independent, if the only solution to \\(X a = \\begin{pmatrix}\n& & & \\cr\nx_1 & x_2 & \\cdots & x_k \\cr\n& & & \\cr\n\\end{pmatrix} \\begin{pmatrix} a_1 \\cr a_2 \\cr \\vdots \\cr a_k\\end{pmatrix} =\\mathbb{0}\\) is \\(a =\\mathbb {0}\\).\n\nExampleExercice\n\n\n\\(x_1 = \\begin{pmatrix}\n1 \\cr\n0 \\cr\n0\n\\end{pmatrix}, \\quad x_2 = \\begin{pmatrix}\n1 \\cr\n1 \\cr\n0\n\\end{pmatrix}, \\quad X=\\begin{pmatrix}\n1 & 1  \\cr\n0 & 1  \\cr\n0 & 0 \\cr\n\\end{pmatrix}, \\quad Xa = \\begin{pmatrix}a_1 + a_2\\cr\na_2 \\cr\n0 \\end{pmatrix}.\\)\n\\(x_1\\) and \\(x_2\\) are linearly independant.\n\n\n\\(x_1 = \\begin{pmatrix}\n1 \\cr\n1 \\cr\n1\n\\end{pmatrix}, \\quad x_2 = \\begin{pmatrix}\n1 \\cr\n1 \\cr\n0\n\\end{pmatrix}, x_3 = \\begin{pmatrix}\n0 \\cr\n1 \\cr\n1\n\\end{pmatrix}, x_4 = \\begin{pmatrix}\n0 \\cr\n-1 \\cr\n1\n\\end{pmatrix}\\)\n\nAre \\((x_1, x_2,x_3)\\) linearly independant ?\nAre \\((x_1, x_2,x_4)\\) linearly independant ?"
  },
  {
    "objectID": "linearalgebra4DS.html#matrix-rank",
    "href": "linearalgebra4DS.html#matrix-rank",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Matrix rank",
    "text": "Matrix rank\nThe sequence of columns of a matrix \\(M\\in \\R^{n\\times d}\\) forms a sequence of \\(d\\) vectors \\((c_1, \\cdots,c_d)\\) in \\(\\R^n\\), while the rows of a matrix forms a sequence of \\(n\\) vectors \\((r_1, \\cdots,r_n)\\) in \\(\\R^d\\).\nThe rank of a matrix is the length of the largest sequence of linearly independant columns or equivalently the length of the largest sequence of linearly independant rows.\n\nExampleRemarksExample with RExample with Python\n\n\n\n\\(X_1 =\\begin{pmatrix}\n1 & 1  \\cr\n0 & 1  \\cr\n0 & 0 \\cr\n\\end{pmatrix}, \\quad rank(X_1) = 2; \\quad \\quad X_2=\\begin{pmatrix}\n1 & 1 & 0 \\cr\n1 & 1 & -1\\cr\n1 & 0 & 1\n\\end{pmatrix}, \\quad rank(X_2) = 3;\\)\n\\(X_3=\\begin{pmatrix}\n1 & 1 & -1 \\cr\n1 & 1 & -1\\cr\n1 & 0 & 1\n\\end{pmatrix}, \\quad rank(X_3) = 2;\\)\n\n\n\nLet \\(A= (a_1, \\cdots, a_d)^\\intercal \\in \\R^d\\), \\(A A^\\intercal = \\begin{pmatrix}\n& &  \\cr\na_1 A & \\cdots & a_d A\\cr\n& &  \\cr\n\\end{pmatrix}\\) if of rank \\(1\\).\n\n\n\n\n\n#install.packages('Matrix')\nlibrary(Matrix)\nX1 &lt;- matrix(c(1,0,0,1,1,0), ncol = 2)\nX2 &lt;- matrix(c(1,1,1,1,1,0, 0, -1,1), ncol = 3)\nX3 &lt;- matrix(c(1,1,1,1,1,0, -1, -1,1), ncol = 3)\n\n\n\nrankMatrix(X1)[1]\n\n[1] 2\n\nrankMatrix(X2)[1]\n\n[1] 3\n\nrankMatrix(X3)[1]\n\n[1] 2\n\n\n\n\n\n\n\nimport numpy as np\nfrom numpy.linalg import matrix_rank\n\nX1 = np.array([[ 1, 1], [ 0, 1], [ 0, 0]])\nX2 = np.array([[ 1, 1, 0], [ 1, 1, -1], [ 1, 0, 1]])\nX3 = np.array([[ 1, 1, -1], [ 1, 1, -1], [ 1, 0, 1]])\n\nmatrix_rank(X1) \n\nnp.int64(2)\n\nmatrix_rank(X2) \n\nnp.int64(3)\n\nmatrix_rank(X3) \n\nnp.int64(2)"
  },
  {
    "objectID": "linearalgebra4DS.html#column-space-rank",
    "href": "linearalgebra4DS.html#column-space-rank",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Column Space, rank",
    "text": "Column Space, rank\nIf \\(A\\in\\R^{n\\times d},\\) and \\(c\\in \\R^d\\), \\[A c = \\begin{pmatrix}\n& & &  \\cr\nA_{.,1} & A_{.,2} & \\cdots & A_{.,d} \\cr\n& & &  \\cr\n\\end{pmatrix} \\begin{pmatrix}\nc_1  \\cr\nc_2 \\cr\n\\vdots \\cr\nc_d  \\cr\n\\end{pmatrix} = \\begin{pmatrix}\nc_1 a_{11} + c_2 a_{12} + \\cdots + c_d a_{1d} \\cr\n\\vdots \\cr\nc_1 a_{n1} + c_2 a_{n2} + \\cdots + c_d a_{nd} \\cr\n\\end{pmatrix} = c_1 A_{.,1} + c_2  A_{.,2} + \\cdots + c_d  A_{.,d}\\]\nImage of \\(A\\) or Column space of A\n\\(Im(A) = \\left \\lbrace y \\in \\R^d; \\exists c\\in R^d, y = \\sum_{l=1}^d c_l A{., l} \\right \\rbrace\\)\nRemarks\n\nIf \\(A_{.,1}\\) and \\(A_{,2}\\) are linearly dependant, \\(A_{,1} = \\lambda A_{,2}\\) and \\(Ac = ( c_1 \\lambda + c_2)  A_{.,2} + \\cdots + c_d  A_{.,d}\\). The rank is the minimal number of vector to generate \\(Im(A)\\), that is the dimension of \\(Im(A)\\)."
  },
  {
    "objectID": "linearalgebra4DS.html#remarkable-matrices",
    "href": "linearalgebra4DS.html#remarkable-matrices",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Remarkable matrices",
    "text": "Remarkable matrices\nIdentity matrix\nThe square matrix \\(I\\) such \\(I_{ij} =\\delta_{ij}\\) is named the Identity matrix and acts as the neutral element for matrix multiplication\nLet \\(I_n\\) be the identity matrix in $^{nn}, for any \\(A \\in \\R^{m\\times n}\\), \\(A I_n = A\\) and \\(I_m A = A\\).\nDiagonal matrix\nA Diagonal matrix is a matrix in which elements outside the main diagonal are all zeros. The term is mostly used for square matrix but the terminology might also be used otherwise.\n\nExampleGeometrical point of view\n\n\nLet’s \\(e_i = (\\delta_{i1}, \\cdots, \\delta_{id})\\) and \\(D = \\begin{pmatrix}2 & 0 & 0 \\cr\n                    0 & 1 & 0 \\cr\n                    0 & 0 & -1\n                    \\end{pmatrix}\\) \\[D \\class{rouge}{e_1} = 2 \\class{rouge}{e_1},\\quad D \\class{bleu}{e_2} =  \\class{bleu}{e_2},\\quad D \\class{orange}{e_3} = - \\class{orange}{e_3}.\\]\n\\(D\\) acts as simple scalar multiplier on the basis vectors.\n\n\n\n\nConsider \\(D = \\begin{pmatrix}2  & 0 \\cr\n                    0  & 0.5\n                    \\end{pmatrix},\\quad Dx = \\begin{pmatrix} 2 x_1 \\cr\n                      0.5 x_2\n                    \\end{pmatrix}.\\)"
  },
  {
    "objectID": "linearalgebra4DS.html#orthogonal-matrix",
    "href": "linearalgebra4DS.html#orthogonal-matrix",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Orthogonal matrix",
    "text": "Orthogonal matrix\nDefinition\nAn orthogonal matrix, \\(Q\\) or orthonormal matrix, is a real square matrix whose columns and rows are orthonormal vectors and therefore verifies \\[Q^\\intercal Q =  Q Q^\\intercal = I.\\]\n\nExampleGeometrical point of view\n\n\n\\(Q = \\begin{pmatrix}\\frac{\\sqrt{2}}{2} & 0 &  -\\frac{\\sqrt{2}}{2}\\cr\n                    \\frac{\\sqrt{2}}{2} & 0 &  \\frac{\\sqrt{2}}{2}\\cr\n                    0 & 1 & 0\n                    \\end{pmatrix}\\) \\[Q  Q^\\intercal = I_3\\]\n\\(D\\) transform the vector basis in new orthonormal vectors.\n\n\n\n\nConsider \\(Q = \\begin{pmatrix}\\frac{\\sqrt{2}}{2} &   -\\frac{\\sqrt{2}}{2}\\cr\n                    \\frac{\\sqrt{2}}{2} &  \\frac{\\sqrt{2}}{2}\n                    \\end{pmatrix},\\quad Qx = \\begin{pmatrix}  \\frac{\\sqrt{2}}{2}x_1 + \\frac{\\sqrt{2}}{2} x_2\\cr\n                  -\\frac{\\sqrt{2}}{2}x_1 + \\frac{\\sqrt{2}}{2} x_2\n                    \\end{pmatrix}.\\)"
  },
  {
    "objectID": "linearalgebra4DS.html#eigen-values-eigen-vectors",
    "href": "linearalgebra4DS.html#eigen-values-eigen-vectors",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Eigen values, Eigen vectors",
    "text": "Eigen values, Eigen vectors\nConsider a square matrix, \\(A \\in \\R^{d\\times d}\\), and a nonzero vector, \\(v \\in \\R^d\\), \\(v\\) is an eigen vector for the eigen value \\(\\lambda\\) if \\[Av = \\lambda v.\\] If \\(v\\) is an eigen vector for \\(A\\), applying \\(A\\) to \\(v\\) just consists in multiplying by the corresponding eigen value.\n\nGeometric interpretationExercises\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the eigen values and eigen vectors for \\(A_1 = \\begin{pmatrix}1 & 0 \\cr 0 & 2\\end{pmatrix}\\), \\(A_2 = \\begin{pmatrix}1 & 3 \\cr 0 & 2\\end{pmatrix},\\) \\(A_3 = \\begin{pmatrix} 2 & 2 \\cr -2 & 2 \\end{pmatrix}.\\)"
  },
  {
    "objectID": "linearalgebra4DS.html#singular-value-decomposition-svd",
    "href": "linearalgebra4DS.html#singular-value-decomposition-svd",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Singular Value Decomposition (SVD)",
    "text": "Singular Value Decomposition (SVD)\nLet’s \\(A\\) a \\(n\\times d\\) matrix of rank \\(r\\), there exists a \\(n\\times n\\) orthogonal matrix \\(P\\), a \\(d\\times d\\) orthogonal matrix \\(Q\\), and \\(D_1\\) a diagonal \\(r\\times r\\) matrix whose diagonal terms are in decreasing order such that: \\[ A = P\n\\begin{pmatrix} D_1 & 0 \\cr\n                0 & 0 \\cr\n    \\end{pmatrix} Q^\\intercal\\]\nFor a nice and visual course on SVD see the Steve Brunton Youbube Channel on this subject!\n\nExamplesRemarksSVD in RSVD in Python\n\n\n\\(A = \\frac{\\sqrt{2}}{6} \\begin{pmatrix}\n0 & 4 \\cr\n6 & 2 \\cr\n-3 & -5\n\\end{pmatrix}\\) then consider\n\\(P = \\frac{1}{3} \\begin{pmatrix}\n1 & -2 & 2 \\cr\n2 & 2 & 1 \\cr\n-2 & 1 & 2\n\\end{pmatrix}, \\quad D = \\begin{pmatrix} 2 & 0 \\cr\n  0 & -1 \\cr\n  0 & 0\n\\end{pmatrix}, \\quad and \\ Q = \\frac{\\sqrt{2}}{2} \\begin{pmatrix} 1 & -1 \\cr\n  1 & 1\n\\end{pmatrix}\\)\n\n\nif \\(A\\) is a \\(\\^{d\\timesd}\\) symmetric matrix\n\\[A = P D P^\\intercal.\\]\n\n\n\nA =matrix(c(0, sqrt(2), - sqrt(2)/2, 2* sqrt(2)/3, sqrt(2)/3, -5*sqrt(2)/6), ncol = 2);A\n\n           [,1]       [,2]\n[1,]  0.0000000  0.9428090\n[2,]  1.4142136  0.4714045\n[3,] -0.7071068 -1.1785113\n\nsvd(A)\n\n$d\n[1] 2 1\n\n$u\n           [,1]       [,2]\n[1,] -0.3333333  0.6666667\n[2,] -0.6666667 -0.6666667\n[3,]  0.6666667 -0.3333333\n\n$v\n           [,1]       [,2]\n[1,] -0.7071068 -0.7071068\n[2,] -0.7071068  0.7071068\n\n\n\n\n\n\narray([[ 0.        ,  0.94280904],\n       [ 1.41421356,  0.47140452],\n       [-0.70710678, -1.1785113 ]])"
  },
  {
    "objectID": "linearalgebra4DS.html#and-now",
    "href": "linearalgebra4DS.html#and-now",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "And now",
    "text": "And now\nIt’s time to use this concept for Data Analysis"
  },
  {
    "objectID": "linearalgebra4DS.html#bibliography",
    "href": "linearalgebra4DS.html#bibliography",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Bibliography",
    "text": "Bibliography\nCohen, M. X. (2022). Practical linear algebra for data science. O’Reilly Media, Inc.\nGentle, J. E. (2007). “Matrix algebra”. In: Springer texts in statistics, Springer, New York, NY, doi 10, pp. 978-0.\nHarville, D. A. (1998). Matrix algebra from a statistician’s perspective.\nPetersen, K. B., M. S. Pedersen, and others (2008). “The matrix cookbook”. In: Technical University of Denmark 7.15, p. 510.\nYoshida, R. (2021). Linear algebra and its applications with R. Chapman and Hall/CRC. URL: https://shainarace.github.io/LinearAlgebra/."
  },
  {
    "objectID": "introduction.html#courses-objective",
    "href": "introduction.html#courses-objective",
    "title": "Topics in Statistics - Presentation",
    "section": "Courses Objective",
    "text": "Courses Objective\n\nReactivate Basic Linear algebra useful in data Science\nProvide basic Data Science tools for representing and analyzing gentle multivariate quantitative Dataset\nImplementing simple R and Python Data analysis"
  },
  {
    "objectID": "introduction.html#organization",
    "href": "introduction.html#organization",
    "title": "Topics in Statistics - Presentation",
    "section": "Organization",
    "text": "Organization\n\nLecture 1: Linear algebra Survival Kit\nLecture 2: Principal Component Analysis\nLecture 3: Regression\nLab1: Principal Component analysis with R\nLab2: Regression with Python"
  },
  {
    "objectID": "introduction.html#planning",
    "href": "introduction.html#planning",
    "title": "Topics in Statistics - Presentation",
    "section": "Planning",
    "text": "Planning\n\nFriday, September 6 - Lecture 1 and 2\nMonday, September 9 - Lecture 2 and lab 1\nThursday, September 12 - Lecture 3\n\nFriday, September 6 - Lab 2"
  },
  {
    "objectID": "introduction.html#material",
    "href": "introduction.html#material",
    "title": "Topics in Statistics - Presentation",
    "section": "Material",
    "text": "Material\n\nLecture and lab material are available on the course website\nThe corresponding Github repository is available from the website by clicking on the Github icon"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  }
]