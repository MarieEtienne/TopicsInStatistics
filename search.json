[
  {
    "objectID": "pca.html#the-example-of-doubs-river-characteristics",
    "href": "pca.html#the-example-of-doubs-river-characteristics",
    "title": "Principal Component Analysis",
    "section": "The Example of Doubs River Characteristics",
    "text": "The Example of Doubs River Characteristics\nWe measured the physico-chemical characteristics at 30 different sites along the Doubs River .\n\n\nThe first 4 rows (out of 30) of the doubs.env dataset\n\n\n# A tibble: 30 × 11\n    das   alt   pen   deb    pH   dur   pho   nit   amm   oxy   dbo\n  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0.3   934  48    0.84   7.9    45  0.01  0.2   0     12.2   2.7\n2   2.2   932   3    1      8      40  0.02  0.2   0.1   10.3   1.9\n3  10.2   914   3.7  1.8    8.3    52  0.05  0.22  0.05  10.5   3.5\n4  18.5   854   3.2  2.53   8      72  0.1   0.21  0     11     1.3\n# ℹ 26 more rows\n\n\nHow can we best visualize these data to reveal the relationships between variables and identify similarities between sites?\n\n\ndas: distance to the source (\\(km\\)),\nalt: altitude (\\(m\\)),\npen: slope (elevation change per 1000m),\ndeb: flow rate (\\(m^3.s^{-1}\\)),\npH: water pH,\ndur: calcium concentration (\\(mg.L^{-1}\\)),\npho: phosphate concentration (\\(mg.L^{-1}\\)),\nnit: nitrate concentration (\\(mg.L^{-1}\\)),\namn: ammonium concentration (\\(mg.L^{-1}\\)),\noxy: dissolved oxygen concentration (\\(mg.L^{-1}\\)),\ndbo: Biological Oxygen Demand (\\(mg.L^{-1}\\))."
  },
  {
    "objectID": "pca.html#the-example-of-penguin-morphology",
    "href": "pca.html#the-example-of-penguin-morphology",
    "title": "Principal Component Analysis",
    "section": "The Example of Penguin Morphology",
    "text": "The Example of Penguin Morphology\nWe measured the morphological characteristics of various penguins:\n\n\nThe first 6 rows (out of 333) of the penguins dataset\n\n\n# A tibble: 333 × 4\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1           39.1          18.7               181        3750\n2           39.5          17.4               186        3800\n3           40.3          18                 195        3250\n4           36.7          19.3               193        3450\n5           39.3          20.6               190        3650\n6           38.9          17.8               181        3625\n# ℹ 327 more rows\n\n\nHow can we best visualize these data to reveal the relationships between variables and identify similarities between individuals?\n\n\nbill_length_mm: bill length,\nbill_depth_mm: bill depth,\nflipper_length_mm: flipper length,\nbody_mass_g: body mass."
  },
  {
    "objectID": "pca.html#formalization",
    "href": "pca.html#formalization",
    "title": "Principal Component Analysis",
    "section": "Formalization",
    "text": "Formalization\n\nFor each individual \\(i\\), we measured \\(p\\) different variables.\nFor each variable \\(k\\), we measured \\(n\\) individuals.\n\nThe data are arranged in a table with \\(n\\) rows and \\(p\\) columns.\n\n\n\n\n\n\n\n\n\n\n\n\nWe denote \\(x_{ik}\\) as the value measured for variable \\(k\\) on individual \\(i\\),\nand\n\n\\(x_{\\bullet k} = \\frac{1}{n} \\sum_{i=1}^n x_{ik}\\) as the mean value of variable \\(k\\),\n\\(s_k  = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ik}-x_{\\bullet k})^2}\\) as the standard deviation of variable \\(k\\)."
  },
  {
    "objectID": "pca.html#the-same-question-in-various-fields",
    "href": "pca.html#the-same-question-in-various-fields",
    "title": "Principal Component Analysis",
    "section": "The Same Question in Various Fields",
    "text": "The Same Question in Various Fields\n\nSensory analysis: score of descriptor \\(k\\) for product \\(i\\)\nEconomics: value of indicator \\(k\\) for year \\(i\\)\nGenomics: gene expression \\(k\\) for patient/sample \\(i\\)\nMarketing: satisfaction index \\(k\\) for brand \\(i\\)\n\netc…\nWe have \\(p\\) variables measured on \\(n\\) individuals, and we want to visualize these data to understand the relationships between variables and the proximity between individuals."
  },
  {
    "objectID": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table",
    "href": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table",
    "title": "Principal Component Analysis",
    "section": "Seeing is Understanding: How to Represent the Information Contained in This Table?",
    "text": "Seeing is Understanding: How to Represent the Information Contained in This Table?\nIdea 1: Represent the relationships between variables 2 by 2"
  },
  {
    "objectID": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table-1",
    "href": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table-1",
    "title": "Principal Component Analysis",
    "section": "Seeing is Understanding: How to Represent the Information Contained in This Table?",
    "text": "Seeing is Understanding: How to Represent the Information Contained in This Table?\nIdea 1: Represent the relationships between variables 2 by 2\n\n\n\n\n\n\n\n\n\nWe lose information on the other axes."
  },
  {
    "objectID": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table-2",
    "href": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table-2",
    "title": "Principal Component Analysis",
    "section": "Seeing is Understanding: How to Represent the Information Contained in This Table?",
    "text": "Seeing is Understanding: How to Represent the Information Contained in This Table?\nIdea 1: Represent the relationships between variables 2 by 2"
  },
  {
    "objectID": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table-3",
    "href": "pca.html#seeing-is-understanding-how-to-represent-the-information-contained-in-this-table-3",
    "title": "Principal Component Analysis",
    "section": "Seeing is Understanding: How to Represent the Information Contained in This Table?",
    "text": "Seeing is Understanding: How to Represent the Information Contained in This Table?\nObjective:\n\nRepresent without losing too much information\nIdeally, individuals far apart in the initial cloud remain far apart in the representation.\n\nWhat We Need:\n\nQuantify the information loss in the representation\nBuild the representation that loses the least amount of information\n\nPrecautions:\n\nPotentially, Make variables expressed in different units comparable"
  },
  {
    "objectID": "pca.html#distance-between-individuals",
    "href": "pca.html#distance-between-individuals",
    "title": "Principal Component Analysis",
    "section": "Distance between individuals",
    "text": "Distance between individuals\nLet \\(X_{i,.}^\\intercal \\in \\R^d\\) be the descriptions of individual \\(i\\). To quantify the distance between indivuals we might used the Euclidian distance in \\(\\R^d,\\)\n\\[d(i, j)^2 = \\sum_{k=1}^d (x_{ik} - x_{jk})^2 = \\norm{X_{i,.}^\\intercal - X_{i,.}^\\intercal}^2 = \\left({X_{i,.}^\\intercal - X_{i,.}^\\intercal}\\right)^\\intercal  \\left({X_{i,.}^\\intercal - X_{i,.}^\\intercal}\\right). \\]\nThis could be misleading when dealing with different variables with different scale.\nExample in the penguins dataset\nBody mass varies form 2700 to 6300, while flipper length varies from 172 to 231.\nVariable Body mass will contribute more to the distance between individuals."
  },
  {
    "objectID": "pca.html#specify-a-different-metric",
    "href": "pca.html#specify-a-different-metric",
    "title": "Principal Component Analysis",
    "section": "Specify a different metric",
    "text": "Specify a different metric\n\n\n\nDefinition\n\n\nA metric in \\(\\R^d\\), \\(M\\) is a definite positive symmetric \\(\\R^{d\\times d}\\) matrix which can be used to define distance between individuals\n\\[d_M(i, j)^2  = \\norm{X_{i,.}^\\intercal - X_{i,.}^\\intercal}^2_M = \\left({X_{i,.}^\\intercal - X_{i,.}^\\intercal}\\right)^\\intercal  M \\left({X_{i,.}^\\intercal - X_{i,.}^\\intercal}\\right).\\]\n\n\n\n\n\n\nIf \\(M=I_d\\),\n\n\\(d_M\\) is the classical Euclidian distance\n\nGet rid of the units used for the variables and attribute the same weight to all variables by choosing\n\n\\(M = \\begin{pmatrix}\n\\frac{1}{s_1^2} & &0 & \\cr\n& \\frac{1}{s_2^2}  & & \\cr\n&0 & \\ddots & \\cr\n&&& \\frac{1}{s_d^2} \\cr\n\\end{pmatrix} = D_{1/s^2}\\)\n\nwith \\(s_k^2=\\sum_{i=1}^n (x_{ik} - x_{.k})^2,\\) \\(x_{.k} =\\frac{1}{n}\\sum_{i=1}^n x_{ik}\\)\n\n\n\nRemarks\n\n\nThe distance defined with \\(D_{1/s^2}\\) is the same than the distance defined on centred and scaled variables with the identity matrix.\n\n\n\nIn the following, we will assume that \\(X\\) is the matrix of centred and scaled variables."
  },
  {
    "objectID": "pca.html#dispersion-measure-inertia-with-respect-to-a-point",
    "href": "pca.html#dispersion-measure-inertia-with-respect-to-a-point",
    "title": "Principal Component Analysis",
    "section": "Dispersion measure: Inertia with respect to a point",
    "text": "Dispersion measure: Inertia with respect to a point\n\n\n\nDefinition\n\n\nInertia (denomination derived from moments of inertia in Physics) with respect to a point \\(a \\in R^{d},\\) according to metric \\(M\\): \\[I_a = \\sum_{i=1}^n \\norm{X_{i,.}^\\intercal - a}_M^2\\]\n\n\n\n\nInertia around the centroïd \\(G\\) (center of gravity of the cloud) plays a central role in Principal Component Analysis:\n\n\\(G = (x_{.,1}, \\cdots, x_{.,d})^\\intercal\\) and \\(I_G = min_{a\\in \\R^d} I_a\\)\nRemarks\nAssume, we deal with centred scaled variables:\n\\(I_G = \\sum_{i=1}^n \\norm{X_{i,.}^\\intercal - G}^2 = \\sum_{i=1}^n \\class{rouge}{\\sum_{k=1}^d} (x_{ik} - x_{.,k})^2 = \\class{rouge}{\\sum_{k=1}^d} \\sum_{i=1}^n  (x_{ik} - x_{.,k})^2.\\)\nAs the variables are scaled, \\(\\sum_{i=1}^n  (x_{ik} - x_{.,k})^2 = n s^2_k = n.\\) and \\(I_G=nd.\\)\nTotal inertia with scaled centred variables is \\(nd\\)"
  },
  {
    "objectID": "pca.html#dispersion-measure-inertia-with-respect-to-a-affine-subspace",
    "href": "pca.html#dispersion-measure-inertia-with-respect-to-a-affine-subspace",
    "title": "Principal Component Analysis",
    "section": "Dispersion measure: Inertia with respect to a affine subspace",
    "text": "Dispersion measure: Inertia with respect to a affine subspace\n\n\n\nDefinition\n\n\nInertia with respect to an affine subspace \\(S\\) according to metric \\(M\\): \\(I_S = \\sum_{i=1}^n d_M(X_{i,.}, S)^2\\)\n\n\n\nHuygens theorem states that if \\(S^G\\) stands for the affine subspace containing \\(G\\) and parallel to \\(S\\) then \\[I_S = I_{S^G} + d_M^2(a, G),\\] where \\(a\\) is the orthogonal projection of \\(G\\) on \\(S\\).\nThe affine subspace \\(S\\) which minimizes inertia is \\(S^G\\)."
  },
  {
    "objectID": "pca.html#inertia-decomposition",
    "href": "pca.html#inertia-decomposition",
    "title": "Principal Component Analysis",
    "section": "Inertia Decomposition",
    "text": "Inertia Decomposition\nSince, variables are centred \\(G=\\mathbb{0}\\), \\(I=\\sum_{i=1}^n d(X_{i,.},0)^2.\\)\nLet \\(S\\) be an affine subspace and \\(U=S^\\intercal,\\) \\(X^S_{i,.}\\) (recip. \\(X^S_{i,.})\\)) the orthogonal projection on \\(S\\) (recip. on \\(U\\)).\nAs \\(d(X_{i,.},0)^2 = d(X^S_{i,.},0)^2 + d(X^U_{i,.},0)^2\\), \\(I=I_S + I_{S^\\intercal}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\\(I_S\\) is the dispersion of the dataset lost by projection on \\(S\\),\nwhile \\(I_{S^\\intercal}\\) is the dispersion of the dataset projected on \\(S\\).\nPCA\nIdentifying \\((U_1, U_d)\\) a sequence of orthogonal unitary vectors such that \\(I_{U_1}\\leq I_{U_2}\\leq \\cdots \\leq I_{U_d}\\).\nThe projection on \\(U_1\\) is the best projection of the dataset in one dimension, \\(U_1\\) define the first Principal Component."
  },
  {
    "objectID": "pca.html#inertia-useful-representation",
    "href": "pca.html#inertia-useful-representation",
    "title": "Principal Component Analysis",
    "section": "Inertia: useful representation",
    "text": "Inertia: useful representation\nLet \\(x_i= X_{i,.}^\\intercal\\), (recall that \\(X_{i,}\\) is a row vector and \\(x_i\\) is the corresponding column vector).\n\\[\\begin{align}\nI & = \\sum_{i}^n d^2(X_{i,.}, 0) = \\sum_{i}^n \\norm{X_{i,.}}\\\\\n& = \\sum_{i}^n x_i^\\intercal x_i \\cr\n& = tr(\\sum_{i}^n x_i^\\intercal x_i) \\cr\n& = tr(\\sum_{i}^n x_i x_i^\\intercal ) \\cr\n& = tr(X^\\intercal  X) \\cr\n\\end{align}\\]\n\n\nRemarks\n\n\\(X^\\intercal  X\\) is the covariance matrix of the \\(d\\) variables,\n\\(X^\\intercal  X\\) is a symmetric \\(\\R^{d\\times d}\\) matrix, and the corresponding SVD\n\n\\[X^\\intercal  X = \\left ( P D Q^\\intercal \\right )^\\intercal \\left ( P D Q^\\intercal \\right ) =  Q D  P^\\intercal  P D Q^\\intercal = Q D^2 Q^\\intercal.\\]\n\n\n\\(I= tr(X^\\intercal X) = tr(Q D^2 Q^\\intercal) = tr(Q^\\intercal Q D  ) = tr( D ) = \\sum_{k=1}^d \\sigma^2_k,\\)\nwhere \\(\\sigma^2_k\\) stands for the k\\(^{th}\\) eigen value."
  },
  {
    "objectID": "pca.html#identifying-u_1",
    "href": "pca.html#identifying-u_1",
    "title": "Principal Component Analysis",
    "section": "Identifying \\(U_1\\)",
    "text": "Identifying \\(U_1\\)\nConsider \\(I_{\\Delta_{U}}\\) the inertia with respect to \\(\\Delta_{U}\\) affine subspace containing \\(G\\) with directed by the unitary vector \\(U\\).\n\\(I_{\\Delta_{U}}\\) is the cost of the projection on \\(\\Delta_{U}\\), i.e the loss information.\nMinimizing \\(I_{\\Delta_{U}}\\) equals maximizing \\(I_{\\Delta_{U^T}}\\), i.e the dispersion of the projected set of data points.\nProjection on \\(U\\)\n\\(d(X_{i,}, U^\\intercal)^2= \\left ( x_i \\dot U^\\intercal \\right )^2 = \\left ( X_i^\\intercal U \\right )^2 = cos(\\theta)^2 \\norm{x_i}^2 ,\\)\nSo that \\[\\begin{align}\nI_{\\Delta_{U^\\intercal}} & = \\sum_{i=1}^n  d(X_{i,}, U^\\intercal)^2 \\cr\n                         & = \\norm{X U}^2 \\cr\n                         & = U^\\intercal X^\\intercal X  U\\cr\n                         & = U^\\intercal Q D^2  Q^\\intercal   U\\cr\n\\end{align}\\]\nBut \\(U^\\intercal Q = \\begin{pmatrix} U^\\intercal Q_{.,1}, \\cdots, U^\\intercal Q_{.,d})\\end{pmatrix}\\) Is the coordinate of the unitary vector \\(U\\) on the basis defined by the eigen vector of \\(X\\): \\(\\omega_1 q_1 +  \\cdots +  \\omega_d q_d,\\) such that \\(\\sum_k \\omega_k^2=1\\)\n\\(I_{\\Delta_{U^\\intercal}} = \\sum_{k=1}^d \\omega_k \\sigma^2_k.\\)\nMaximizing \\(I_{\\Delta_{U^\\intercal}}\\), as \\(\\sigma^2_k\\) are in decreasing order, consists in choosing \\(\\omega_1=1\\)"
  },
  {
    "objectID": "pca.html#principal-components",
    "href": "pca.html#principal-components",
    "title": "Principal Component Analysis",
    "section": "Principal Components",
    "text": "Principal Components\nConsider the sequence of eigen vectors in \\(Q\\), they\n\nform an orthonormal basis,\nare ordered according their corresponding eigen values.\n\nFrom a mathematical point of view, PCA consist in using this new basis.\nConsequences\n\nDefine \\(d\\) new variables \\(C_1, \\cdots, C_d,\\) which are Linear combination of initial variables, i.e \\(C_j = \\sum_{k= 1}^d q_{jk} V_k\\)\nQuantify the quantity of information captured by each variable,\nBeing able to quantify the quality of representation for one individual or one variable"
  },
  {
    "objectID": "pca.html#practical-considerations",
    "href": "pca.html#practical-considerations",
    "title": "Principal Component Analysis",
    "section": "Practical considerations",
    "text": "Practical considerations\n\n\nNormalizing inertia\nTotal Inertia of the scaled dataset depends on the number of individual.\nIt is common to define a scaled Inertia to get rid of\\(n\\).\n\\[I^{s} = \\frac{1}{n} I,\\]\nWe also define the Covariance matrix \\(\\Sigma\\)\n\\[\\Sigma = \\frac{1}{n} X^\\intercal X,\\] and we considered the SVD for \\(\\frac{1}{\\sqrt{n}} X.\\)\nThis is generally the implemented version.\n\nSVD of \\(X^\\intercal X\\) instead of \\(X\\)\nAs we are mainly interesting in \\(Q\\), \\(d\\times d\\) matrix (not \\(P\\) the \\(n\\times n\\) matrix) and the square of the eigen values, it is more efficient to consider the SVD of \\(\\Sigma\\) directly.\nGeometrically\n\nThe matrix \\(Q^\\intercal\\) transform the original canonical basis \\((e_1, \\cdots, e_d)\\) of \\(\\R^d\\) in the new ACP basis \\((v_1, \\cdots, v_d)\\); .\nIn the new basis variable \\(k\\),\n\n\\[ \\sum_{j=1}^d \\sqrt{\\sigma_j^2} q_{kj} v_j = \\begin{pmatrix} \\sigma_1 q_{1j}, \\cdots, \\sigma_d q_{dj} \\end{pmatrix}^\\intercal\\]"
  },
  {
    "objectID": "pca.html#palmer-penguins-example",
    "href": "pca.html#palmer-penguins-example",
    "title": "Principal Component Analysis",
    "section": "Palmer Penguins example",
    "text": "Palmer Penguins example\n\n## the core scale function normalizes by n-1 instead of n\nscale2 &lt;- function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm)) / ( sqrt((length(x)-1) / length(x)) *sd(x, na.rm) )\n\nX &lt;- penguins |&gt; \n  mutate(year = as.factor(year))|&gt;  ## year willnot be considered as numerical variable\n  select(where(is.numeric)) |&gt;      ## select all numeric columns\n  mutate_all(list(scale2))          ## and scale them\n\nn &lt;- nrow(X) # number of individuals\nd &lt;- ncol(X)\n\nX_mat &lt;- X |&gt;  as.matrix()          ## the data point matrix \nX_norm_mat &lt;- 1/sqrt(n) * X_mat     ## the version considered to get rid of the number of individuals\n\nX_mat_trace &lt;- sum( diag(t(X_mat)%*% X_mat) )  # n d \nX_norm_mat_trace &lt;- sum( diag( t( X_norm_mat)%*% X_norm_mat ) )  # d\n\npenguins_svd &lt;- svd( t(X_mat)%*% X_mat )\npenguins_norm_svd &lt;- svd( t(X_norm_mat)%*% X_norm_mat )\n\n## eigenvalues\npenguins_eigenvalue &lt;- penguins_svd$d\nsum(penguins_eigenvalue)\n\n[1] 1332\n\npenguins_norm_eigenvalue &lt;- penguins_norm_svd$d\nsum(penguins_norm_eigenvalue)\n\n[1] 4"
  },
  {
    "objectID": "pca.html#palmer-penguins-example-1",
    "href": "pca.html#palmer-penguins-example-1",
    "title": "Principal Component Analysis",
    "section": "Palmer Penguins example",
    "text": "Palmer Penguins example\n\n\npenguins_Q &lt;- penguins_svd$u\npenguins_norm_Q &lt;- penguins_norm_svd$u\n\n## From orginal variable to new basis\n## t(Q) %*% t(X[1,]) for 1st individual\n## t(Q) %*% t(X) for all individuals, or in order to keep individual in lines\n## X %*% Q \n\ncoord_newbasis &lt;- X_mat %*% (penguins_Q) \ncoord_norm_newbasis &lt;- X_norm_mat %*% (penguins_Q) \n\ncoord_newbasis &lt;- coord_newbasis |&gt; \n  as_tibble() |&gt; \n  rename(Dim1 = V1, Dim2 = V2, Dim3 = V3, Dim4 = V4)\n\nggplot(coord_newbasis) + aes(x= Dim1, y = Dim2) + geom_point()\n\n\n\n\n\n\n\n\n#ggplot(coord_newbasis) + aes(x= Dim1, y = Dim2) + geom_point()\ndist(X[1:5,])\ndist(coord_newbasis[1:5,])\ndist(coord_newbasis[1:5,])\ndist(coord_newbasis[1:5,1:2])\n\n#dist(coord_norm_newbasis[1:5,]) * sqrt(n)\nX_C &lt;- as.matrix(coord_newbasis)\n\n          1         2         3         4\n2 0.7576266                              \n3 1.2500130 0.9980553                    \n4 1.0773914 1.2792018 0.9767689          \n5 1.1679522 1.6632525 1.4687302 0.8784467\n          1         2         3         4\n2 0.7576266                              \n3 1.2500130 0.9980553                    \n4 1.0773914 1.2792018 0.9767689          \n5 1.1679522 1.6632525 1.4687302 0.8784467\n          1         2         3         4\n2 0.7576266                              \n3 1.2500130 0.9980553                    \n4 1.0773914 1.2792018 0.9767689          \n5 1.1679522 1.6632525 1.4687302 0.8784467\n          1         2         3         4\n2 0.7175826                              \n3 0.5146672 0.2886753                    \n4 0.0545691 0.7139414 0.5300224          \n5 0.7883291 1.3981852 1.1195367 0.8306741"
  },
  {
    "objectID": "pca.html#global-quality-of-the-representation",
    "href": "pca.html#global-quality-of-the-representation",
    "title": "Principal Component Analysis",
    "section": "Global quality of the representation",
    "text": "Global quality of the representation\nLet \\(C_{12}\\) designs the plan defined by \\(C_1\\) and \\(C_2\\) and let \\(X^{C}\\) be the coordinates of the individuals in the new basis.\nThe information preserved by projection is \\[\\begin{align}\nI_{C_{12}^\\perp} & = \\sum_{i=1}^n \\left ( \\left(x^{C}_{i1)}\\right)^2 + \\left(x^{C}_{i2}\\right)^2 \\right) \\cr\n                  & = tr\\left( \\left(X^{C}_{,1:2}\\right)^\\intercal X^{C}_{,1:2}\\right)\\cr\n                  & =  \\lambda_1^2 + \\lambda^2_2\\cr\n                  & = n (\\sigma_1^2 + \\sigma^2_2)\\cr\n\\end{align}\\]\n\ncat(\"On plan 12: \", sum(penguins_eigenvalue[1:2]), \".\\n\")\ncat(\"Working with the correlation matrix (/n), sum of eigenvalues is \", sum(penguins_norm_eigenvalue[1:2]), \". \\n This has to be multiply by the size of the dataset to get inertia:\", sum(penguins_norm_eigenvalue[1:2]) * n, \"\\n\")\n\ncat(\"This is easier to appreciate when expressed  as a propotion of total inertia:\",  round(sum(penguins_norm_eigenvalue[1:2])/sum(penguins_norm_eigenvalue)*100,2),\"%. \\n\")\n\nOn plan 12:  1173.316 .\nWorking with the correlation matrix (/n), sum of eigenvalues is  3.523473 . \n This has to be multiply by the size of the dataset to get inertia: 1173.316 \nThis is easier to appreciate when expressed  as a propotion of total inertia: 88.09 %."
  },
  {
    "objectID": "pca.html#representing-jointly-initial-variables-and-principal-components",
    "href": "pca.html#representing-jointly-initial-variables-and-principal-components",
    "title": "Principal Component Analysis",
    "section": "Representing jointly initial variables and principal components",
    "text": "Representing jointly initial variables and principal components\nTo understand the links between original and new variables, or between original variables themselve.\n\n\nnew_var_coord &lt;- diag(sqrt(penguins_norm_eigenvalue))%*%t(penguins_norm_Q)\n\n## for graphical purpose\nnew_var_coord_df &lt;- t(new_var_coord) |&gt; as_tibble() |&gt; rename(C1=V1, C2= V2, C3 =V3, C4 = V4)\ncircle_df &lt;- data.frame(theta = seq(0, 2*pi, length.out = 501)) |&gt; \n  mutate(x = cos(theta), y  = sin(theta))\n\nggplot(new_var_coord_df) + geom_point(aes(x=C1, y=C2)) + coord_fixed() + xlim(c(-1,1)) + ylim(c(-1,1)) + \n  geom_segment(aes(x=rep(0,4), y = rep(0,4), xend=C1, yend = C2),  arrow = arrow(length = unit(0.5, \"cm\"))) +\n  geom_path(data = circle_df, aes(x=x, y=y)) +\n  annotate(\"text\", label = colnames(X), x = new_var_coord_df$C1  + 0.05, y = new_var_coord_df$C2- 0.05)"
  },
  {
    "objectID": "pca.html#quality-fo-the-representation-of-the-variables",
    "href": "pca.html#quality-fo-the-representation-of-the-variables",
    "title": "Principal Component Analysis",
    "section": "Quality fo the representation of the variables",
    "text": "Quality fo the representation of the variables\n\nThe quality of the projection on a component \\(j\\) depends on the angle \\(\\theta_{ij}\\) between the original variable and \\(C^j\\).\n\n\\[cos(\\theta_{ij})^2 = \\frac{(C_j^T X^i)^2}{\\norm{X^i}\\norm{X^i}} =  \\sigma^2_j q_{ij}^2\\]\n\nThe quality of the projection on a the plan \\((C^1C^2)\\) depends on the angle \\(\\theta_{i,1-2}\\) between the original variable and the projection plan.\n\n\\[ cos(\\theta_{i,1-2})^2 = \\sum_{j=1}^2 \\frac{(C_j^T X^i)^2}{\\norm{X^i}\\norm{C^j}} =  \\sum_{j=1}^2 \\sigma^2_j  q_{ij}^2\\]\n\n\n## quality of the representation for variable 1 on teh different axis\nnew_var_coord[,1]^2 \n\n## quality of the projection on plan C1-C2\nsum((new_var_coord[,1]^2 )[1:2])\n\n\n[1] 0.565246607 0.280304201 0.152175611 0.002273581\n[1] 0.8455508"
  },
  {
    "objectID": "pca.html#visualizing-individuals",
    "href": "pca.html#visualizing-individuals",
    "title": "Principal Component Analysis",
    "section": "Visualizing individuals",
    "text": "Visualizing individuals\n\n\\(X\\) is the matrix of individuals on the original basis, \\(XQ\\) is the matrix of individual in the new basis (no information lost)\nTo visualizing the dataset, we can project on the two first dimensions of the new basis\n\n\n\nXQ &lt;- X_mat %*% penguins_norm_Q\n\nXQ_df &lt;- XQ |&gt; \n  as_tibble() |&gt; \n  rename(C1 = V1, C2 = V2, C3 = V3, C4 = V4)\n\nggplot(XQ_df) + geom_point(aes(x=C1, y=C2))"
  },
  {
    "objectID": "pca.html#interpretation-1",
    "href": "pca.html#interpretation-1",
    "title": "Principal Component Analysis",
    "section": "Interpretation",
    "text": "Interpretation\nQuality of representation of individual \\(i\\) on the plan\n\nThe angle between the original individual vector \\(i\\) and component \\(j\\):\n\n\\[cos(\\theta_{ij})^2 = \\frac{( X_i Q^j)^2}{\\norm{X_i}^2} \\]\nContribution to the axe \\(C_i\\)\nThe total inertia on component \\(1\\) is \\(\\sigma^2_i = \\sum_{i=1}^n (XQ)_{i1}^2\\).\nIndividual \\(i\\) contributes for \\((XQ)_{i1}^2/\\sigma^2_i\\)"
  },
  {
    "objectID": "pca.html#practically-speaking",
    "href": "pca.html#practically-speaking",
    "title": "Principal Component Analysis",
    "section": "Practically speaking",
    "text": "Practically speaking\nIt is way simpler !!!\nYou can have a look at the book Husson, Lê, and Pagès [HLP11] and visit François Husson MOOC, author of the FactoMineR package.\n\n\n\n# install.packages('FactoMineR')\n# install.packages('FactoShiny')\nlibrary(FactoMineR)\n\npenguins_pca &lt;- PCA(X, ncp = ncol(X), graph = FALSE)\n\npenguins_pca$eig\n\npenguins_pca$var\n\npenguins_pca$ind\n\nplot(penguins_pca, axes = c(1,2), choix = \"ind\")\n\nplot(penguins_pca, axes = c(3,4) , choix = \"var\")\n\n\n\nlibrary(Factoshiny)\nPCAshiny(penguins)"
  },
  {
    "objectID": "pca.html#the-doubs-river-example",
    "href": "pca.html#the-doubs-river-example",
    "title": "Principal Component Analysis",
    "section": "The Doubs River example",
    "text": "The Doubs River example\n\nPCAshiny(doubs.env)"
  },
  {
    "objectID": "regression.html#lecture-objectives",
    "href": "regression.html#lecture-objectives",
    "title": "Linear Regression",
    "section": "Lecture objectives",
    "text": "Lecture objectives\n\nPresent the classical statistical version of linear model ( a bit different from the machine learning approach)\nEquivalence between Ordinary Least square estimation and normal likelihood estimation\nA reminder about statistical testing and an overview of the classical test in linear modelling\nSome diagnostic regarding the assumptions\nA tutorial with Python"
  },
  {
    "objectID": "regression.html#regression-analysis-context",
    "href": "regression.html#regression-analysis-context",
    "title": "Linear Regression",
    "section": "Regression analysis context",
    "text": "Regression analysis context\nFocus on a quantitative variable \\(Y\\) and interesting in\n\nunderstanding the relationship with this variables and other variables of interest, named covariates (explanatory variables, regressors, features),\nselecting the meaningful covariates,\npredicting the value of \\(Y\\) given the values of the covariates.\n\nExamples\n\nEconomic: \\(Y\\) is the inflation rate and the covariates are interest rates, unemployment rates, and government spending.\nMarketing: \\(Y\\) is the value of a customer purchase, explained by age, salary, marketting campaigns exposure, …\nEnvironmental Science: \\(Y\\) is the daily Ozone concentration at a given site which might depend on industrial activity, population density, and meteorological factors.\nAgriculture: \\(Y\\) is the crop yields, which might be explained by soil quality, rainfall, previous production, the quantity of fertilizers, .."
  },
  {
    "objectID": "regression.html#multiple-linear-regression-model",
    "href": "regression.html#multiple-linear-regression-model",
    "title": "Linear Regression",
    "section": "Multiple Linear Regression Model",
    "text": "Multiple Linear Regression Model\n\\[Y_{k} = \\beta_0 +\\beta_1 x_{k}^{1}  + \\beta_2 x_{k}^{2} + \\ldots +  \\beta_p x_{k}^{p}  +  E_{k},\\quad E_{k}\\overset{ind}{\\sim}\\mathcal{N}(0, \\sigma^2),\\]\nWhere: - \\(x_{k}^{l}\\) is the value of explanatory variable \\(l\\) for observation \\(k\\), - \\(k=1,\\ldots,n\\) is the index of the individual, and \\(n\\) is the total number of individuals, - \\(\\beta_0\\) is the intercept, - \\(\\beta_l\\) represents the effect of variable \\(X^{l}\\) on the dependent variable, - \\(\\sigma^2\\) is the variance.\nEquivalent Notation\n\\[Y_{k} \\overset{ind}{\\sim}\\mathcal{N}(\\beta_0 +\\beta_1 x_{k}^{1}  + \\beta_2 x_{k}^{2} + \\ldots +  \\beta_p x_{k}^{p} , \\sigma^2).\\]\nNumber of Parameters\n\n\\(p+1\\) parameters for the mean \\((\\beta_0, \\beta_1, \\ldots, \\beta_p)\\),\n1 parameter for the variance \\(\\sigma^2\\) (the noise)."
  },
  {
    "objectID": "regression.html#sous-forme-matricielle",
    "href": "regression.html#sous-forme-matricielle",
    "title": "Linear Regression",
    "section": "Sous forme matricielle",
    "text": "Sous forme matricielle\n\\[\\bf{Y = X\\beta + E}\\]\n\\[Y=\\begin{pmatrix}\nY_{1}\\\\\nY_{2}\\\\\n\\vdots\\\\\nY_{k}\\\\\n\\vdots\\\\\nY_{n}\\end{pmatrix},\n\\quad\n{\\bf{X}} ={\\begin{pmatrix}\n1 & x_1^{1} & x_1^{2} & \\ldots &x_1^{l}\\\\\n1 & x_2^{1} & x_2^{2} & \\ldots &x_2^{l}\\\\\n\\vdots & \\vdots& \\vdots && \\vdots\\\\\n1 & x_k^{1} & x_k^{2} & \\ldots &x_k^{l}\\\\\n\\vdots & \\vdots& \\vdots && \\vdots\\\\\n1 & x_n^{1} & x_n^{2} & \\ldots &x_n^{l}\\\\\n\\end{pmatrix}},\\quad\n{\\bf{\\beta}} =\\begin{pmatrix}\n\\beta_0\\\\\n\\beta_1\\\\\n\\beta_2\\\\\n\\vdots\\\\\n\\beta_l\\\\\n\\end{pmatrix}, \\quad{\\bf{E}} = \\overset{}{\\begin{pmatrix}\nE_{1}\\\\\nE_{2}\\\\\n\\vdots\\\\\nE_{k}\\\\\n\\vdots\\\\\nE_{n}\\\\\n\\end{pmatrix}}.\\]"
  },
  {
    "objectID": "regression.html#python-requirement",
    "href": "regression.html#python-requirement",
    "title": "Linear Regression",
    "section": "Python requirement",
    "text": "Python requirement\n\nconda create --name topicsinstats python=3.11 pip \nconda activate topicsinstats\npip install -r requirements.txt"
  },
  {
    "objectID": "regression.html#palmers-penuins-companion-data-set",
    "href": "regression.html#palmers-penuins-companion-data-set",
    "title": "Linear Regression",
    "section": "Palmers penuins companion Data set",
    "text": "Palmers penuins companion Data set\nJust go back to our dear penguins. We want to explain the flipper length by the body mass and the bill length.\nWrite the first rows of \\(Y\\), \\(X\\) and specify the dimension of the different objects\n\nData presentationSolutionSolution with Python\n\n\n\n\nimport pandas as pd\nimport seaborn as sns \nfrom palmerpenguins import load_penguins\nimport statsmodels.api as sm\n\npenguins = load_penguins() # load dataset\npenguins = penguins.dropna() # drop na\npenguins[['flipper_length_mm', 'body_mass_g', 'bill_length_mm']] # show first rows\n\n\n     flipper_length_mm  body_mass_g  bill_length_mm\n0                181.0       3750.0            39.1\n1                186.0       3800.0            39.5\n2                195.0       3250.0            40.3\n4                193.0       3450.0            36.7\n5                190.0       3650.0            39.3\n..                 ...          ...             ...\n339              207.0       4000.0            55.8\n340              202.0       3400.0            43.5\n341              193.0       3775.0            49.6\n342              210.0       4100.0            50.8\n343              198.0       3775.0            50.2\n\n[333 rows x 3 columns]\n\n\n\n\n\\[ Y = \\begin{pmatrix}\n181 \\\\\n186 \\\\\n195 \\\\\n193 \\\\\n190 \\\\\n  \\vdots\n   \\end{pmatrix}, \\quad\n   X = \\begin{pmatrix}\n   1 & 3750 & 39.10 \\\\\n   1 & 3800 & 39.50 \\\\\n   1 & 3250 & 40.30 \\\\\n   1 & 3450 & 36.70 \\\\\n   1 & 3650 & 39.30 \\\\  \n    & \\vdots&\n    \\end{pmatrix}, \\quad \\beta = \\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\end{pmatrix}.\n\\]\n\n\n\n\nY = penguins['flipper_length_mm']  # regressor / features      \nX = penguins[['body_mass_g', 'bill_length_mm']]   # regressor / features      \nX.head()\nprint(\"\\n\")\n\nX = sm.add_constant(X)          # Add the constant to the regressor\nX.head()\n\n\n   body_mass_g  bill_length_mm\n0       3750.0            39.1\n1       3800.0            39.5\n2       3250.0            40.3\n4       3450.0            36.7\n5       3650.0            39.3\n\n   const  body_mass_g  bill_length_mm\n0    1.0       3750.0            39.1\n1    1.0       3800.0            39.5\n2    1.0       3250.0            40.3\n4    1.0       3450.0            36.7\n5    1.0       3650.0            39.3"
  },
  {
    "objectID": "regression.html#quick-reminder-on-gaussian-multivariate-random-vector",
    "href": "regression.html#quick-reminder-on-gaussian-multivariate-random-vector",
    "title": "Linear Regression",
    "section": "Quick Reminder on Gaussian Multivariate Random Vector",
    "text": "Quick Reminder on Gaussian Multivariate Random Vector\nA Gaussian Multivariate Random Vector \\(\\mathbf{U} \\in \\mathbb{R}^n\\) is defined by the following properties:\n\nMean Vector \\(\\boldsymbol{\\mu} = \\mathbb{E}[\\mathbf{U}] \\in \\mathbb{R}^n\\):\n\\[\\boldsymbol{\\mu} = \\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\vdots \\\\\n\\mu_n\n\\end{bmatrix}\\]\nCovariance Matrix \\(\\boldsymbol{\\Sigma} = \\mathbb{E}[(\\mathbf{U} - \\boldsymbol{\\mu})(\\mathbf{U} - \\boldsymbol{\\mu})^\\top] \\in \\mathbb{R}^{n \\times n}\\):\n\\[\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1n} \\\\\n\\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{n1} & \\sigma_{n2} & \\dots & \\sigma_{nn}\n\\end{bmatrix}\\]\nProbability Density Function (PDF) for \\(\\mathbf{U}\\): \\[f(\\mathbf{U}) = \\frac{1}{(2\\pi)^{n/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{U} - \\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{U} - \\boldsymbol{\\mu})\\right)\\]"
  },
  {
    "objectID": "regression.html#key-properties",
    "href": "regression.html#key-properties",
    "title": "Linear Regression",
    "section": "Key Properties",
    "text": "Key Properties\n\nMarginals: Each component \\(X_i\\) of \\(\\mathbf{X}\\) is normally distributed: \\(X_i \\sim \\mathcal{N}(\\mu_i, \\sigma_{ii})\\).\nIndependence: If the covariance matrix \\(\\boldsymbol{\\Sigma}\\) is diagonal, then all components are independent.\nLinear Combinations: Any linear combination of the components of a Gaussian vector is also Gaussian.\n\n\\[c^\\intercal \\mathbf{U}  \\sim \\mathcal{N} \\left ( c^\\intercal \\mathbb{E}(\\mathbf{U}), c^\\intercal \\mathbb{\\Sigma} c \\right).\\]"
  },
  {
    "objectID": "regression.html#cochrans-theorem",
    "href": "regression.html#cochrans-theorem",
    "title": "Linear Regression",
    "section": "Cochran’s Theorem",
    "text": "Cochran’s Theorem\n\n\n\nTheorem\n\n\nLet \\(\\mathbf{X} \\in \\mathbb{R}^n\\) be a multivariate Gaussian vector such that:\n\\[\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_n),\\] let \\(Q_1, \\cdots, Q_k\\) be quadratic forms with ranks \\(r_1,\\cdots,r_k\\) and suppose the \\(Q_1 +\\cdots Q_k= X^\\intercal X,\\)\nThen \\(Q_1, \\cdots, Q_k\\) are independently distributed as \\(\\chi^2\\) with \\(r_1, \\cdots, r_k\\) degrees of freedom respectively if and only if \\(\\sum_{i=1}^k r_i =n\\)\n\n\n\nUseful Application\nConsider \\(P\\) the projector on some subspace \\(V \\subset \\R^n\\) of dimension \\(p\\), so that \\(P=P^\\intercal\\) and \\(P^2=P\\). * \\((X-PX) \\perp PX\\), since \\(P (X-PX) = PX - P^2 X = 0,\\) * \\[\\begin{align}\nX^\\intercal X & = \\left ((X-PX) + PX \\right)^\\intercal  \\left ((X-PX) + PX \\right) = (X-PX)^\\intercal(X-PX) +  (PX)^\\intercal (PX) +  (X-PX)^\\intercal  PX + (PX)^\\intercal (X-PX)\\cr\n  & =(X-PX)^\\intercal(X-PX) +  (PX)^\\intercal (PX),\n  \\end{align}\\] and \\(rank(P^\\intercal P)=p\\) \\(rank((I-P)^\\intercal (I-P))=n-p,\\) so that \\[\\norm{PX}^2 \\sim \\chi^2(p), \\norm{X-PX}^2 \\sim \\chi^2(n-p),  \\quad \\norm{PX}^2 \\indep  \\norm{X-PX}^2.\\]"
  },
  {
    "objectID": "regression.html#parameter-estimation",
    "href": "regression.html#parameter-estimation",
    "title": "Linear Regression",
    "section": "Parameter estimation",
    "text": "Parameter estimation\nHow to pick a good value for \\(\\beta\\) in\n\\[Y = X \\beta + E\\]\n\nIntuitivelyGeometrical point of view: OLSStatistical point of view\n\n\n\\[\\begin{align}\n\\mathbb{E}(Y) &= X\\beta \\cr\nX^\\intercal \\mathbb{E}(Y) &= X^\\intercal X\\beta \\mbox{ if } X \\mbox {is full rank} \\cr\n\\left( X^\\intercal X\\right)^{-1} X^\\intercal \\mathbb{E}(Y) &= \\beta \\cr\n\\end{align}\\]\nA good guess would be \\(\\hat{\\beta} = \\left( X^\\intercal X\\right)^{-1} X^\\intercal Y\\)\n\n\n\nFind the \\(\\hat{\\beta}\\) so that \\(X\\hat{\\beta}\\) is as close as possible from \\(Y\\), i.e\nFind the element of \\(Im(X)\\) as close as possible from \\(Y\\),\nConsider the orthogonal projection on \\(Im(X)\\) \\(X\\hat{\\beta},\\)\n\nThen \\(Y - X\\hat{\\beta}\\) is orthogonal to any vector of \\(Im(X)= Xv,\\) \\[\\begin{align}\n\\forall v\\in \\R^n, \\quad &lt;Xv, Y-X\\hat{\\beta}&gt; & = 0\\cr\n\\forall v\\in \\R^n, \\quad v^\\intercal X^\\intercal (Y-X\\hat{\\beta}) & = 0\\cr\n\\forall v\\in \\R^n, \\quad v^\\intercal X^\\intercal Y & = v^\\intercal X^\\intercal X\\hat{\\beta}\\cr\nX^\\intercal Y & = X^\\intercal X\\hat{\\beta}\\cr\n\\left(  X^\\intercal X\\right)^{-1} X^\\intercal Y & = \\hat{\\beta}  \\mbox{ if } X \\mbox {is full rank} \\cr\n\\end{align}\\]\nOrdinary Least Square estimation: \\(\\hat{\\beta}\\), so that \\(\\norm{Y-X\\hat{\\beta}}^2\\) is minimal.\n\n\nThe log-likelihood of the model \\(\\ell(\\beta, \\sigma^2; Y)\\) is given by\n\\[\\ell(\\beta, \\sigma^2; Y)  = Cte - \\frac{n}{2} \\log{\\sigma^2}  -\\frac{1}{2\\sigma^2} (\\mathbf{Y} - X\\beta)^\\intercal   (\\mathbf{Y} - X\\beta).\\] By maximizing the log-likelihood and after a matrix derivation, we get maximum likelihood estimator (MLE) \\(\\hat{\\beta}\\) is solution to \\[X^\\intercal Y = X^\\intercal X \\hat{\\beta}.\\]\nThe MLE for \\(\\sigma^2\\) is \\[\\widehat{\\sigma^2}_{MLE} = \\frac{1}{n} (\\mathbf{Y} - X\\hat{\\beta})^\\intercal   (\\mathbf{Y} - X\\hat{\\beta})\\]"
  },
  {
    "objectID": "regression.html#estimators-properties",
    "href": "regression.html#estimators-properties",
    "title": "Linear Regression",
    "section": "Estimators properties",
    "text": "Estimators properties\n\n\\(\\hat{\\beta}\\) properties\\(\\widehat{\\sigma^2}_{MLE}\\) properties\n\n\nAs \\(\\hat{\\beta} =  \\left(X^\\intercal X\\right)^{-1} X^\\intercal Y\\) is a linear combination of a Gaussian vector, \\[\\hat{\\beta} \\sim\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} ),\\] with\n\n\\(\\boldsymbol{\\mu} = \\left(X^\\intercal X\\right)^{-1} X^\\intercal \\mathbb{E}\\left(Y\\right)=\\left(X^\\intercal X\\right)^{-1} X^\\intercal X \\beta = \\beta\\),\nand \\(\\boldsymbol{\\Sigma} = \\left(X^\\intercal X\\right)^{-1} X^\\intercal \\sigma^2 I_n  \\left( \\left(X^\\intercal X\\right)^{-1} X^\\intercal\\right)^\\intercal = \\sigma^2 \\left(X^\\intercal X\\right)^{-1}.\\)\n\n\\(\\mathbb{E}{\\hat{\\beta}}=\\beta\\), the estimator is unbiased.\n\n\nRecall that \\(X\\hat{\\beta}\\) is the the orthogonal projection on \\(Im(X)\\), so that the projector \\(P\\) is defined by \\(P = X\\hat{\\beta}=X \\left(X^\\intercal X\\right)^{-1} X^\\intercal.\\)\nAnd \\(\\norm{Y-X\\theta}^2 =  \\norm{Y-PY}^2 + \\norm{PY-X\\beta}^2,\\) following Cochran theorem\n\\[\\frac{1}{\\sigma^2}\\norm{Y-PY}^2\\sim\\chi^2(n-rank(X)), \\quad \\frac{1}{\\sigma^2}\\norm{PY-X\\theta}^2\\sim\\chi^2(rank(X)), \\quad \\norm{Y-PY}^2\\indep \\norm{PY-X\\theta}^2.\\] As \\(\\mathbb{E}\\left(\\frac{1}{n} \\norm{Y-PY}^2 \\right) = \\frac{n-rank(X)}{n}\\) of \\(\\sigma^2\\), the MLE estimator is biased\nWe prefer the unbiased version \\[\\widehat{\\sigma}^2 = \\frac{1}{n-rank(X)} \\norm{Y-PY}^2.\\]"
  },
  {
    "objectID": "regression.html#illustrationon-the-palmer-penguins",
    "href": "regression.html#illustrationon-the-palmer-penguins",
    "title": "Linear Regression",
    "section": "Illustrationon the Palmer penguins",
    "text": "Illustrationon the Palmer penguins\n\n\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.stats.anova import anova_lm\n\nn = X.shape[0]\n\n# linear regression model\nlm1 = sm.OLS(Y, X)  # OLS: Ordinary Least Squares (Moindres Carrés Ordinaires)\nfit1 = lm1.fit()    # fit the model\nprint(fit1.summary())\n\nX_numpy = X.to_numpy()\nY_numpy = Y.to_numpy()\np = np.linalg.matrix_rank(X_numpy)\n\nXT_X = X_numpy.T.dot(X_numpy)\nXT_X_inv = np.linalg.inv(XT_X)\nhat_beta = XT_X_inv.dot(X_numpy.T.dot(Y_numpy))\nprint(hat_beta)\n\nP = X_numpy.dot(XT_X_inv.dot(X_numpy.T))\nR = Y_numpy - P.dot(Y_numpy)\nprint(R[range(0,5)])\n## to compare with \nfit1.resid\n\nhat_sigma2 = R.T.dot(R)/(n-p)\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:      flipper_length_mm   R-squared:                       0.791\nModel:                            OLS   Adj. R-squared:                  0.790\nMethod:                 Least Squares   F-statistic:                     626.3\nDate:                Tue, 09 Sep 2025   Prob (F-statistic):          4.51e-113\nTime:                        09:44:58   Log-Likelihood:                -1090.1\nNo. Observations:                 333   AIC:                             2186.\nDf Residuals:                     330   BIC:                             2198.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst            122.2687      2.864     42.697      0.000     116.635     127.902\nbody_mass_g        0.0130      0.001     24.034      0.000       0.012       0.014\nbill_length_mm     0.5440      0.080      6.822      0.000       0.387       0.701\n==============================================================================\nOmnibus:                        8.498   Durbin-Watson:                   1.854\nProb(Omnibus):                  0.014   Jarque-Bera (JB):                8.830\nSkew:                          -0.393   Prob(JB):                       0.0121\nKurtosis:                       2.867   Cond. No.                     3.49e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.49e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n[1.22268670e+02 1.30173337e-02 5.44035274e-01]\n[-11.35545088  -7.22393167   8.50037362   5.85543388  -1.16252457]\n0     -11.355451\n1      -7.223932\n2       8.500374\n4       5.855434\n5      -1.162525\n         ...    \n339     2.304827\n340    11.806861\n341    -5.393255\n342     6.723270\n343    -0.719676\nLength: 333, dtype: float64"
  },
  {
    "objectID": "regression.html#test-on-a-linear-combination-of-parameter",
    "href": "regression.html#test-on-a-linear-combination-of-parameter",
    "title": "Linear Regression",
    "section": "Test on a linear combination of parameter",
    "text": "Test on a linear combination of parameter\nTo test whether or not a linear combination \\(c\\) of parameters equals \\(0\\), we would like to decide between two assumptions \\[H_0 = \\left \\lbrace c^\\intercal \\beta = 0\\right\\rbrace\\quad \\mbox{vs} \\quad H_1 = \\left \\lbrace c^\\intercal \\beta \\ne 0\\right\\rbrace.\\]\nSince \\(c^\\intercal \\hat{\\beta} \\sim \\mathcal{N}\\left(c^\\intercal \\beta, \\sigma^2 c^\\intercal \\left(X^\\intercal X\\right)^{-1} c\\right)\\) and \\(\\sigma^{-2}\\norm{Y-PY}^2 \\sim \\chi^2(n-p),\\) then\n\\[\\sqrt{n-p}\\frac{c^\\intercal\\hat{\\beta} - c^\\intercal{\\beta}}{\\norm{Y-PY} \\sqrt{c^\\intercal \\left(X^\\intercal X\\right)^{-1} c}} \\sim \\mathcal{T}(n-p).\\] So that under assumption \\(H_0\\) \\[\\sqrt{n-p}\\frac{c^\\intercal\\hat{\\beta}}{\\norm{Y-PY} \\sqrt{c^\\intercal \\left(X^\\intercal X\\right)^{-1} c}} \\sim \\mathcal{T}(n-p).\\]\n\nExerciceSolution\n\n\nDescribe this test when \\(p=3\\) and \\(c = (0, 0, 1)^\\intercal\\)\n\n\n\n\\(c^\\intercal \\beta = \\beta_2\\)\nUnder assumption \\(H_0 = \\left\\lbrace \\beta_2=0 \\right\\rbrace\\) \\[\\frac{\\hat{\\beta}}{ \\sqrt{ (n-p) \\widehat{\\sigma^2}  \\left(X^\\intercal X\\right)^{-1}_{33} }} \\sim \\mathcal{T}(n-p).\\]"
  },
  {
    "objectID": "regression.html#nested-model-test",
    "href": "regression.html#nested-model-test",
    "title": "Linear Regression",
    "section": "Nested model test",
    "text": "Nested model test\nA model \\(M_1\\) is said to be nested within a more complex model \\(M_2\\) if all the parameters in \\(M_1\\) are also present in \\(M_2\\), but \\(M_2\\) has additional parameters that \\(M_1\\).\n\nExampleComparing nested modelsPython example\n\n\n\n\\(M2:\\quad  Y_k = \\beta_0 + \\beta_1 x_{k1} + \\beta_2 x_{k2} + E_k\\) and \\(M1:\\quad  Y_k = \\beta_0 + \\beta_1 x_{k1} +  E_k\\)\n\nBut also\n\n\\(M_2:\\quad  Y_k = \\beta_0 + \\beta_1 x_{k1} + \\beta_2 x_{k2} + E_k\\) and \\(M_1:\\quad  Y_k = \\beta_0 + \\beta_1 (x_{k1} + x_{k2})  E_k\\)\n\n\n\nIf \\(M_1\\) and \\(M_2\\) are two linear models, with \\(M_1\\) nested in \\(M_2\\).\n\\[M_1:\\quad Y = X_1 \\beta^1 + E^1; \\quad \\quad M_2:\\quad Y = X_2 \\beta^2 + E^2;\\] with \\(Im(X_1) \\subset Im(X_2).\\)\nApplying Cochran theorem, under \\(H_0=\\left\\lbrace X_1\\beta^1 =X_2 \\beta^2\\right\\rbrace,\\)\n\\(\\sigma^{-2}\\norm{Y - P_2 Y}^2 \\sim \\chi^2\\left(n-rank(X_2)\\right)\\), \\(\\sigma^{-2}\\norm{P_2 Y - P_1 Y }^2 \\sim \\chi^2(rank(X_2)-rank(X_1))\\) and \\(\\norm{P_2 Y - P_1 Y }^2 \\indep \\norm{Y- P_2 Y}^2.\\)\nUnder \\(H_0=\\left\\lbrace X_1\\beta^1 =X_2 \\beta^2\\right\\rbrace,\\) \\[\\frac{\\frac{\\norm{P_2 Y - P_1 Y}^2}{p_2-p_1}}{\\frac{\\norm{Y- P_2 Y}^2}{n-p_2}}\\sim \\mathcal{F}_{p_2-p_1, n- p_2}\\]\n\n\n\n\n## fit the null model \nX0 = X[['const']]\nlm0 = sm.OLS(Y, X0)  # OLS: Ordinary Least Squares (Moindres Carrés Ordinaires)\nfit0 = lm0.fit()    # fit the model\n\nanova_results = anova_lm(fit0, fit1)\nprint(anova_results)\n\n\n   df_resid           ssr  df_diff       ss_diff           F         Pr(&gt;F)\n0     332.0  65218.636637      0.0           NaN         NaN            NaN\n1     330.0  13598.383598      2.0  51620.253039  626.349572  4.508888e-113"
  },
  {
    "objectID": "regression.html#what-about-categorical-variables",
    "href": "regression.html#what-about-categorical-variables",
    "title": "Linear Regression",
    "section": "What about categorical variables ?",
    "text": "What about categorical variables ?\nDoes the flipper length varies between species?\nThe model\n\\[Y_{sk} = \\mu + \\alpha_s + E_{sk},\\] where \\(s\\) stands for the species.\n\nModelIdentifiabilityEffect of one factorPractical\n\n\n\nTransform into dummies variables:\n\n\n\npenguins_dummies = pd.get_dummies(penguins, \ncolumns=['species'],  dtype=\"int\")\nprint(penguins_dummies.iloc[:, [1] + list(range(7,10))].head())\n\n\n   bill_length_mm  species_Adelie  species_Chinstrap  species_Gentoo\n0            39.1               1                  0               0\n1            39.5               1                  0               0\n2            40.3               1                  0               0\n4            36.7               1                  0               0\n5            39.3               1                  0               0\n\n\n\nWhat is the rank of the so define design matrix ?\n\n\n\n\n\npenguins_dummies_c = pd.get_dummies(penguins,\ncolumns=['species'],  dtype=\"int\", drop_first = True)\npenguins_dummies_c.iloc[:, [1] + list(range(7,9))].head()\n\n\n   bill_length_mm  species_Chinstrap  species_Gentoo\n0            39.1                  0               0\n1            39.5                  0               0\n2            40.3                  0               0\n4            36.7                  0               0\n5            39.3                  0               0\n\n\nBe very careful in the interpretation\n\n\n\n\nX_unid = penguins_dummies[['species_Adelie', 'species_Gentoo', 'species_Chinstrap']]\nX_unid = sm.add_constant(X_unid)          # Add the constant to the regressor\np_unid = np.linalg.matrix_rank(X_unid)\nprint(p_unid)\n\n\nX = penguins_dummies_c[[ 'species_Gentoo', 'species_Chinstrap']]\nX = sm.add_constant(X)          # Add the constant to the regressor\np = np.linalg.matrix_rank(X)\nprint(p)\n\nY = penguins_dummies['flipper_length_mm']\nX_unid = X_unid[['const', 'species_Adelie', 'species_Chinstrap', 'species_Gentoo']]\n# linear regression model\nlm1 = sm.OLS(Y, X_unid)  # OLS: Ordinary Least Squares (Moindres Carrés Ordinaires)\nfit1 = lm1.fit()    # fit the model\nprint(fit1.summary())\n\n\n3\n3\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:      flipper_length_mm   R-squared:                       0.775\nModel:                            OLS   Adj. R-squared:                  0.773\nMethod:                 Least Squares   F-statistic:                     567.4\nDate:                Tue, 09 Sep 2025   Prob (F-statistic):          1.59e-107\nTime:                        09:44:59   Log-Likelihood:                -1103.0\nNo. Observations:                 333   AIC:                             2212.\nDf Residuals:                     330   BIC:                             2223.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=====================================================================================\n                        coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nconst               150.7904      0.289    522.250      0.000     150.222     151.358\nspecies_Adelie       39.3123      0.486     80.950      0.000      38.357      40.268\nspecies_Chinstrap    45.0331      0.641     70.266      0.000      43.772      46.294\nspecies_Gentoo       66.4449      0.520    127.769      0.000      65.422      67.468\n==============================================================================\nOmnibus:                        1.423   Durbin-Watson:                   2.268\nProb(Omnibus):                  0.491   Jarque-Bera (JB):                1.470\nSkew:                           0.156   Prob(JB):                        0.479\nKurtosis:                       2.904   Cond. No.                     2.10e+15\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 1.03e-28. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\n\n\n\n\n                  sum_sq     df           F         PR(&gt;F)\nC(species)  50525.883615    2.0  567.406992  1.587418e-107\nResidual    14692.753022  330.0         NaN            NaN\n\n\n\n\n\nQuality of the estimation"
  },
  {
    "objectID": "regression.html#condidence-interval",
    "href": "regression.html#condidence-interval",
    "title": "Linear Regression",
    "section": "Condidence interval",
    "text": "Condidence interval\n\\[\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^2 \\left ( X^\\intercal X\\right )^{-1} ),\\]\nSo that \\[  \\sigma^{-1} \\left ( X^\\intercal X\\right )^{1/2} (\\hat{\\beta}-\\beta) \\sim \\mathcal{N}(0, I_p)\\]"
  },
  {
    "objectID": "regression.html#prediction-interval",
    "href": "regression.html#prediction-interval",
    "title": "Linear Regression",
    "section": "Prediction interval",
    "text": "Prediction interval"
  },
  {
    "objectID": "overshoot_regression.html",
    "href": "overshoot_regression.html",
    "title": "How the overshoot day is it linked to life expectancy, human dvp index, per_capita, population, production and consumption ?",
    "section": "",
    "text": "import pandas as pd\nimport statsmodels.formula.api as smf\n\n# Lire le fichier CSV\ndta = pd.read_csv('/home/metienne/git/TopicsInStatistics/data/overshootday_overview.csv')\ndta = dta.dropna()\n# Summary for numerical variables\nprint(dta.describe())\n\n       life_expectancy         hdi  per_capita_gdp          pop  total_prod  \\\ncount       167.000000  167.000000      167.000000   167.000000  167.000000   \nmean         72.531798    0.722108    14134.070787    44.491325    3.056235   \nstd           7.484902    0.150773    19763.533118   158.168844    2.615103   \nmin          52.805000    0.391000      210.804080     0.096286    0.300103   \n25%          67.475500    0.599000     1779.638788     3.060985    1.253923   \n50%          74.131000    0.746000     5471.835506     9.630960    1.985990   \n75%          77.617700    0.839000    16194.654522    31.758650    4.240380   \nmax          84.211000    0.956000   111380.649478  1459.380000   12.055943   \n\n       total_cons  number_of_countries_required  number_of_earths_required  \\\ncount  167.000000                    167.000000                 167.000000   \nmean     3.227056                      3.702138                   2.039055   \nstd      2.308365                      8.888881                   1.458568   \nmin      0.508542                      0.038129                   0.321328   \n25%      1.542461                      0.893073                   0.974622   \n50%      2.422773                      1.829049                   1.530858   \n75%      4.486365                      3.018712                   2.834764   \nmax     14.270954                    104.616270                   9.017275   \n\n       overshoot_day  \ncount     167.000000  \nmean      235.041916  \nstd       110.559658  \nmin        40.000000  \n25%       128.000000  \n50%       238.000000  \n75%       365.000000  \nmax       365.000000  \nregion\nAfrica                       46\nAsia-Pacific                 28\nEU                           27\nMiddle East/Central Asia     23\nCentral America/Caribbean    17\nOther Europe                 12\nSouth America                11\nNorth America                 3\nName: count, dtype: int64\nincome_group\nHI    47\nUM    44\nLM    44\nLI    32\nName: count, dtype: int64\n# Summary for cetegorical variables\nprint(dta['region'].value_counts())\nprint(dta['income_group'].value_counts())\n\nregion\nAfrica                       46\nAsia-Pacific                 28\nEU                           27\nMiddle East/Central Asia     23\nCentral America/Caribbean    17\nOther Europe                 12\nSouth America                11\nNorth America                 3\nName: count, dtype: int64\nincome_group\nHI    47\nUM    44\nLM    44\nLI    32\nName: count, dtype: int64\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# correlations\n# numerical columns \nnumeric_columns = dta.select_dtypes(include=['float64', 'int64']).columns\n\n# Calcul des corrélations uniquement sur les colonnes numériques\ncorr_matrix = dta[numeric_columns].corr()\n\n# Affichage de la heatmap des corrélations\nplt.figure(figsize=(10,8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.show()"
  },
  {
    "objectID": "overshoot_regression.html#does-this-reation-change-according-to-the-world-region",
    "href": "overshoot_regression.html#does-this-reation-change-according-to-the-world-region",
    "title": "How the overshoot day is it linked to life expectancy, human dvp index, per_capita, population, production and consumption ?",
    "section": "Does this reation change according to the world region ?",
    "text": "Does this reation change according to the world region ?\n\nm1 = smf.ols(formula='overshoot_day ~ life_expectancy + hdi + per_capita_gdp + pop + total_prod + total_cons ', data=dta)\n\n# model ajust\nresults_m1 = m1.fit()\n\n# Affichage des résultats de la régression\nprint(results_m1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          overshoot_day   R-squared:                       0.888\nModel:                            OLS   Adj. R-squared:                  0.884\nMethod:                 Least Squares   F-statistic:                     212.2\nDate:                Sun, 15 Sep 2024   Prob (F-statistic):           1.76e-73\nTime:                        22:49:17   Log-Likelihood:                -839.22\nNo. Observations:                 167   AIC:                             1692.\nDf Residuals:                     160   BIC:                             1714.\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n===================================================================================\n                      coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nIntercept         476.3372     42.542     11.197      0.000     392.322     560.353\nlife_expectancy     2.2914      0.992      2.309      0.022       0.332       4.251\nhdi              -436.3195     56.374     -7.740      0.000    -547.652    -324.987\nper_capita_gdp      0.0008      0.000      3.271      0.001       0.000       0.001\npop                 0.0028      0.019      0.150      0.881      -0.034       0.039\ntotal_prod          0.5293      1.887      0.280      0.780      -3.198       4.257\ntotal_cons        -32.6661      2.698    -12.105      0.000     -37.995     -27.337\n==============================================================================\nOmnibus:                       27.181   Durbin-Watson:                   0.985\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               39.308\nSkew:                           0.902   Prob(JB):                     2.91e-09\nKurtosis:                       4.547   Cond. No.                     5.25e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.25e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\nfrom statsmodels.stats.anova import anova_lm\n\nm2 = smf.ols(formula='overshoot_day ~ life_expectancy + hdi + per_capita_gdp + total_cons ', data=dta)\nresults_m2 = m2.fit()\n\n\nm1 = smf.ols(formula='overshoot_day ~ life_expectancy + hdi + per_capita_gdp + pop + total_prod + total_cons ', data=dta)\n# model ajust\nresults_m1 = m1.fit()\n\n\nm2 = smf.ols(formula='overshoot_day ~ life_expectancy + hdi + per_capita_gdp + total_cons ', data=dta)\nresults_m2 = m2.fit()\n\n## compare m1 and m2\nanova_results = anova_lm(results_m2, results_m1)\nprint(anova_results)\n\n   df_resid            ssr  df_diff    ss_diff         F   Pr(&gt;F)\n0     162.0  226676.176959      0.0        NaN       NaN      NaN\n1     160.0  226532.705100      2.0  143.47186  0.050667  0.95061\n\n\n\nprint(results_m2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          overshoot_day   R-squared:                       0.888\nModel:                            OLS   Adj. R-squared:                  0.886\nMethod:                 Least Squares   F-statistic:                     322.0\nDate:                Sun, 15 Sep 2024   Prob (F-statistic):           5.75e-76\nTime:                        22:55:18   Log-Likelihood:                -839.27\nNo. Observations:                 167   AIC:                             1689.\nDf Residuals:                     162   BIC:                             1704.\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n===================================================================================\n                      coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nIntercept         476.8718     42.177     11.306      0.000     393.583     560.160\nlife_expectancy     2.2697      0.980      2.317      0.022       0.335       4.204\nhdi              -434.4145     55.536     -7.822      0.000    -544.082    -324.747\nper_capita_gdp      0.0008      0.000      3.277      0.001       0.000       0.001\ntotal_cons        -32.2095      2.110    -15.262      0.000     -36.377     -28.042\n==============================================================================\nOmnibus:                       26.859   Durbin-Watson:                   0.983\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               38.476\nSkew:                           0.898   Prob(JB):                     4.42e-09\nKurtosis:                       4.518   Cond. No.                     5.21e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.21e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n# Getting the covariance matrix of the parameter estimates\ncov_matrix = results_m1.cov_params()\nprint(cov_matrix)\n\n                   Intercept  life_expectancy          hdi  per_capita_gdp  \\\nIntercept        1809.795372       -38.053115  1307.662264    2.399071e-03   \nlife_expectancy   -38.053115         0.984492   -47.442516   -1.224311e-05   \nhdi              1307.662264       -47.442516  3177.984385   -2.098635e-03   \nper_capita_gdp      0.002399        -0.000012    -0.002099    5.922858e-08   \npop                 0.028414        -0.000912     0.023378    1.128577e-07   \ntotal_prod         -5.144821         0.195591   -14.080334    2.688437e-05   \ntotal_cons         -1.542778         0.161323   -27.815941   -2.850716e-04   \n\n                          pop  total_prod  total_cons  \nIntercept        2.841366e-02   -5.144821   -1.542778  \nlife_expectancy -9.124414e-04    0.195591    0.161323  \nhdi              2.337776e-02  -14.080334  -27.815941  \nper_capita_gdp   1.128577e-07    0.000027   -0.000285  \npop              3.432641e-04   -0.000096    0.001336  \ntotal_prod      -9.620714e-05    3.562455   -3.141453  \ntotal_cons       1.336426e-03   -3.141453    7.281748  \n\n\n\nm3 = smf.ols(formula='overshoot_day ~ income_group + life_expectancy + hdi + per_capita_gdp + total_cons + income_group:life_expectancy + income_group:hdi + income_group:per_capita_gdp + income_group:total_cons', data=dta)\nresults_m3 = m3.fit()\nprint(results_m3.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          overshoot_day   R-squared:                       0.957\nModel:                            OLS   Adj. R-squared:                  0.952\nMethod:                 Least Squares   F-statistic:                     172.6\nDate:                Sun, 15 Sep 2024   Prob (F-statistic):           2.17e-90\nTime:                        23:01:31   Log-Likelihood:                -759.34\nNo. Observations:                 167   AIC:                             1559.\nDf Residuals:                     147   BIC:                             1621.\nDf Model:                          19                                         \nCovariance Type:            nonrobust                                         \n======================================================================================================\n                                         coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------------------\nIntercept                            475.5719     76.750      6.196      0.000     323.896     627.248\nincome_group[T.LI]                   -82.5973    104.243     -0.792      0.429    -288.605     123.411\nincome_group[T.LM]                   -26.6372     89.044     -0.299      0.765    -202.609     149.335\nincome_group[T.UM]                   -60.8450    117.021     -0.520      0.604    -292.106     170.416\nlife_expectancy                       -0.0580      1.816     -0.032      0.975      -3.647       3.531\nincome_group[T.LI]:life_expectancy    -0.6213      2.393     -0.260      0.796      -5.351       4.109\nincome_group[T.LM]:life_expectancy     0.1047      2.069      0.051      0.960      -3.985       4.194\nincome_group[T.UM]:life_expectancy     2.5267      2.334      1.082      0.281      -2.086       7.140\nhdi                                 -334.8283    128.881     -2.598      0.010    -589.527     -80.130\nincome_group[T.LI]:hdi               416.3604    172.789      2.410      0.017      74.890     757.831\nincome_group[T.LM]:hdi               294.6598    152.695      1.930      0.056      -7.101     596.421\nincome_group[T.UM]:hdi                32.9497    196.861      0.167      0.867    -356.095     421.994\nper_capita_gdp                         0.0006      0.000      2.633      0.009       0.000       0.001\nincome_group[T.LI]:per_capita_gdp     -0.0116      0.018     -0.640      0.523      -0.047       0.024\nincome_group[T.LM]:per_capita_gdp     -0.0058      0.004     -1.622      0.107      -0.013       0.001\nincome_group[T.UM]:per_capita_gdp     -0.0006      0.002     -0.410      0.683      -0.004       0.002\ntotal_cons                           -15.2504      1.885     -8.092      0.000     -18.975     -11.526\nincome_group[T.LI]:total_cons         -2.8151     15.266     -0.184      0.854     -32.985      27.355\nincome_group[T.LM]:total_cons        -40.0300      4.097     -9.770      0.000     -48.127     -31.933\nincome_group[T.UM]:total_cons        -33.7400      3.693     -9.137      0.000     -41.037     -26.443\n==============================================================================\nOmnibus:                       15.235   Durbin-Watson:                   1.513\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               20.237\nSkew:                           0.567   Prob(JB):                     4.03e-05\nKurtosis:                       4.273   Cond. No.                     3.61e+06\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.61e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\nm4 = smf.ols(formula='overshoot_day ~ income_group + life_expectancy + hdi + per_capita_gdp + total_cons + income_group:hdi + income_group:total_cons', data=dta)\nresults_m4 = m4.fit()\nanova_results = anova_lm(results_m4, results_m3)\nprint(anova_results)\n\nprint(results_m4.summary())\n\n   df_resid           ssr  df_diff      ss_diff         F    Pr(&gt;F)\n0     153.0  90805.946608      0.0          NaN       NaN       NaN\n1     147.0  87032.348502      6.0  3773.598106  1.062285  0.387932\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          overshoot_day   R-squared:                       0.955\nModel:                            OLS   Adj. R-squared:                  0.951\nMethod:                 Least Squares   F-statistic:                     251.2\nDate:                Sun, 15 Sep 2024   Prob (F-statistic):           4.78e-96\nTime:                        23:06:15   Log-Likelihood:                -762.89\nNo. Observations:                 167   AIC:                             1554.\nDf Residuals:                     153   BIC:                             1597.\nDf Model:                          13                                         \nCovariance Type:            nonrobust                                         \n=================================================================================================\n                                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------------------\nIntercept                       449.4824     61.112      7.355      0.000     328.749     570.216\nincome_group[T.LI]              -87.4938     67.903     -1.289      0.200    -221.643      46.655\nincome_group[T.LM]               -2.3332     65.644     -0.036      0.972    -132.018     127.352\nincome_group[T.UM]               50.9642     86.889      0.587      0.558    -120.692     222.621\nlife_expectancy                   0.6123      0.661      0.926      0.356      -0.693       1.918\nhdi                            -364.7203     80.702     -4.519      0.000    -524.154    -205.287\nincome_group[T.LI]:hdi          336.1997     90.462      3.716      0.000     157.483     514.916\nincome_group[T.LM]:hdi          247.6362     84.096      2.945      0.004      81.497     413.775\nincome_group[T.UM]:hdi          130.0903    109.889      1.184      0.238     -87.005     347.185\nper_capita_gdp                    0.0006      0.000      2.442      0.016       0.000       0.001\ntotal_cons                      -15.0522      1.882     -7.999      0.000     -18.770     -11.335\nincome_group[T.LI]:total_cons    -6.9814     13.057     -0.535      0.594     -32.777      18.814\nincome_group[T.LM]:total_cons   -42.3619      3.798    -11.154      0.000     -49.865     -34.859\nincome_group[T.UM]:total_cons   -35.1038      3.621     -9.693      0.000     -42.258     -27.949\n==============================================================================\nOmnibus:                       24.387   Durbin-Watson:                   1.492\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               40.838\nSkew:                           0.745   Prob(JB):                     1.36e-09\nKurtosis:                       4.911   Cond. No.                     2.48e+06\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.48e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "liste &lt;- \n  readLines('topics.bib') |&gt; \n  as_tibble() |&gt; \n  filter(stringr::str_detect(value, pattern = '@')) |&gt; \n  mutate(key = stringr::str_remove(value, pattern = '@[:alnum:]+\\\\{')) |&gt; \n  mutate(key = stringr::str_remove(key, pattern = ',')) |&gt; \n  pull()\n  \nNoCite(bib = myBib, liste)\n\n\nBanerjee, S. and A. Roy (2014). Linear algebra and matrix analysis for statistics. Vol. 181. Crc Press Boca Raton.\nCohen, M. X. (2022). Practical linear algebra for data science. O’Reilly Media, Inc.\nGentle, J. E. (2007). “Matrix algebra”. In: Springer texts in statistics, Springer, New York, NY, doi 10, pp. 978-0.\nHarville, D. A. (1998). Matrix algebra from a statistician’s perspective.\nHusson, F., S. Lê, and J. Pagès (2011). Exploratory multivariate analysis by example using R. Vol. 15. CRC press Boca Raton.\nPetersen, K. B., M. S. Pedersen, and others (2008). “The matrix cookbook”. In: Technical University of Denmark 7.15, p. 510.\nYoshida, R. (2021). Linear algebra and its applications with R. Chapman and Hall/CRC. URL: https://shainarace.github.io/LinearAlgebra/."
  },
  {
    "objectID": "footprint_prep.html",
    "href": "footprint_prep.html",
    "title": "Footprint Data preparation",
    "section": "",
    "text": "This document presents the data preparation of the original Overshoot day data available on the website It contains ecological footprint and biocapacity results for 184 countries.\nIncomplete data will be omitted and the name of the different variables will be formatted in a more readable manner."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Topics In Statistics - Smart Data",
    "section": "",
    "text": "This refresher course is intended to help you brush up on essential linear algebra tools and some key statistical concepts.\nCourse Overview\nThis course is specifically tailored for those who are stepping into data science, it is intended to present or refresh two key tools in Data Science : Principal Component Analysis (PCA) and Regression analysis. These two mathematical conepts in Data science are built on basic Linear algebra ideas. While not designed as a math course, this course will provide crucial reminders of linear algebra indispensable for statistics and how they are used in PCA and Regression. We will also have labs to use this method on practical example.\nWhat You Will Learn\n\nLinear Surival Kit:\n\nVectors and matrices: Definitions, operations, and properties\nSystems of linear equations\nMatrix decomposition and eigenvalues\n\nPrincipal Component Analysis (PCA):\n\nUnderstanding the concept and purpose of PCA\nSteps involved in performing PCA\nApplications of PCA in data reduction and visualization\n\nLinear Regression:\n\nIntroduction to regression analysis\nBuilding and interpreting multiple regression models\nEvaluating model performance\n\n\nCourse Format\nThis course is conducted in English and combines lectures, practical examples, and hands-on exercises to reinforce learning. Whether you’re a beginner or just need to refresh these topics, the course will be helpful by highlighting the link between the linear algebra concept and their use in statistics. This will help you grasp and apply these usefulf concepts effectively. These concepts will be important for the whole master year.\nHow to use this website\nYou can find the lecture notes on the top menu on the right, as well as the Rmarkdown and/or Jupyter notebook used during the labs. The website will be updated during the course.\nPlease, if you have any remarks regarding the material on this website (typos, lack of clarity or anything else) please contact me.\nEnjoy!"
  },
  {
    "objectID": "linearalgebra4DS.html#references-on-linear-algebra-for-data-scientist.",
    "href": "linearalgebra4DS.html#references-on-linear-algebra-for-data-scientist.",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "References on Linear Algebra for Data Scientist.",
    "text": "References on Linear Algebra for Data Scientist.\n\n\nFor a general overview\n [Gen07]\n [Har98]\n [BR14]\n\nWith R or Python applications\n [Yos21]\n [Coh22]\nMatrix computations reference book\n [PP08]"
  },
  {
    "objectID": "linearalgebra4DS.html#linear-algebra-what-and-why",
    "href": "linearalgebra4DS.html#linear-algebra-what-and-why",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Linear Algebra : What and why",
    "text": "Linear Algebra : What and why\n\n\n\nDefinition\n\n\nLinear algebra is the branch of mathematics concerning linear equations such as \\(a_1 x_1 + \\cdots  a_n x_n = b,\\) and linear maps such as \\((x_{1},\\ldots ,x_{n})\\mapsto a_{1}x_{1}+\\cdots +a_{n}x_{n},\\) and their representations in vector spaces and through matrices.\n\n\n\nLink between Data science and Linear Algebra\nData are organized in rectangular array, understood as matrices, and the mathematical concept associated with matrices are used to manipulate data."
  },
  {
    "objectID": "linearalgebra4DS.html#palmer-penguins-illustration",
    "href": "linearalgebra4DS.html#palmer-penguins-illustration",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Palmer penguins illustration",
    "text": "Palmer penguins illustration\n\n## install.packages('palmerpenguins\")\nlibrary(\"palmerpenguins\")\npenguins_nona &lt;- na.omit(penguins) ## remove_na for now\npenguins_nona |&gt; print(n=3)\n\n# A tibble: 333 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 330 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nOne row describes one specific penguin,\nOne column corresponds to one specific attribute of the penguin.\n\nFocusing on quantitative variables, we get rectangular array with n=333 rows, and d=5 columns.\nEach penguin might be seen as a vector in \\(\\mathbb{R}^d\\) and each variable might be seen as a vector in \\(\\mathbb{R}^n\\). The whole data set is a matrix of dimension \\((n,d)\\)."
  },
  {
    "objectID": "linearalgebra4DS.html#linear-algebra-what-and-why-1",
    "href": "linearalgebra4DS.html#linear-algebra-what-and-why-1",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Linear Algebra : What and why",
    "text": "Linear Algebra : What and why\nRegression Analysis\n\nto predict one variable, the response variable named \\(Y\\), given the others, the regressors\nor equivalently to identify a linear combination of the regressors which provide a good approximation of the responsbe variable,\nor equivalently to specify parameters \\(\\theta,\\) so that the prediction \\(\\hat{Y} = X \\theta,\\) linear combination of the regressors which is as close as possible from \\(Y\\).\n\nWe are dealing with linear equations, enter the world of Linear Algebra.\nPrincipal Component Analysis\n\nto explore relationship between variables,\nor equivalently to quantify proximity between vectors,\nbut also to define a small set of new variables (vectors) which represent most of the initial variables,\n\nWe are dealing with vectors, vector basis, enter the world of Linear Algebra."
  },
  {
    "objectID": "linearalgebra4DS.html#vectors",
    "href": "linearalgebra4DS.html#vectors",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Vectors",
    "text": "Vectors\n\\(x\\in \\R^n\\) is a vector of \\(n\\) components.\nBy convention, \\(x\\) is a column vector \\[x=\\begin{pmatrix} x_1 \\cr \\vdots\\cr x_n \\end{pmatrix}.\\]\nWhen a row vector is needed we use operator \\(\\ ^\\intercal,\\)\n\\[x^\\intercal = \\begin{pmatrix} x_1, \\cdots,  x_n \\end{pmatrix}.\\] The null vector \\(\\mathbb{0}\\in\\R^d\\),\n\\[\\mathbb{0}^\\intercal = \\begin{pmatrix} 0, \\cdots 0\\end{pmatrix}.\\]"
  },
  {
    "objectID": "linearalgebra4DS.html#matrix",
    "href": "linearalgebra4DS.html#matrix",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Matrix",
    "text": "Matrix\nA matrix \\(A\\in \\R^{n\\times d}\\) has \\(n\\) rows and \\(d\\) columns:\n\\[A = \\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1d} \\cr\na_{21} & a_{22} & \\cdots & a_{2d} \\cr\n\\vdots & \\vdots &        & \\vdots \\cr\na_{n1} & a_{n2} & \\cdots & a_{nd}\n\\end{pmatrix}\\]\nThe term on row \\(i\\) and column \\(j\\) is refered to as \\(a_{ij}\\),\n\n\nThe column \\(j\\) is refer to as \\(A_{.,j}\\), \\(A_{.,j} = \\begin{pmatrix} a_{1j} \\cr \\vdots\\cr a_{nj} \\end{pmatrix}.\\)\n\nTo respect the column vector convention, the row \\(i\\) is refer to as \\(A_{i,.}^\\intercal\\), \\(A_{i,.}^\\intercal = \\begin{pmatrix} a_{i1}, & \\cdots &  , a_{id} \\end{pmatrix}\\)"
  },
  {
    "objectID": "linearalgebra4DS.html#simple-operations",
    "href": "linearalgebra4DS.html#simple-operations",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Simple operations",
    "text": "Simple operations\n\n\nMultiplication by a scalar\nlet’s \\(\\lambda\\in\\R\\),\n\\[\\lambda x = \\begin{pmatrix} \\lambda x_1 \\cr \\vdots\\cr \\lambda x_n \\end{pmatrix}.\\]\n\\[\\lambda A = \\begin{pmatrix}\n\\lambda  a_{11} & \\lambda a_{12} & \\cdots & \\lambda  a_{1d} \\cr\n\\lambda  a_{21} & \\lambda  a_{22} & \\cdots & \\lambda a_{2d} \\cr\n\\vdots & \\vdots &        & \\vdots \\cr\n\\lambda a_{n1} & \\lambda a_{n2} & \\cdots & \\lambda a_{nd}\n\\end{pmatrix}\\]\n\nSum\nIf \\(x\\in \\R^n\\) and \\(y\\in R^n\\), then \\(x+y\\) exists\n\\[x+ y =  \\begin{pmatrix} x_1 + y_1 \\cr \\vdots\\cr x_n + y_n\\end{pmatrix}.\\]\nIf \\(A\\) and \\(B\\) are two matrices with the same dimensions, \\(A+B\\) exists\n\\[\nA+B = \\begin{pmatrix}\na_{11} + b_{11} &  \\cdots &  a_{1d} + b_{1d}\\cr\n\\vdots &  &         \\vdots \\cr\na_{n1} + b_{n1}& \\cdots & a_{nd} + b_{nd}\n\\end{pmatrix}.\n\\]"
  },
  {
    "objectID": "linearalgebra4DS.html#visualization",
    "href": "linearalgebra4DS.html#visualization",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Visualization",
    "text": "Visualization\nConsider \\(x=\\begin{pmatrix} 1 \\cr 0 \\end{pmatrix}\\) and \\(y= \\begin{pmatrix} 0 \\cr 1 \\end{pmatrix},\\) and \\(z=x+2 y.\\)\n\nVector representationVector representation 2"
  },
  {
    "objectID": "linearalgebra4DS.html#matrix-product",
    "href": "linearalgebra4DS.html#matrix-product",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Matrix product",
    "text": "Matrix product\n\nDefinitionSolutionRPython\n\n\nLet \\(A\\in \\R^{n \\times p}\\) and \\(B\\in\\R^{p \\times d}\\), then the product \\(AB\\) is well defined, it is a matrix \\(C\\) in \\(\\R^{n\\times ,d}\\), so that\n\\[C_{ij} = \\sum_{k=1}^p A_{ik} B_{kj}, \\quad 1\\leq i\\leq n, 1\\leq j \\leq d.\\]\nExample\n\n\\(A\\in \\R^{3 \\times 2}\\), \\(B\\in\\R^{2 \\times 4}\\), with \\(A = \\begin{pmatrix}\n\\color{color\n}{1} & 2 \\cr\n1 & 3 \\cr\n-1 & -1 \\end{pmatrix}\\) and \\(B =\\begin{pmatrix}\n{-1} & {-2} & {0} & {1}\\cr\n{-2} & {1} & {1} & {0}\\end{pmatrix},\\)\n\n\n\n\\(A B\\) exists as the number of colums in \\(A\\) equals the number of rows in \\(B\\).\nA useful presentation: \\[\\begin{matrix}\n& & \\begin{pmatrix}\n\\class{bleuf}{-1} & \\class{bleu}{-2} & \\class{vert}{0} & \\class{clair}{1}\\cr\n\\class{bleuf}{-2} & \\class{bleu}{1} & \\class{vert}{1} & \\class{clair}{0}\\end{pmatrix}   \\cr\n& \\begin{pmatrix}\n\\class{jaune}{1} & \\class{jaune}{2} \\cr\n\\class{orange}{1} & \\class{orange}{3} \\cr\n\\class{rouge}{-1} & \\class{rouge}{-1} \\end{pmatrix} &\n\\begin{pmatrix}\n\\class{jaune}{\\bullet} \\class{bleuf}{\\bullet}   & \\class{jaune}{\\bullet} \\class{bleu}{\\bullet}  & \\class{jaune}{\\bullet} \\class{vert}{\\bullet} & \\class{jaune}{\\bullet} \\class{clair}{\\bullet} \\cr\n\\class{orange}{\\bullet} \\class{bleuf}{\\bullet}   & \\class{orange}{\\bullet} \\class{bleu}{\\bullet}  & \\class{orange}{\\bullet} \\class{vert}{\\bullet} & \\class{orange}{\\bullet} \\class{clair}{\\bullet} \\cr\n\\class{rouge}{\\bullet} \\class{bleuf}{\\bullet}   & \\class{rouge}{\\bullet} \\class{bleu}{\\bullet}  & \\class{rouge}{\\bullet} \\class{vert}{\\bullet} & \\class{rouge}{\\bullet} \\class{clair}{\\bullet} \\cr\n\\end{pmatrix},\n\\end{matrix}\n\\] * \\[AB =   \\begin{pmatrix}\n\\class{jaune}{1}\\times\\class{bleuf}{(-1)} +\\class{jaune}{2}\\times \\class{bleuf}{(-2)}    & \\class{jaune}{1}\\times \\class{bleu}{(-2)} + \\class{jaune}{2} \\times\\class{bleu}{1}  & \\class{jaune}{1}\\times \\class{vert}{0} + \\class{jaune}{2}\\times \\class{vert}{1}  & \\class{jaune}{1} \\times\\class{clair}{1} + \\class{jaune}{2}\\times \\class{clair}{0} \\cr\n\\class{orange}{1} \\times \\class{bleuf}{(-1)}  +  \\class{orange}{3} \\times \\class{bleuf}{(-2)}   & \\class{orange}{1} \\times  \\class{bleu}{(-2)} +  \\class{orange}{3} \\times  \\class{bleu}{1} & \\class{orange}{1} \\times \\class{vert}{0} + \\class{orange}{3} \\times \\class{vert}{1}  & \\class{orange}{1} \\times\\class{clair}{1} + \\class{orange}{3}\\times \\class{clair}{0}  \\cr\n\\class{rouge}{(-1)} \\times \\class{bleuf}{(-1)}  +  \\class{rouge}{(-1)} \\times \\class{bleuf}{(-2)}   & \\class{rouge}{(-1)} \\times  \\class{bleu}{(-2)} +  \\class{rouge}{(-1)} \\times  \\class{bleu}{1} & \\class{rouge}{(-1)} \\times \\class{vert}{0} + \\class{rouge}{(-1)} \\times \\class{vert}{1}  & \\class{rouge}{(-1)} \\times\\class{clair}{1} + \\class{rouge}{(-1)}\\times \\class{clair}{0}  \\cr\n\\end{pmatrix},\\]\n\\[AB = \\begin{pmatrix}\n-5   & 0  & 2  & 1 \\cr\n-7   & 1 & 3 & 1  \\cr\n3  & 1 &-1  & -1  \\cr\n\\end{pmatrix}. \\]\n\n\nA matrix is defined column by column\n\nA = matrix(c(1,1,-1,2,3,-1), ncol = 2)\nB = matrix(c(-1,-2, -2,1, 0, 1, 1, 0), ncol = 4)\n\nThe default operation \\(A*B\\) is not the matrix product but a product element by element, the matrix product is obtained by\n\nA %*% B\n\n     [,1] [,2] [,3] [,4]\n[1,]   -5    0    2    1\n[2,]   -7    1    3    1\n[3,]    3    1   -1   -1\n\n\n\n\nMatrix product is available through the numpy library\n\nimport numpy as np\n\nA = np.array([[ 1, 2], [ 1, 3], [ -1, 1]]); A.view()\n\narray([[ 1,  2],\n       [ 1,  3],\n       [-1,  1]])\n\nB = np.array([[-1, -2, 0, 1], [ -2, 1, 1, 0]]); B.view()\n\narray([[-1, -2,  0,  1],\n       [-2,  1,  1,  0]])\n\nC = A.dot(B);  C.view()\n\narray([[-5,  0,  2,  1],\n       [-7,  1,  3,  1],\n       [-1,  3,  1, -1]])"
  },
  {
    "objectID": "linearalgebra4DS.html#matrix-vector-product",
    "href": "linearalgebra4DS.html#matrix-vector-product",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Matrix vector product",
    "text": "Matrix vector product\nAs a vector \\(x\\) in \\(\\R^n\\) is also a matrix in \\(\\R^{n\\times 1}\\), the product \\(A x\\) exisst is the number of columns in \\(A\\) equals the number of row for \\(x\\).\n\nExampleExercises\n\n\n\\[A = \\begin{pmatrix}\n1 & -1 \\cr\n1 & 0 \\cr\n-1 & 0.5\n\\end{pmatrix},\\]\nLet \\(x\\in \\R^2\\), \\(Ax\\) exists\n\\[Ax = \\begin{pmatrix}\nx_1 - x_2 \\cr\nx_1 \\cr\n-x_1 + 0.5 x\n\\end{pmatrix},\\]\n\n\nLet \\(x\\in\\R^2\\), \\(y\\in\\R^3\\) and \\[A = \\begin{pmatrix}\n1 & -1 & 0 \\cr\n1 & 0 & -0.5 \\cr\n-1 & 0.5 & 2\n\\end{pmatrix}, B = \\begin{pmatrix}\n1 & 1 & 0 \\cr\n-1 & 0 & -0.5\n\\end{pmatrix},\\quad  C = \\begin{pmatrix}\n1 & -1  \\cr\n-1 & 2  \\cr\n1 & -0.5\n\\end{pmatrix},\\]\nWhen possible, compute the following quantites\n\\[ Ax; \\quad Ay; \\quad AB; \\quad BA; \\quad AC; \\quad CA; \\quad Bx; \\quad Cx. \\]"
  },
  {
    "objectID": "linearalgebra4DS.html#transposition",
    "href": "linearalgebra4DS.html#transposition",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Transposition",
    "text": "Transposition\nThe transpose, denoted by \\(\\ ^\\intercal\\), of a matrix results from switching the rows and columns. Let \\(A \\in \\R^{n\\times d}\\), \\[A^{\\intercal} \\in \\R^{d\\times n},\\  and (A^{\\intercal})_{ij}= A_{ji}.\\]\n\\[\nA =  \\begin{pmatrix}\n\\class{jaune}{\\bullet} & \\class{jaune}{\\bullet} \\cr\n\\class{orange}{\\bullet} & \\class{orange}{\\bullet} \\cr\n\\class{rouge}{\\bullet} & \\class{rouge}{\\bullet} \\cr\n\\end{pmatrix}, \\quad A^{\\intercal} = \\begin{pmatrix}\n\\class{jaune}{\\bullet} & \\class{orange}{\\bullet} & \\class{rouge}{\\bullet}\\cr\n\\class{jaune}{\\bullet} & \\class{orange}{\\bullet} & \\class{rouge}{\\bullet}\n\\end{pmatrix}\n\\]\n\nIllustrationExercise\n\n\nExample\n\\[A = \\begin{pmatrix}\n1 & -1 \\cr\n1 & 0 \\cr\n-1 & 0.5\n\\end{pmatrix}, \\quad A^\\intercal =  \\begin{pmatrix}\n1 &  1 & -1 \\cr\n-1 & 0  & 0.5\n\\end{pmatrix},\n\\]\nProperties\n\n\\((A^\\intercal)^\\intercal = A,\\)\n\\((AB)^\\intercal = (B)^\\intercal (A)^\\intercal,\\)\n\\((A+B)^\\intercal = (A)^\\intercal + (B)^\\intercal.\\)\n\n\n\n\\[A = \\begin{pmatrix}\n1 & -1 & 0 \\cr\n1 & 0 & -0.5 \\cr\n-1 & 0.5 & 2\n\\end{pmatrix}, B = \\begin{pmatrix}\n1 & 1 & 0 \\cr\n-1 & 0 & -0.5\n\\end{pmatrix}, C=\\begin{pmatrix}\n1 & 2 \\cr\n2 & 1\n\\end{pmatrix}, x \\in \\R^2, y\\in\\R^3.\\]\nCompute \\((C)^\\intercal, (BA)^\\intercal, x^\\intercal B, (Ay)^\\intercal, y^\\intercal A^\\intercal\\)"
  },
  {
    "objectID": "linearalgebra4DS.html#dot-product-and-norm",
    "href": "linearalgebra4DS.html#dot-product-and-norm",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Dot product and norm",
    "text": "Dot product and norm\nLet \\(x,y\\) be in \\((\\R^n)^2,\\) the dot product, also known as inner product or scalar product is defined as \\[x\\cdot y = y\\cdot x = \\sum_{i=1}^n x_i y_i = x^\\intercal y= y^\\intercal x.\\] The norm of a vector \\(x\\), symbolized with \\(\\norm{x},\\) is defined by \\[\\norm{x}^2 =\\sum_{i=1}^n x_i^2 = x^\\intercal x.\\]\n\nExampleGeometrical PropertiesUseful remark\n\n\n\\[x=\\begin{pmatrix}\n1\\cr\n0\n\\end{pmatrix},  y=\\begin{pmatrix}\n0\\cr\n1\n\\end{pmatrix},  z=\\begin{pmatrix}\n1\\cr\n2\n\\end{pmatrix} \\]\n\\[x\\cdot y = 0, x\\cdot z = 1, y \\cdot z = 2, \\quad \\norm{x}=\\norm{y}=1, \\quad \\norm{z} = \\sqrt{5}\\]\n\n\n\nLet \\((x,y)\\in(\\R^n)^2\\),\n\\(x\\cdot y = \\norm{x} \\norm{y} \\cos(\\theta)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\cos(\\theta)= \\frac{x\\cdot y}{\\norm{x} \\norm{y}}\\)"
  },
  {
    "objectID": "linearalgebra4DS.html#linear-independance",
    "href": "linearalgebra4DS.html#linear-independance",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Linear independance",
    "text": "Linear independance\nA sequence of vectors \\(x_1, \\cdots, x_k\\) from \\(\\R^n\\) is said to be linearly independent, if the only solution to \\(\\sum_{i=1}^k a_i x_i  =\\mathbb{0}\\) is \\(a =\\mathbb {0}\\).\n\nExampleExercice\n\n\n\\(x_1 = \\begin{pmatrix}\n1 \\cr\n0 \\cr\n0\n\\end{pmatrix}, \\quad x_2 = \\begin{pmatrix}\n1 \\cr\n1 \\cr\n0\n\\end{pmatrix}, \\quad X=\\begin{pmatrix}\n1 & 1  \\cr\n0 & 1  \\cr\n0 & 0 \\cr\n\\end{pmatrix}, \\quad Xa = \\begin{pmatrix}a_1 + a_2\\cr\na_2 \\cr\n0 \\end{pmatrix}.\\)\n\\(x_1\\) and \\(x_2\\) are linearly independant.\n\n\n\\(x_1 = \\begin{pmatrix}\n1 \\cr\n1 \\cr\n1\n\\end{pmatrix}, \\quad x_2 = \\begin{pmatrix}\n1 \\cr\n1 \\cr\n0\n\\end{pmatrix}, x_3 = \\begin{pmatrix}\n0 \\cr\n1 \\cr\n1\n\\end{pmatrix}, x_4 = \\begin{pmatrix}\n0 \\cr\n-1 \\cr\n1\n\\end{pmatrix}\\)\n\nAre \\((x_1, x_2,x_3)\\) linearly independant ?\nAre \\((x_1, x_2,x_4)\\) linearly independant ?"
  },
  {
    "objectID": "linearalgebra4DS.html#span-of-a-set-of-vectors",
    "href": "linearalgebra4DS.html#span-of-a-set-of-vectors",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Span of a set of vectors",
    "text": "Span of a set of vectors\n\nDefinitionExercisesIllustration\n\n\nThe span of a set of vectors is the set of all possible linear combinations of these vectors.\nIn other words, if you take some vectors \\(v_1, v_2, \\dots, v_k\\), their span is the collection of vectors you can build by multiplying each \\(v_i\\) by a scalar and then adding the results together.\nFormally:\n\\[\n\\text{span}(v_1, v_2, \\dots, v_k)\n= \\{ a_1 v_1 + a_2 v_2 + \\cdots + a_k v_k \\;\\mid\\; a_1, a_2, \\dots, a_k \\in \\mathbb{R} \\}.\n\\]\nIntuitively, the span represents the space generated by those vectors:\n\nIf you have a single nonzero vector in \\(\\mathbb{R}^2\\), its span is a line through the origin.\n\nIf you have two linearly independent vectors in \\(\\mathbb{R}^2\\), their span is the whole plane.\n\nIn higher dimensions, the span gives you the smallest subspace that contains all the vectors.\n\n\n\n\\[\\text{span}\\left(\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\right );\\] \\[\\text{span}\\left(\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\right );\\] \\[\\text{span}\\left(\\begin{pmatrix} 1 \\\\ 1 \\\\1\\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\right );\\]"
  },
  {
    "objectID": "linearalgebra4DS.html#vector-space",
    "href": "linearalgebra4DS.html#vector-space",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Vector space",
    "text": "Vector space\n\n\n\nDefinition\n\n\nA real vector space \\((V, +, . ,\\R)\\) is a set \\(V\\) equipped with two operations satisfybing the following properties for all \\((x,y,z)\\in V^3\\) and all \\((\\lambda,\\mu)\\in\\R^2:\\)\n\n(close for addition) \\(x + y \\in V,\\)\n(close for scalar multiplication) \\(\\lambda x  \\in V,\\)\n(commutativity) \\(x + y = y + x,\\)\n(associativity of +) \\((x + y) + z  = x + (y + z),\\)\n(Null element for +) \\(x + \\mathbb{0} = x,\\)\n(Existence of additive inverse +) There exists \\(-x \\in V\\), such that $ + (-x) = ,$\n(Associativity of scalar multiplication) \\(\\lambda (\\mu x)   = (\\lambda \\mu ) x,\\)\n(Distributivity of scalar sums) \\((\\lambda + \\mu ) x   = \\lambda x + \\mu x,\\)\n(Distributivity of vector sums) \\(\\lambda (x + y )   = \\lambda x + \\lambda y,\\)\n(Scalar multiplication identity) \\(1 x   = x.\\)\n\n\n\n\nIt is enough for the course to think to the set of vectors in \\(\\R^d\\)."
  },
  {
    "objectID": "linearalgebra4DS.html#basis-of-a-vector-space",
    "href": "linearalgebra4DS.html#basis-of-a-vector-space",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Basis of a Vector Space",
    "text": "Basis of a Vector Space\nA set B of elements of a vector space V is called a basis if every element of V can be written in a unique way as a finite linear combination of elements of B.\n\nPropertyExample - Exercises\n\n\nA subset \\(B \\subset V\\) is a basis if it satisfies the two following conditions:\n\nLinear independence\nFor every finite subset \\(\\{ \\mathbf{v}_1, \\dotsc, \\mathbf{v}_m \\}\\) of \\(B\\),\nif \\[\nc_{1}\\mathbf{v}_{1} + \\cdots + c_{m}\\mathbf{v}_{m} = \\mathbf{0}\n\\] for some \\(c_{1}, \\dotsc, c_{m} \\in F\\), then necessarily \\[\nc_{1} = \\cdots = c_{m} = 0.\n\\]\nSpanning property\nFor every vector \\(\\mathbf{v} \\in V\\), one can choose \\(a_{1}, \\dotsc, a_{n} \\in F\\)\nand \\(\\mathbf{v}_{1}, \\dotsc, \\mathbf{v}_{n} \\in B\\) such that \\[\n\\mathbf{v} = a_{1}\\mathbf{v}_{1} + \\cdots + a_{n}\\mathbf{v}_{n}.\n\\]\n\nThe coefficients of this linear combination are referred to as components or coordinates of the vector with respect to B. The elements of a basis are called basis vectors.\nIf \\(\\norm{v_i} =1\\) for all \\(i\\), the basis is orthonormal.\n\n\n\\((\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\right )\\) is a basis of \\(\\R^2\\)\nCheck whether or not this set of vectors form Orthobormal bases\n\nin \\(\\R^2\\) \\((\\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}\\right )\\)\nin \\(\\R^2\\) \\((\\begin{pmatrix} \\sqrt{2}/2 \\\\ \\sqrt{2}/2 \\end{pmatrix}, \\begin{pmatrix} -\\sqrt{2}/2 \\\\ \\sqrt{2}/2 \\end{pmatrix}\\right )\\)\nin \\(\\R^3\\) \\((\\begin{pmatrix} \\sqrt{2}/2 \\\\ \\sqrt{2}/2 \\\\ 1\\end{pmatrix}, \\begin{pmatrix} -\\sqrt{2}/2 \\\\ 1\\\\ \\sqrt{2}/2 \\end{pmatrix}\\right )\\)\nin \\(\\R^3\\) [ _1=\n\\[\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}\\]\n, _2=\n\\[\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix}\\]\n, _3=\n\\[\\begin{pmatrix}1\\\\1\\\\-2\\end{pmatrix}\\]\n. ]"
  },
  {
    "objectID": "linearalgebra4DS.html#linear-mapping",
    "href": "linearalgebra4DS.html#linear-mapping",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Linear mapping",
    "text": "Linear mapping\nLet \\(f\\) be a function from \\(\\R^d\\) to \\(\\R^n\\), \\(f\\) is linear if and only if\nFor all \\((\\lambda,\\mu) \\in \\R^2\\) and for all \\((x,y) (\\R^d)^2,\\) \\[f(\\lambda x + \\mu y ) = \\lambda  f(x) + \\mu f( y )\\]\nExample\n\\[\\begin{align}\nf : \\R^2 & \\to \\R\\cr\nx & \\mapsto x_1-x_2\n\\end{align}\\]\n\\[\\begin{align}\nf(\\lambda x + \\mu y) & = f \\left (\n\\begin{pmatrix}\n\\lambda x_1 + \\mu y_1\\cr\n\\lambda x_2 + \\mu y_2\\cr\n\\end{pmatrix}\n\\right) =  (\\lambda x_1 + \\mu y_1) - (\\lambda x_2 + \\mu y_2)=\\lambda (x_1 - x_2) + \\mu (y_1 -y_2)\\cr\n& = \\lambda f( x )  + \\mu f( y )\\cr\n\\end{align}\\]\n\\[f(x) = A x, \\quad A = \\begin{pmatrix}\n1 & -1\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "linearalgebra4DS.html#matrix-as-a-linear-map-on-vector-space",
    "href": "linearalgebra4DS.html#matrix-as-a-linear-map-on-vector-space",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Matrix as a linear map on vector space",
    "text": "Matrix as a linear map on vector space\nA real matrix \\(A\\in\\R^{n\\times d}\\) is a linear function operating from one real vector space of dimension \\(d\\) to real vector space of dimension \\(n\\).\n\\[\\begin{align}\nA: &\\R^d \\to  \\R^n\\\\\n  & x  \\mapsto  y = Ax\n\\end{align}\\]\n\nExamplesRemarksVisualisationExercicesSolutions\n\n\n\n\n\\[A = \\begin{pmatrix}\n\\color{color\n}{1} & 2 \\cr\n1 & 3 \\cr\n-1 & -1 \\end{pmatrix}\\]\n\\[\\begin{align}\nA: &\\R^2 \\to  \\R^3\\\\\n  & x  \\mapsto  y = \\begin{pmatrix}\n\\color{color\n}{1} & 2 \\cr\n1 & 3 \\cr\n-1 & -1 \\end{pmatrix} \\begin{pmatrix}\nx_1\\cr\nx_2 \\end{pmatrix} =\\begin{pmatrix}\n                      x_1 + 2 x_2 \\cr\n                      x_1 + 3 x_2 \\cr\n                      -x_1-x_2\n                    \\end{pmatrix}\n                    \n\\end{align}\\]\n\n\n\n\\[M = \\begin{pmatrix}\n\\color{color\n}{1} & -1 \\cr\n1 & 2\n\\end{pmatrix}\\]\n\\[\\begin{align}\n  & x  \\mapsto  y = \\begin{pmatrix}\n                    1 & -1 \\cr\n                    1 & 2\n                    \\end{pmatrix} \\begin{pmatrix}\nx_1\\cr\nx_2 \\end{pmatrix} =\\begin{pmatrix}\n                      x_1 -  x_2 \\cr\n                      x_1 + 2 x_2\n                    \\end{pmatrix}\n\\end{align}\\]\n\n\n\n\\(A \\begin{pmatrix}1\\cr 0\\end{pmatrix} = A_{.,1}\\)\n\\(A \\begin{pmatrix} 0\\cr 1\\end{pmatrix} = A_{.,2}.\\)\n\n\n\n\n\\[\\begin{align}\nM: &\\R^2 \\to  \\R^2\\\\\n  & x  \\mapsto  y = \\begin{pmatrix}\n  1 & -1 \\cr\n1 & 2\n\\end{pmatrix} x\n\\end{align}\\] is a transformation of \\(\\R^2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 1\nLet’s consider the application from \\(\\R^2\\) to \\(\\R^2\\) which projects on the \\(x\\) axis.\n\nIs it a linear transformation of \\(\\R^2\\) ?\nIf so, could you write the corresponding matrix \\(P\\) ?\n\n\nExercice 2\nLet’s consider the application from \\(\\R^2\\) to \\(\\R\\) which computes the square norm of the vector \\(|| x||^2 = x_1^2 + x_2^2\\)\n\nIs it a linear transformation of \\(\\R^2\\) ?\nIf so, could you write the corresponding matrix \\(N\\) ?\n\n\n\n\n\n\nExercice 1\n\nProjection on \\(x\\) axis consists in forgetting the \\(y\\) axis component. \\[\\begin{align}\nf(\\lambda_1 x + \\lambda_2 z) & = f\\left ( \\begin{pmatrix} \\lambda_1 x_1 + \\lambda_2 z_1 \\cr\n\\lambda_1 x_1 + \\lambda_2 z_2\\cr\n\\end{pmatrix} \\right) \\cr\n& =  \\begin{pmatrix} \\lambda_1 x_1 + \\lambda_2 z_1 \\cr\n0\\cr\n\\end{pmatrix}\\cr\n& = \\lambda_1 f(x) + \\lambda_2 f(z),\n\\end{align}\\]\n\\(f\\) is linear, its matrix form \\(P:\\)\n\n\\[P = \\begin{pmatrix} 1 & 0 \\cr 0  & 0 \\end{pmatrix},\\]\n\n\\[Px = \\begin{pmatrix} 1 & 0 \\cr 0  & 0 \\end{pmatrix} \\begin{pmatrix} x_1  \\cr x_2  \\end{pmatrix} =  \\begin{pmatrix} x_1  \\cr 0  \\end{pmatrix}\\]\nExercice 2\n\nSquare Norm \\(f\\) of \\(x\\) equals \\(x_1^2 + x_2^2,\\) As \\(f(\\lambda_1 x ) = \\lambda_1^2 f( x)\\), the square norm is not a linear function and could not be expressed as a matrix."
  },
  {
    "objectID": "linearalgebra4DS.html#matrix-rank",
    "href": "linearalgebra4DS.html#matrix-rank",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Matrix rank",
    "text": "Matrix rank\nThe sequence of columns of a matrix \\(M\\in \\R^{n\\times d}\\) forms a sequence of \\(d\\) vectors \\((c_1, \\cdots,c_d)\\) in \\(\\R^n\\), while the rows of a matrix forms a sequence of \\(n\\) vectors \\((r_1, \\cdots,r_n)\\) in \\(\\R^d\\).\nThe rank of a matrix is the length of the largest sequence of linearly independant columns or equivalently the length of the largest sequence of linearly independant rows.\n\nExampleRemarksExample with RExample with Python\n\n\n\n\\(X_1 =\\begin{pmatrix}\n1 & 1  \\cr\n0 & 1  \\cr\n0 & 0 \\cr\n\\end{pmatrix}, \\quad rank(X_1) = 2; \\quad \\quad X_2=\\begin{pmatrix}\n1 & 1 & 0 \\cr\n1 & 1 & -1\\cr\n1 & 0 & 1\n\\end{pmatrix}, \\quad rank(X_2) = 3;\\)\n\\(X_3=\\begin{pmatrix}\n1 & 1 & -1 \\cr\n1 & 1 & -1\\cr\n1 & 0 & 1\n\\end{pmatrix}, \\quad rank(X_3) = 2;\\)\n\n\n\nLet \\(A= (a_1, \\cdots, a_d)^\\intercal \\in \\R^d\\), \\(A A^\\intercal = \\begin{pmatrix}\n& &  \\cr\na_1 A & \\cdots & a_d A\\cr\n& &  \\cr\n\\end{pmatrix}\\) if of rank \\(1\\).\n\n\n\n\n\n#install.packages('Matrix')\nlibrary(Matrix)\nX1 &lt;- matrix(c(1,0,0,1,1,0), ncol = 2)\nX2 &lt;- matrix(c(1,1,1,1,1,0, 0, -1,1), ncol = 3)\nX3 &lt;- matrix(c(1,1,1,1,1,0, -1, -1,1), ncol = 3)\n\n\n\nrankMatrix(X1)[1]\n\n[1] 2\n\nrankMatrix(X2)[1]\n\n[1] 3\n\nrankMatrix(X3)[1]\n\n[1] 2\n\n\n\n\n\n\nimport numpy as np\nfrom numpy.linalg import matrix_rank\n\nX1 = np.array([[ 1, 1], [ 0, 1], [ 0, 0]])\nX2 = np.array([[ 1, 1, 0], [ 1, 1, -1], [ 1, 0, 1]])\nX3 = np.array([[ 1, 1, -1], [ 1, 1, -1], [ 1, 0, 1]])\n\nmatrix_rank(X1) \n\nnp.int64(2)\n\nmatrix_rank(X2) \n\nnp.int64(3)\n\nmatrix_rank(X3) \n\nnp.int64(2)"
  },
  {
    "objectID": "linearalgebra4DS.html#column-space-rank",
    "href": "linearalgebra4DS.html#column-space-rank",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Column Space, rank",
    "text": "Column Space, rank\nIf \\(A\\in\\R^{n\\times d},\\) and \\(c\\in \\R^d\\), \\[A c = \\begin{pmatrix}\n& & &  \\cr\nA_{.,1} & A_{.,2} & \\cdots & A_{.,d} \\cr\n& & &  \\cr\n\\end{pmatrix} \\begin{pmatrix}\nc_1  \\cr\nc_2 \\cr\n\\vdots \\cr\nc_d  \\cr\n\\end{pmatrix} = \\begin{pmatrix}\nc_1 a_{11} + c_2 a_{12} + \\cdots + c_d a_{1d} \\cr\n\\vdots \\cr\nc_1 a_{n1} + c_2 a_{n2} + \\cdots + c_d a_{nd} \\cr\n\\end{pmatrix} = c_1 A_{.,1} + c_2  A_{.,2} + \\cdots + c_d  A_{.,d}\\]\nImage of \\(A\\) or Column space of A\n\\(Im(A) = \\left \\lbrace y \\in \\R^d; \\exists c\\in R^d, y = \\sum_{l=1}^d c_l A_{., l} \\right \\rbrace\\)\nRemarks\n\nIf \\(A_{.,1}\\) and \\(A_{,2}\\) are linearly dependant, \\(A_{,1} = \\lambda A_{,2}\\) and \\(Ac = ( c_1 \\lambda + c_2)  A_{.,2} + \\cdots + c_d  A_{.,d}\\). The rank is the minimal number of vector to generate \\(Im(A)\\), that is the dimension of \\(Im(A)\\)."
  },
  {
    "objectID": "linearalgebra4DS.html#remarkable-matrices",
    "href": "linearalgebra4DS.html#remarkable-matrices",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Remarkable matrices",
    "text": "Remarkable matrices\nIdentity matrix\nThe square matrix \\(I\\) such \\(I_{ij} =\\delta_{ij}\\) is named the Identity matrix and acts as the neutral element for matrix multiplication\nLet \\(I_n\\) be the identity matrix in $^{nn}, for any \\(A \\in \\R^{m\\times n}\\), \\(A I_n = A\\) and \\(I_m A = A\\).\nDiagonal matrix\nA Diagonal matrix is a matrix in which elements outside the main diagonal are all zeros. The term is mostly used for square matrix but the terminology might also be used otherwise.\n\nExampleGeometrical point of view\n\n\nLet’s \\(e_i = (\\delta_{i1}, \\cdots, \\delta_{id})\\) and \\(D = \\begin{pmatrix}2 & 0 & 0 \\cr\n                    0 & 1 & 0 \\cr\n                    0 & 0 & -1\n                    \\end{pmatrix}\\) \\[D \\class{rouge}{e_1} = 2 \\class{rouge}{e_1},\\quad D \\class{bleu}{e_2} =  \\class{bleu}{e_2},\\quad D \\class{orange}{e_3} = - \\class{orange}{e_3}.\\]\n\\(D\\) acts as simple scalar multiplier on the basis vectors.\n\n\n\n\nConsider \\(D = \\begin{pmatrix}2  & 0 \\cr\n                    0  & 0.5\n                    \\end{pmatrix},\\quad Dx = \\begin{pmatrix} 2 x_1 \\cr\n                      0.5 x_2\n                    \\end{pmatrix}.\\)"
  },
  {
    "objectID": "linearalgebra4DS.html#orthogonal-matrix",
    "href": "linearalgebra4DS.html#orthogonal-matrix",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Orthogonal matrix",
    "text": "Orthogonal matrix\nDefinition\nAn orthogonal matrix, \\(Q\\) or orthonormal matrix, is a real square matrix whose columns and rows are orthonormal vectors and therefore verifies \\[Q^\\intercal Q =  Q Q^\\intercal = I.\\]\n\nExampleGeometrical point of view\n\n\n\\(Q = \\begin{pmatrix}\\frac{\\sqrt{2}}{2} & 0 &  -\\frac{\\sqrt{2}}{2}\\cr\n                    \\frac{\\sqrt{2}}{2} & 0 &  \\frac{\\sqrt{2}}{2}\\cr\n                    0 & 1 & 0\n                    \\end{pmatrix}\\) \\[Q  Q^\\intercal = I_3\\]\n\\(D\\) transform the vector basis in new orthonormal vectors.\n\n\n\n\nConsider \\(Q = \\begin{pmatrix}\\frac{\\sqrt{2}}{2} &   -\\frac{\\sqrt{2}}{2}\\cr\n                    \\frac{\\sqrt{2}}{2} &  \\frac{\\sqrt{2}}{2}\n                    \\end{pmatrix},\\quad Qx = \\begin{pmatrix}  \\frac{\\sqrt{2}}{2}x_1 + \\frac{\\sqrt{2}}{2} x_2\\cr\n                  -\\frac{\\sqrt{2}}{2}x_1 + \\frac{\\sqrt{2}}{2} x_2\n                    \\end{pmatrix}.\\)"
  },
  {
    "objectID": "linearalgebra4DS.html#how-are-matrices-used-in-data-science",
    "href": "linearalgebra4DS.html#how-are-matrices-used-in-data-science",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "How are matrices used in Data Science",
    "text": "How are matrices used in Data Science\nSolving Linear systems\nMatrices provide a compact way to represent and solve systems of linear equations, which appear in regression models, optimization problems, and numerical simulations.\nIdentifying Invariant Directions\nMatrices help reveal invariant directions in data transformations. This is the foundation of techniques like Principal Component Analysis (PCA), which reduces dimensionality while preserving essential structure."
  },
  {
    "objectID": "linearalgebra4DS.html#linear-systems",
    "href": "linearalgebra4DS.html#linear-systems",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Linear systems",
    "text": "Linear systems\nNumerically, linear regression sums up at solving a linear system \\[X \\theta = b.\\] where \\(X\\in \\R^{n\\times p}\\), \\(\\theta\\in\\R^p\\) and \\(b\\in \\R^n\\). Assume \\(n&gt;=p\\).\n\nCase \\(rg(X) = p\\)Case \\(rg(X) &lt; p\\)Case \\(rg(X) &gt; p\\)"
  },
  {
    "objectID": "linearalgebra4DS.html#eigen-values-eigen-vectors",
    "href": "linearalgebra4DS.html#eigen-values-eigen-vectors",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Eigen values, Eigen vectors",
    "text": "Eigen values, Eigen vectors\nConsider a square matrix, \\(A \\in \\R^{d\\times d}\\), and a nonzero vector, \\(v \\in \\R^d\\), \\(v\\) is an eigen vector for the eigen value \\(\\lambda\\) if \\[Av = \\lambda v.\\] If \\(v\\) is an eigen vector for \\(A\\), applying \\(A\\) to \\(v\\) just consists in multiplying by the corresponding eigen value.\n\nGeometric interpretationExercises\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the eigen values and eigen vectors for \\(A_1 = \\begin{pmatrix}1 & 0 \\cr 0 & 2\\end{pmatrix}\\), \\(A_2 = \\begin{pmatrix}1 & 3 \\cr 0 & 2\\end{pmatrix},\\) \\(A_3 = \\begin{pmatrix} 2 & 2 \\cr -2 & 2 \\end{pmatrix}.\\)"
  },
  {
    "objectID": "linearalgebra4DS.html#singular-value-decomposition-svd",
    "href": "linearalgebra4DS.html#singular-value-decomposition-svd",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Singular Value Decomposition (SVD)",
    "text": "Singular Value Decomposition (SVD)\nLet’s \\(A\\) a \\(n\\times d\\) matrix of rank \\(r\\), there exists a \\(n\\times n\\) orthogonal matrix \\(P\\), a \\(d\\times d\\) orthogonal matrix \\(Q\\), and \\(D_1\\) a diagonal \\(r\\times r\\) matrix whose diagonal terms are in decreasing order such that: \\[ A = P\n\\begin{pmatrix} D_1 & 0 \\cr\n                0 & 0 \\cr\n    \\end{pmatrix} Q^\\intercal\\]\nFor a nice and visual course on SVD see the Steve Brunton Youbube Channel on this subject!\n\nExamplesRemarksSVD in RSVD in Python\n\n\n\\(A = \\frac{\\sqrt{2}}{6} \\begin{pmatrix}\n0 & 4 \\cr\n6 & 2 \\cr\n-3 & -5\n\\end{pmatrix}\\) then consider\n\\(P = \\frac{1}{3} \\begin{pmatrix}\n1 & -2 & 2 \\cr\n2 & 2 & 1 \\cr\n-2 & 1 & 2\n\\end{pmatrix}, \\quad D = \\begin{pmatrix} 2 & 0 \\cr\n  0 & -1 \\cr\n  0 & 0\n\\end{pmatrix}, \\quad and \\ Q = \\frac{\\sqrt{2}}{2} \\begin{pmatrix} 1 & -1 \\cr\n  1 & 1\n\\end{pmatrix}\\)\n\n\nif \\(A\\) is a \\(\\^{d\\timesd}\\) symmetric matrix\n\\[A = P D P^\\intercal.\\]\n\n\n\nA =matrix(c(0, sqrt(2), - sqrt(2)/2, 2* sqrt(2)/3, sqrt(2)/3, -5*sqrt(2)/6), ncol = 2);A\n\n           [,1]       [,2]\n[1,]  0.0000000  0.9428090\n[2,]  1.4142136  0.4714045\n[3,] -0.7071068 -1.1785113\n\nsvd(A)\n\n$d\n[1] 2 1\n\n$u\n           [,1]       [,2]\n[1,] -0.3333333  0.6666667\n[2,] -0.6666667 -0.6666667\n[3,]  0.6666667 -0.3333333\n\n$v\n           [,1]       [,2]\n[1,] -0.7071068 -0.7071068\n[2,] -0.7071068  0.7071068\n\n\n\n\n\n\narray([[ 0.        ,  0.94280904],\n       [ 1.41421356,  0.47140452],\n       [-0.70710678, -1.1785113 ]])"
  },
  {
    "objectID": "linearalgebra4DS.html#and-now",
    "href": "linearalgebra4DS.html#and-now",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "And now",
    "text": "And now\nIt’s time to use this concept for Data Analysis"
  },
  {
    "objectID": "linearalgebra4DS.html#bibliography",
    "href": "linearalgebra4DS.html#bibliography",
    "title": "Linear Algebra Survival Kit For Data Science",
    "section": "Bibliography",
    "text": "Bibliography\nBanerjee, S. and A. Roy (2014). Linear algebra and matrix analysis for statistics. Vol. 181. Crc Press Boca Raton.\nCohen, M. X. (2022). Practical linear algebra for data science. O’Reilly Media, Inc.\nGentle, J. E. (2007). “Matrix algebra”. In: Springer texts in statistics, Springer, New York, NY, doi 10, pp. 978-0.\nHarville, D. A. (1998). Matrix algebra from a statistician’s perspective.\nPetersen, K. B., M. S. Pedersen, and others (2008). “The matrix cookbook”. In: Technical University of Denmark 7.15, p. 510.\nYoshida, R. (2021). Linear algebra and its applications with R. Chapman and Hall/CRC. URL: https://shainarace.github.io/LinearAlgebra/."
  },
  {
    "objectID": "introduction.html#courses-objective",
    "href": "introduction.html#courses-objective",
    "title": "Topics in Statistics - Presentation",
    "section": "Courses Objective",
    "text": "Courses Objective\n\nReactivate Basic Linear algebra useful in data Science\nProvide basic Data Science tools for representing and analyzing multivariate quantitative Dataset\nImplementing simple R analysis (codes in Python also provided)"
  },
  {
    "objectID": "introduction.html#organization",
    "href": "introduction.html#organization",
    "title": "Topics in Statistics - Presentation",
    "section": "Organization",
    "text": "Organization\n\nLecture 1: Linear algebra Survival Kit\nLecture 2: Principal Component Analysis\nLecture 3: Regression"
  },
  {
    "objectID": "introduction.html#planning",
    "href": "introduction.html#planning",
    "title": "Topics in Statistics - Presentation",
    "section": "Planning",
    "text": "Planning\n\nMonday, September 15\nTuesday, September 16\nMonday, September 22\nWednesday, September 24"
  },
  {
    "objectID": "introduction.html#material",
    "href": "introduction.html#material",
    "title": "Topics in Statistics - Presentation",
    "section": "Material",
    "text": "Material\n\nLecture and lab material are available on the course website\nThe corresponding Github repository is available from the website by clicking on the Github icon"
  },
  {
    "objectID": "pca_labs.html",
    "href": "pca_labs.html",
    "title": "Principal Component Analysis – Labs",
    "section": "",
    "text": "The dataset used in this lab comes from the website It contains ecological footprint and biocapacity results for 184 countries_comp.\n\n\nMore details can be found here \n\nGlobal hectare (gha): It is the unit chosen to express all quantities of interest regarding Carbon consumption/emission. A unit of area corresponding to the average productivity of one hectare of world land. An hectare of farmland will be worth more global hectares than an hectare of desert.\nEcological footprint (in gha): The number of gha required to produce the needs and absorb the waste of a country.\nBiocapacity (in gha): Biocapacity is given in gha per year, per country, and by land type. It translates the (actual) surface area of different land types in a country into gha, considering the climate, agricultural techniques, etc.\nnumber_of_earths_required (one value per country): The number of Earths needed if everyone lived like the people in the given country (number of gha per person in the country / 1.583 (number of gha per inhabitant of the Earth, this figure is updated every year))\nnumber_of_country_required (one per country): The number of countries_comp required for the country to be self-sufficient (e.g., Japan has 0.6 gha per inhabitant, uses 4.6 gha per year, so 4.6/0.6 Japan per year, Japan consumes 4.6/0.6 times what it is able to produce)\n\n\n\n\nWe can divide the variables into three groups:\n\nThe socio-economic data of the country: “country” (the country), “data_quality”, “sdgi” (Sustainable Development Goals Index: an index indicating how close each country is to achieving sustainable development goals), “life_expectancy”, “hdi” (Human Development Index), “per_capita_gdp”, “region”, “income_group”, “pop”.\n\nThe other two categories are given in gha by land type:\n\ncropland (food and fiber for human consumption, livestock feed, oilseeds, and rubber)\ngrazing (for livestock)\nforest (for wood and timber)\nfish (for fish, counted based on the maximum sustainable yield for various fish species, then translated into product quantities)\nbuiltup (built-up area)\ntotal: represents the total ecological footprint.\nProductions (ending in _prod) They represent in gha the production of a country for each type of land, summing what is harvested and what is wasted per year. (For forest, for instance, it’s the area required to reproduce all the wood harvested.) (for cropland, grazing, forest, fish, built_up, carbon, total)\nBiocapacities (ending in _bioc) They translate the actual surface area of different land types in a country into gha, considering the climate and agricultural techniques.\nConsumptions by land type (ending in _cons) They represent in gha the consumption of a country (i.e., the number of gha it produces minus the number it exports plus the number it imports). These data are available by land type because different imported and exported products are linked to different land types.\n\nRemarks: Built-up areas have the same values for cons, prod, and biocapacity. It is considered that all built-up areas are used within the country. To avoid considering the same variable three times, we will only keep one."
  },
  {
    "objectID": "pca_labs.html#data-description",
    "href": "pca_labs.html#data-description",
    "title": "Principal Component Analysis – Labs",
    "section": "",
    "text": "The dataset used in this lab comes from the website It contains ecological footprint and biocapacity results for 184 countries_comp.\n\n\nMore details can be found here \n\nGlobal hectare (gha): It is the unit chosen to express all quantities of interest regarding Carbon consumption/emission. A unit of area corresponding to the average productivity of one hectare of world land. An hectare of farmland will be worth more global hectares than an hectare of desert.\nEcological footprint (in gha): The number of gha required to produce the needs and absorb the waste of a country.\nBiocapacity (in gha): Biocapacity is given in gha per year, per country, and by land type. It translates the (actual) surface area of different land types in a country into gha, considering the climate, agricultural techniques, etc.\nnumber_of_earths_required (one value per country): The number of Earths needed if everyone lived like the people in the given country (number of gha per person in the country / 1.583 (number of gha per inhabitant of the Earth, this figure is updated every year))\nnumber_of_country_required (one per country): The number of countries_comp required for the country to be self-sufficient (e.g., Japan has 0.6 gha per inhabitant, uses 4.6 gha per year, so 4.6/0.6 Japan per year, Japan consumes 4.6/0.6 times what it is able to produce)\n\n\n\n\nWe can divide the variables into three groups:\n\nThe socio-economic data of the country: “country” (the country), “data_quality”, “sdgi” (Sustainable Development Goals Index: an index indicating how close each country is to achieving sustainable development goals), “life_expectancy”, “hdi” (Human Development Index), “per_capita_gdp”, “region”, “income_group”, “pop”.\n\nThe other two categories are given in gha by land type:\n\ncropland (food and fiber for human consumption, livestock feed, oilseeds, and rubber)\ngrazing (for livestock)\nforest (for wood and timber)\nfish (for fish, counted based on the maximum sustainable yield for various fish species, then translated into product quantities)\nbuiltup (built-up area)\ntotal: represents the total ecological footprint.\nProductions (ending in _prod) They represent in gha the production of a country for each type of land, summing what is harvested and what is wasted per year. (For forest, for instance, it’s the area required to reproduce all the wood harvested.) (for cropland, grazing, forest, fish, built_up, carbon, total)\nBiocapacities (ending in _bioc) They translate the actual surface area of different land types in a country into gha, considering the climate and agricultural techniques.\nConsumptions by land type (ending in _cons) They represent in gha the consumption of a country (i.e., the number of gha it produces minus the number it exports plus the number it imports). These data are available by land type because different imported and exported products are linked to different land types.\n\nRemarks: Built-up areas have the same values for cons, prod, and biocapacity. It is considered that all built-up areas are used within the country. To avoid considering the same variable three times, we will only keep one."
  },
  {
    "objectID": "pca_labs.html#import-data",
    "href": "pca_labs.html#import-data",
    "title": "Principal Component Analysis – Labs",
    "section": "Import data",
    "text": "Import data\nThe data have been prepared to be easy more easy to manipulate. The preparation script is available on the Github repository and is named footprint_prep.qmd. The resulting datasets are available as a.RData object data/countries_comp.RData and data/countries.RData. The first one is a simplified version focusing on the socio economic variables.\n\nlibrary(tidyverse)\nlibrary(readxl)\ntheme_set(theme_minimal())\nload(\"data/countries_comp.RData\")\nload(\"data/countries.RData\")\n#Identify countries by their names\npal &lt;- wes_palette(8, name = \"Zissou1\", type = \"continuous\") # nice color based on wesanderson moovie"
  },
  {
    "objectID": "pca_labs.html#descriptive-analysis",
    "href": "pca_labs.html#descriptive-analysis",
    "title": "Principal Component Analysis – Labs",
    "section": "Descriptive analysis",
    "text": "Descriptive analysis\n\nConduct a brief descriptive study of the different variables\n\n\nsummary(countries)\n\n data_quality            sdgi       life_expectancy      hdi        \n Length:184         Min.   :38.30   Min.   :52.80   Min.   :0.3910  \n Class :character   1st Qu.:58.75   1st Qu.:67.05   1st Qu.:0.5820  \n Mode  :character   Median :68.30   Median :73.82   Median :0.7350  \n                    Mean   :66.69   Mean   :72.29   Mean   :0.7146  \n                    3rd Qu.:74.20   3rd Qu.:77.44   3rd Qu.:0.8230  \n                    Max.   :85.90   Max.   :84.21   Max.   :0.9560  \n                    NA's   :21      NA's   :8       NA's   :7       \n per_capita_gdp        region          income_group            pop           \n Min.   :   210.8   Length:184         Length:184         Min.   :   0.0716  \n 1st Qu.:  1756.2   Class :character   Class :character   1st Qu.:   2.7964  \n Median :  5467.9   Mode  :character   Mode  :character   Median :   9.6092  \n Mean   : 13839.4                                         Mean   :  41.4348  \n 3rd Qu.: 15881.8                                         3rd Qu.:  29.5638  \n Max.   :111380.6                                         Max.   :1459.3800  \n NA's   :12                                                                  \n cropland_prod       grazing_prod     forest_product_prod   fish_prod       \n Min.   :0.000846   Min.   :0.00000   Min.   :0.000492    Min.   :0.000000  \n 1st Qu.:0.154880   1st Qu.:0.02598   1st Qu.:0.091821    1st Qu.:0.003108  \n Median :0.332667   Median :0.09402   Median :0.224672    Median :0.026386  \n Mean   :0.475706   Mean   :0.26175   Mean   :0.543812    Mean   :0.184808  \n 3rd Qu.:0.568109   3rd Qu.:0.20968   3rd Qu.:0.482257    3rd Qu.:0.116898  \n Max.   :2.751370   Max.   :5.44232   Max.   :8.136008    Max.   :5.518731  \n NA's   :22         NA's   :22        NA's   :22          NA's   :22        \n  carbon_prod          total_prod      cropland_cons      grazing_cons    \n Min.   : 0.009275   Min.   : 0.3001   Min.   :0.08915   Min.   :0.00000  \n 1st Qu.: 0.215931   1st Qu.: 1.2149   1st Qu.:0.37924   1st Qu.:0.07877  \n Median : 0.832324   Median : 1.9104   Median :0.53177   Median :0.17775  \n Mean   : 1.506820   Mean   : 2.9263   Mean   :0.54153   Mean   :0.25956  \n 3rd Qu.: 2.066430   3rd Qu.: 3.7680   3rd Qu.:0.68909   3rd Qu.:0.31387  \n Max.   :11.810378   Max.   :12.0559   Max.   :1.49325   Max.   :4.44132  \n NA's   :22          NA's   :1         NA's   :22        NA's   :22       \n forest_product_cons   fish_cons         carbon_cons         total_cons     \n Min.   :0.02045     Min.   :0.000756   Min.   : 0.03487   Min.   : 0.5085  \n 1st Qu.:0.16786     1st Qu.:0.025297   1st Qu.: 0.32282   1st Qu.: 1.5073  \n Median :0.25760     Median :0.068960   Median : 1.16135   Median : 2.3916  \n Mean   :0.42907     Mean   :0.149919   Mean   : 1.73370   Mean   : 3.1477  \n 3rd Qu.:0.49035     3rd Qu.:0.141522   3rd Qu.: 2.57061   3rd Qu.: 4.3699  \n Max.   :4.03395     Max.   :5.144393   Max.   :12.82295   Max.   :14.2710  \n NA's   :22          NA's   :22         NA's   :22         NA's   :1        \n cropland_bioc      grazing_land_bioc forest_land_bioc   fishing_ground_bioc\n Min.   :0.000846   Min.   :0.00000   Min.   : 0.00000   Min.   : 0.00000   \n 1st Qu.:0.154880   1st Qu.:0.04866   1st Qu.: 0.07533   1st Qu.: 0.01944   \n Median :0.332667   Median :0.13673   Median : 0.30881   Median : 0.10781   \n Mean   :0.475706   Mean   :0.41283   Mean   : 2.44214   Mean   : 0.59577   \n 3rd Qu.:0.568109   3rd Qu.:0.37282   3rd Qu.: 1.29848   3rd Qu.: 0.33874   \n Max.   :2.751370   Max.   :7.22182   Max.   :78.24552   Max.   :13.73367   \n NA's   :22         NA's   :22        NA's   :22         NA's   :22         \n    builtup_       total_biocapacity   ecological      \n Min.   :0.00000   Min.   : 0.0567   Min.   :-13.3506  \n 1st Qu.:0.03735   1st Qu.: 0.6328   1st Qu.: -2.0697  \n Median :0.05612   Median : 1.2998   Median : -0.7640  \n Mean   :0.06658   Mean   : 3.6791   Mean   :  0.5313  \n 3rd Qu.:0.08554   3rd Qu.: 2.7252   3rd Qu.:  0.2094  \n Max.   :0.22554   Max.   :92.1399   Max.   : 90.2615  \n NA's   :22        NA's   :1         NA's   :1         \n number_of_earths_required number_of_countries_required\n Min.   :0.3213            Min.   :  0.02039           \n 1st Qu.:0.9524            1st Qu.:  0.89307           \n Median :1.5112            Median :  1.83882           \n Mean   :1.9889            Mean   :  3.73551           \n 3rd Qu.:2.7612            3rd Qu.:  3.08943           \n Max.   :9.0173            Max.   :104.61627           \n NA's   :1                 NA's   :1                   \n   overshoot                      overshoot_day  \n Min.   :2018-02-10 11:28:07.17   Min.   : 40.0  \n 1st Qu.:2018-04-27 15:37:18.96   1st Qu.:131.5  \n Median :2018-06-17 10:29:52.94   Median :241.0  \n Mean   :2018-07-09 11:40:43.92   Mean   :238.0  \n 3rd Qu.:2018-09-12 11:15:33.48   3rd Qu.:365.0  \n Max.   :2018-12-26 08:45:50.03   Max.   :365.0  \n NA's   :52                       NA's   :1      \n\n\na. Represent, by world region, the overshoot day, and then the development indices (hdi, sdgi)\n\ncountries_comp %&gt;% \n  ggplot(aes(y = overshoot_day, fill = region, x = region)) + geom_boxplot() + \n  scale_fill_manual(values=pal) + \n  geom_point(alpha=0.5, position = \"jitter\") + coord_flip()\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\ncountries_comp %&gt;% \n  select(overshoot_day,  hdi, region, total_biocapacity) %&gt;% \n  pivot_longer(cols = -\"region\", names_to = \"type\", values_to = \"value\") %&gt;% \n  mutate(type = factor(type, c(\"overshoot_day\", \"sdgi\", \"hdi\", \"total_biocapacity\"))) %&gt;% \n  ggplot(aes(y = value, fill = region, x = region)) + \n  geom_boxplot() +  geom_point(alpha = 0.5, position = \"jitter\")  +\n  facet_grid(type ~ ., scale = \"free_y\") + scale_fill_manual(values = pal) +  scale_color_manual(values = pal) +\n  theme(axis.text.x = element_blank())\n\nWarning: Removed 9 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nRepresent the overshoot day according to the human development index, then the number of countries_comp, then the number of Earths required per country. Make a representation allowing a visual comparison of world regions. (You can display certain countries_comp using ggrepel::geom_label_repel(aes(label = country))|)\n\n\ncountries_comp %&gt;% \n  ggplot(aes(x = hdi, y = overshoot_day, color = region)) + \n  geom_point() + \n  scale_colour_manual(values = pal) # + ggrepel::geom_label_repel(aes(label = country))\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n##earths\ncountries_comp %&gt;% \n  ggplot(aes(x = hdi, y = number_of_earths_required, color = region)) + \n  geom_point() + \n  scale_colour_manual(values = pal) # + ggrepel::geom_label_repel(aes(label = country))\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\ncountries_comp %&gt;% \n  ggplot(aes(x = hdi, y = number_of_earths_required, color = region)) + \n  geom_point() + \n  scale_colour_manual(values = pal) + \n  #ggrepel::geom_label_repel(aes(label = country)) + \n  scale_y_log10()\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n## countries_comp \ncountries_comp %&gt;% \n  ggplot(aes(x = hdi, y = number_of_countries_required, color = region)) + \n  geom_point() + \n  scale_colour_manual(values = pal) # + ggrepel::geom_label_repel(aes(label = country))\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\ncountries_comp %&gt;% \n  ggplot(aes(x = hdi, y = number_of_countries_required, color = region)) + \n  geom_point() + scale_y_log10() +\n  scale_colour_manual(values = pal) # + ggrepel::geom_label_repel(aes(label = country))\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "pca_labs.html#principal-component-analysis",
    "href": "pca_labs.html#principal-component-analysis",
    "title": "Principal Component Analysis – Labs",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nPerform a PCA on the countries_comp.\n\n\n#|echo: true\n#|eval: false\n\n# library(Factoshiny)\n# PCAshiny(countries_comp)\n# res.PCA&lt;-PCA(countries_comp,quali.sup=c(4,5),graph=FALSE)\n# plot.PCA(res.PCA,choix='var')\n# plot.PCA(res.PCA)\n\n\nDiscuss the quality of the representation on the first principal plan.\nHow many axis should you consider?\nWhich variables are well represented?\nCould you discuss the link between total_cons and overshoot_day, pop and overshoot_day, number_of_countries_required and overshot_day.\nCite the countries which contribute the most to the different axis.\nIf you choose to consider overshoot_day as a supplementary variable, are you confident to be able to predict it with other variables ?"
  },
  {
    "objectID": "pca_labs.html#principal-component-analysis-1",
    "href": "pca_labs.html#principal-component-analysis-1",
    "title": "Principal Component Analysis – Labs",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nExplore the whole dataset countries. Remember that PCA is, at first, a solution to visualize the data and you are free to use it as you want."
  }
]