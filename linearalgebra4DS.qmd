---
title: Linear Algebra Survival Kit For Data Science
author: Marie-Pierre Etienne
date: "Last updated on `r format(Sys.time(), '%d %B, %Y')`"
institute: https://marieetienne.github.io/TopicsInStatistics/
execute: 
  freeze: false
editor: 
  markdown: 
    wrap: 72
css: mpe_pres_revealjs.css
format:
  revealjs: 
    theme: [default, custom.scss]
    width: 1050
    margin: 0.05
    slide-number: true
    show-slide-number: print
    menu:
      useTextContentForMissingTitles: false
---

```{r setup, include=FALSE, eval = TRUE}
with_sol <- TRUE ## in order to control the output
with_course <- TRUE
library(RefManageR)
library(tidyverse) ## to benefit from the tydiverse coding system
library(reticulate) ## to use python from R
library(wesanderson)
library(plotly)
```

```{r reference,  include=FALSE, cache=FALSE, eval = TRUE}
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "alphabetic",
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)
myBib <- ReadBib("./topics.bib", check = FALSE)
theme_set(theme_minimal())
options(ggplot2.discrete.colour=   scale_color_manual(values = wesanderson::wes_palette(name = "Darjeeling1")) )
couleur <-  wesanderson::wes_palette(name = "Darjeeling1")
```

::: hidden
$$
\newcommand\R{{\mathbb{R}}}
\newcommand\norm[1]{\lVert#1\rVert}
$$
\definecolor{wongBlue}{RGB}{0, 114, 178}

:::

## References on Linear Algebra for Data Scientist.


:::: {.columns}

::: {.column width="45%" }

### For a general overview

[![](img/51GMICIEVcL._AC_UF1000,1000_QL80_.jpg){width="10%"}](https://marieetienne.github.io/TopicsInStatistics/img/51GMICIEVcL._AC_UF1000,1000_QL80_.jpg) `r Cite(myBib,c('gentle2007matrix'))` 

[![](img/978-0-387-22677-4.jpeg){width="15%"} ](https://marieetienne.github.io/TopicsInStatistics/img/978-0-387-22677-4.jpeg) `r Cite(myBib,c('harville1998matrix'))`

[![](img/linear_algebra_and_matrix.jpeg){width="15%"} ](https://marieetienne.github.io/TopicsInStatistics/img/linear_algebra_and_matrix.jpeg) `r Cite(myBib,c('banerjee2014linear'))` 


:::

::: {.column width="45%" }

### With R or Python applications

[![](img/linear_algebra_R.jpg){width="15%"}](https://marieetienne.github.io/TopicsInStatistics/img/linear_algebra_R.jpg) `r Cite(myBib,c('yoshida2021linear'))` 

[![](img/practical_python.jpg){width="15%"}](https://marieetienne.github.io/TopicsInStatistics/img/practical_python.jpg) `r Cite(myBib,c('cohen2022practical'))`


### Matrix computations reference book

[![](img/matrix_cookbook.jpg){width="15%"}](https://marieetienne.github.io/TopicsInStatistics/img/matrix_cookbook.jpg) `r Cite(myBib,c('petersen2008matrix'))` 

:::

::::


## Linear Algebra : What and why {.smaller}


:::: {.callout-note icon="false"}
### Definition

Linear algebra is the branch of mathematics concerning linear equations
such as $a_1 x_1 + \cdots  a_n x_n = b,$ and linear maps such as
$(x_{1},\ldots ,x_{n})\mapsto a_{1}x_{1}+\cdots +a_{n}x_{n},$ and their
representations in vector spaces and through matrices.
::::

### Link between Data science and Linear Algebra

Data are organized in rectangular array, understood as matrices, and the
mathematical concept associated with matrices are used to manipulate
data.


## Palmer penguins illustration

```{r}
#| echo: true
## install.packages('palmerpenguins")
library("palmerpenguins")
penguins_nona <- na.omit(penguins) ## remove_na for now
penguins_nona |> print(n=3)
```


```{r}
#| echo: false
penguins_dta <- penguins_nona |> dplyr::select(where(is.numeric)) 
d <- ncol(penguins_dta)
n <- nrow(penguins_dta)
```
 
- One row describes one specific penguin, 
- One column corresponds to one specific attribute of the penguin. 

Focusing on quantitative variables, we get rectangular array with n=`r n` rows, and
d=`r sum(sapply(penguins_nona[1,], is.numeric))` columns.

Each penguin might be seen as a vector in $\mathbb{R}^d$ and
each variable might be seen as a vector in $\mathbb{R}^n$. The whole
data set is a matrix of dimension $(n,d)$.


## Linear Algebra : What and why {.smaller}

### Regression Analysis 
- to *predict*  one variable, *the response
variable* named $Y$, given the others, *the regressors*

- or equivalently to *identify a linear combination of the regressors* which provide a good
approximation of the responsbe variable, 

- or equivalently to specify *parameters* $\theta,$ so that the *prediction* $\hat{Y} = X \theta,$  linear combination of the regressors which is
as close as possible from $Y$.

We are dealing with linear equations, [enter the world of Linear Algebra]{.rouge}.


### Principal Component Analysis

- to explore relationship between variables, 

- or equivalently to quantify proximity between vectors,

- but also to *small set set* of new variables (vectors) which represent most of the initial variables, 

We are dealing with vectors, vector basis, [enter the world of Linear Algebra]{.rouge}.


# Linear Algebra Basic objects


## Vectors

$x\in \R^n$ is a vector of $n$ components.

By convention, $x$ is a **column vector**
$$x=\begin{pmatrix} x_1 \cr \vdots\cr x_n \end{pmatrix}.$$

When a row vector is needed we use operator $\ ^\intercal,$

$$x^\intercal = \begin{pmatrix} x_1, \cdots,  x_n \end{pmatrix}.$$
The null vector $\mathbb{0}\in\R^d$,

$$\mathbb{0}^\intercal = \begin{pmatrix} 0, \cdots 0\end{pmatrix}.$$


## Matrix

A matrix $A\in \R^{n\times d}$ has $n$ rows and $d$ columns:

$$A = \begin{pmatrix} 
a_{11} & a_{12} & \cdots & a_{1d} \cr
a_{21} & a_{22} & \cdots & a_{2d} \cr
\vdots & \vdots &        & \vdots \cr
a_{n1} & a_{n2} & \cdots & a_{nd} 
\end{pmatrix}$$

The term on row $i$ and column $j$ is refered to as $a_{ij}$,



:::: {.columns}

::: {.column width="35%" }
The column $j$ is refer to as $A_{.,j}$,
$A_{.,j} = \begin{pmatrix} a_{1j} \cr \vdots\cr a_{nj} \end{pmatrix}.$
:::

::: {.column width="55%"}
To respect the column vector convention, the row $i$ is refer to as $A_{i,.}^\intercal$,
$A_{i,.}^\intercal = \begin{pmatrix} a_{i1}, & \cdots &  , a_{id} \end{pmatrix}$
:::

::::



# Basic operations

## Simple operations

:::: {.columns}

::: {.column width="40%" }

**Multiplication by a scalar**

let's $\lambda\in\R$,

$$\lambda x = \begin{pmatrix} \lambda x_1 \cr \vdots\cr \lambda x_n \end{pmatrix}.$$

$$\lambda A = \begin{pmatrix} 
\lambda  a_{11} & \lambda a_{12} & \cdots & \lambda  a_{1d} \cr
\lambda  a_{21} & \lambda  a_{22} & \cdots & \lambda a_{2d} \cr
\vdots & \vdots &        & \vdots \cr
\lambda a_{n1} & \lambda a_{n2} & \cdots & \lambda a_{nd} 
\end{pmatrix}$$
:::

::: {.column width="45%"}

**Sum**

If $x\in \R^n$ and $y\in R^n$, then $x+y$ exists

$$x+ y =  \begin{pmatrix} x_1 + y_1 \cr \vdots\cr x_n + y_n\end{pmatrix}.$$


If $A$ and $B$ are two matrices with the same dimension, $A+B$ exists

$$
A+B = \begin{pmatrix} 
a_{11} + b_{11} &  \cdots &  a_{1d} + b_{1d}\cr
\vdots &  &         \vdots \cr
a_{n1} + b_{n1}& \cdots & a_{nd} + b_{nd} 
\end{pmatrix}.
$$

:::

::::



## Visualization

Consider $x=\begin{pmatrix} 1 \cr 0 \end{pmatrix}$ and
$y= \begin{pmatrix} 0 \cr 1 \end{pmatrix},$ and $z=x+y.$

```{r}
#| echo: false
# Create a scatter plot
p1 <- ggplot() + 
  xlim(c(-1,2)) + 
  ylim(c(-0.5,2.5)) +
  geom_segment(aes(x = 0, y = 0, xend = 1, yend = 0), arrow = arrow(length = unit(0.5, "cm")), col = couleur[1]) +
  annotate( "text", label = "x", x=0.5, y=-0.1, col = couleur[1], size = 10)

p2_0 <- p1 +   geom_segment(aes(x = 0, y = 0, xend = 0, yend = 1), arrow = arrow(length = unit(0.5, "cm")), col = couleur[2], linewidth = 1.3) +
  annotate( "text", label = "y", x=-0.1, y=0.5, col = couleur[2], size = 10)

p2 <- p1 +
  geom_segment(aes(x = 1, y = 0, xend = 1, yend = 1), arrow = arrow(length = unit(0.5, "cm")), col = couleur[2], linewidth = 1.3) +
  annotate( "text", label = "y", x=1.3, y=0.5, col = couleur[2], size = 10) 

p3 <- p2 +
  geom_segment(aes(x = 1, y = 0, xend = 1, yend = 2), arrow = arrow(length = unit(0.5, "cm")), col = couleur[3], linewidth = 1) +
  annotate( "text", label = "2y", x=1.3, y=1.5, col = couleur[3], size = 10) 

p4 <- p3 + geom_segment(aes(x = 0, y = 0, xend = 1, yend = 2), arrow = arrow(length = unit(0.5, "cm")), col = couleur[4])  +
  annotate( "text", label = "x+2y", x=0.3, y=1.2, col = couleur[4], size = 10) 

ggpubr::ggarrange(p2, p3, p4, ncol = 3)
```




## Matrix product {.incremental}

::: {.panel-tabset} 

## Definition  

Let $A\in \R^{n \times p}$ and $B\in\R^{p \times d}$, then the product $AB$ is well defined, it is a matrix $C$ in $\R^{n\times,d}, so that 

$$C_{ij} = \sum_{k=1}^p A_{ik} B_{kj}, \quad 1\leq i\leq n, 1\leq j \leq d.$$

### Example



* $A\in \R^{3 \times 2}$, $B\in\R^{2 \times 4}$,   with $A = \begin{pmatrix} 
\color{color 
}{1} & 2 \cr
1 & 3 \cr
-1 & -1 \end{pmatrix}$ and $B =\begin{pmatrix} 
{-1} & {-2} & {0} & {1}\cr
{-2} & {1} & {1} & {0}\end{pmatrix},$

\class{rouge}{Compute A B}

## Solution {.smaller .incremental}
$A B$ exists as the number of colums in $A$ equals the number of rows in $B$.

A useful presentation:
$$\begin{matrix}
& & \begin{pmatrix} 
\class{bleuf}{-1} & \class{bleu}{-2} & \class{vert}{0} & \class{clair}{1}\cr
\class{bleuf}{-2} & \class{bleu}{1} & \class{vert}{1} & \class{clair}{0}\end{pmatrix}   \cr
 & \begin{pmatrix} 
\class{jaune}{1} & \class{jaune}{2} \cr
\class{orange}{1} & \class{orange}{3} \cr
\class{rouge}{-1} & \class{rouge}{-1} \end{pmatrix} &
\begin{pmatrix} 
\class{jaune}{\bullet} \class{bleuf}{\bullet}   & \class{jaune}{\bullet} \class{bleu}{\bullet}  & \class{jaune}{\bullet} \class{vert}{\bullet} & \class{jaune}{\bullet} \class{clair}{\bullet} \cr
\class{orange}{\bullet} \class{bleuf}{\bullet}   & \class{orange}{\bullet} \class{bleu}{\bullet}  & \class{orange}{\bullet} \class{vert}{\bullet} & \class{orange}{\bullet} \class{clair}{\bullet} \cr
\class{rouge}{\bullet} \class{bleuf}{\bullet}   & \class{rouge}{\bullet} \class{bleu}{\bullet}  & \class{rouge}{\bullet} \class{vert}{\bullet} & \class{rouge}{\bullet} \class{clair}{\bullet} \cr
\end{pmatrix}, 
\end{matrix}
$$
*
$$AB =   \begin{pmatrix} 
\class{jaune}{1}\times\class{bleuf}{(-1)} +\class{jaune}{2}\times \class{bleuf}{(-2)}    & \class{jaune}{1}\times \class{bleu}{(-2)} + \class{jaune}{2} \times\class{bleu}{1}  & \class{jaune}{1}\times \class{vert}{0} + \class{jaune}{2}\times \class{vert}{1}  & \class{jaune}{1} \times\class{clair}{1} + \class{jaune}{2}\times \class{clair}{0} \cr
\class{orange}{1} \times \class{bleuf}{(-1)}  +  \class{orange}{3} \times \class{bleuf}{(-2)}   & \class{orange}{1} \times  \class{bleu}{(-2)} +  \class{orange}{3} \times  \class{bleu}{1} & \class{orange}{1} \times \class{vert}{0} + \class{orange}{3} \times \class{vert}{1}  & \class{orange}{1} \times\class{clair}{1} + \class{orange}{3}\times \class{clair}{0}  \cr
\class{rouge}{(-1)} \times \class{bleuf}{(-1)}  +  \class{rouge}{(-1)} \times \class{bleuf}{(-2)}   & \class{rouge}{(-1)} \times  \class{bleu}{(-2)} +  \class{rouge}{(-1)} \times  \class{bleu}{1} & \class{rouge}{(-1)} \times \class{vert}{0} + \class{rouge}{(-1)} \times \class{vert}{1}  & \class{rouge}{(-1)} \times\class{clair}{1} + \class{rouge}{(-1)}\times \class{clair}{0}  \cr
\end{pmatrix},$$

$$AB = \begin{pmatrix} 
-5   & 0  & 2  & 1 \cr
-7   & 1 & 3 & 1  \cr
3  & 1 &-1  & -1  \cr
\end{pmatrix}. $$


## R

A matrix is defined column by column
```{r}
#| echo: true
A = matrix(c(1,1,-1,2,3,-1), ncol = 2)
B = matrix(c(-1,-2, -2,1, 0, 1, 1, 0), ncol = 4)
```

The default operation $A*B$ is not the matrix product but a product elment by element, the matrix product is obtained by

```{r}
#| echo: true
A %*% B
```

## Python
Matrix product is available through the ` numpy` library 
```{python}
#| echo: true

import numpy as np

A = np.array([[ 1, 2], [ 1, 3], [ -1, 1]]); A.view()
B = np.array([[-1, -2, 0, 1], [ -2, 1, 1, 0]]); B.view()
C = A.dot(B);  C.view()
```
:::


## Matrix vector product {.incremental}

As a vector $x$ in $\R^n$ is also a matrix in $\R^{n\times 1}$, The product $A x$ exist is the number of columns in $_A$ equals the number of row for $x$.


::: {.panel-tabset}

## Example

A = \begin{pmatrix}
1 & -1 \cr
1 & 0 \cr
-1 & 0.5
\end{pmatrix},

Let $x\in \R^2$, $Ax$ exists  

$$Ax = \begin{pmatrix}
x_1 - x_2 \cr
x_1 \cr
-x_1 + 0.5 x
\end{pmatrix},$$

## Exercises

Let $x\in\R^2$, $y\in\R^3$ and 
$$A = \begin{pmatrix}
1 & -1 & 0 \cr
1 & 0 & -0.5 \cr
 -1 & 0.5 & 2
\end{pmatrix}, B = \begin{pmatrix}
1 & 1 & 0 \cr
-1 & 0 & -0.5
\end{pmatrix},\quad  C = \begin{pmatrix}
1 & -1  \cr
-1 & 2  \cr
 1 & -0.5 
\end{pmatrix},$$

When possible, compute the following quantites

$$ Ax; \quad Ay; \quad AB; \quad BA; \quad AC; \quad CA; \quad Bx; \quad Cx. $$
:::



## Transposition

The **transpose**, denoted by $\ ^\intercal$, of a matrix results from switching the rows and columns. Let $A \in \R^{n\times d}$,
$$A^{\intercal} \in \R^{d\times n},\  and (A^{\intercal})_{ij}= A_{ji}.$$


$$
A =  \begin{pmatrix} 
\class{jaune}{\bullet} & \class{jaune}{\bullet} \cr
\class{orange}{\bullet} & \class{orange}{\bullet} \cr
\class{rouge}{\bullet} & \class{rouge}{\bullet} \cr
\end{pmatrix}, \quad A^{\intercal} = \begin{pmatrix} 
\class{jaune}{\bullet} & \class{orange}{\bullet} & \class{rouge}{\bullet}\cr
\class{jaune}{\bullet} & \class{orange}{\bullet} & \class{rouge}{\bullet} 
\end{pmatrix}
$$

::: {.panel-tabset}

## Illustration

### Example

$$A = \begin{pmatrix}
1 & -1 \cr
1 & 0 \cr
-1 & 0.5
\end{pmatrix}, \quad A^\intercal =  \begin{pmatrix}
1 &  1 & -1 \cr
-1 & 0  & 0.5
\end{pmatrix},
$$

### Properties 
* $(A^\intercal)^\intercal = A,$
* $(AB)^\intercal = (B)^\intercal (A)^\intercal,$
* $(A+B)^\intercal = (A)^\intercal + (B)^\intercal.$


## Exercise

$$A = \begin{pmatrix}
1 & -1 & 0 \cr
1 & 0 & -0.5 \cr
 -1 & 0.5 & 2
\end{pmatrix}, B = \begin{pmatrix}
1 & 1 & 0 \cr
-1 & 0 & -0.5
\end{pmatrix}, C=\begin{pmatrix}
1 & 2 \cr
2 & 1
\end{pmatrix}, x \in \R^2, y\in\R^3.$$

Compute $(C)^\intercal, (BA)^\intercal, x^\intercal B, (Ay)^\intercal, y^\intercal A^\intercal$
:::

## Dot product  and norm 

 
Let $x,y$ be in $(\R^n)^2,$ the **dot product**, also known as inner product or scalar product is defined as
$$x\cdot y = y\cdot x = \sum_{i=1}^n x_i y_i = x^\intercal y= y^\intercal x.$$
The **norm** of a vector $x$, symbolized with $\norm{x},$ is defined by 
$$\norm{x}^2 =\sum_{i=1}^n x_i^2 = x^\intercal x.$$

::: {.panel-tabset}

## Example

$$x=\begin{pmatrix}
1\cr
0
\end{pmatrix},  y=\begin{pmatrix}
0\cr
1
\end{pmatrix},  z=\begin{pmatrix}
1\cr
2
\end{pmatrix} $$

$$x\cdot y = 0, x\cdot z = 1, y \cdot z = 2, \quad \norm{x}=\norm{y}=1, \quad \norm{z} = \sqrt{5}$$

## Geometrical Properties

:::: {.columns}

Let $(x,y)\in(\R^n)^2$,

::: {.column width="30%" }

$x\cdot y = \norm{x} \norm{y} \cos(\theta)$

:::

::: {.column width="60%"}
 
```{r geom_def_dotproduct}
#|echo: false

# Définir les vecteurs
vecteur1 <- c(3, 2)  # Par exemple (3, 2)
vecteur2 <- c(1, 4)  # Par exemple (1, 4)

# Créer un data frame pour ggplot
data <- data.frame(
  x = c(0, 0, vecteur1[1], vecteur2[1]),
  y = c(0, 0, vecteur1[2], vecteur2[2]),
  group = c(1, 2, 1, 2)
)

# Calcul de l'angle en radians
angle_radians <- acos(sum(vecteur1 * vecteur2) / (sqrt(sum(vecteur1^2)) * sqrt(sum(vecteur2^2))))

# Convertir l'angle en degrés
angle_degrees <- angle_radians * 180 / pi

# Créer le graphique
p1 <- ggplot() +
  geom_segment(aes(x= c(0,0), y = c(0,0), xend = c(vecteur1[1], vecteur2[1]), yend = c(vecteur1[2], vecteur2[2])), 
               arrow = arrow(type = "closed", length = unit(0.2, "inches"))) +
  geom_point(aes(x = 0, y = 0), size = 3) +
  annotate("text", x = 0.2 * (vecteur1[1] + vecteur2[1]), y = 0.15 * (vecteur1[2] + vecteur2[2]),
                label = expression(theta), vjust = -1, col = couleur[1]) +
  xlim(-1, 4) +
  ylim(-1, 4)  +
  coord_fixed() +xlab('') + ylab('')

angle1 <-  acos(vecteur1[1] / sqrt(sum(vecteur1^2)))
angle2 <-  acos(vecteur2[1] / sqrt(sum(vecteur2^2)))
arc <- data.frame(theta=seq(angle1, angle2, length.out = 1000)) |> 
  mutate(x= cos(theta), y =sin(theta))

p1 + geom_line(data=arc, aes(x=x, y= y), col = couleur[1], size = 0.5)  

```

:::

::::


## Useful remark
:::: {.columns}


::: {.column width="30%" }

 $\cos(\theta)= \frac{x\cdot y}{\norm{x} \norm{y}}$

:::

::: {.column width="60%"}
 
```{r geom_def_dotproduct2}
#|echo: false

# Définir les vecteurs
vecteur1 <- c(3, 3)  # Par exemple (3, 2)
vecteur2 <- c(3, -3)  # Par exemple (1, 4)
vecteur3 <- c(-1, -1)  # Par exemple (1, 4)

# Créer un data frame pour ggplot
data <- data.frame(
  x = c(0, 0, vecteur1[1], vecteur2[1]),
  y = c(0, 0, vecteur1[2], vecteur2[2]),
  group = c(1, 2, 1, 2)
)

# Calcul de l'angle en radians
angle_radians <- acos(sum(vecteur1 * vecteur2) / (sqrt(sum(vecteur1^2)) * sqrt(sum(vecteur2^2))))

# Convertir l'angle en degrés
angle_degrees <- angle_radians * 180 / pi

# Créer le graphique
p1 <- ggplot() +
  geom_segment(aes(x= c(0,0,0), y = c(0,0, 0), xend = c(vecteur1[1], vecteur2[1], vecteur3[1]), yend = c(vecteur1[2], vecteur2[2], vecteur3[2])), col = couleur[1:3], 
               arrow = arrow(type = "closed", length = unit(0.2, "inches"))) +
  geom_point(aes(x = 0, y = 0), size = 3)  +
  coord_fixed() +xlab('') + ylab('')

p1
```

:::

::::

:::

# Link with Linear applications

## Vector space


:::: {.callout-note icon="false"}
### Definition
 A real vector space $(V, +, . \R)$ is a set $V$ equipped with two operations satisfybing the following properties for all $(x,y,z)\in V^3$ and  all $(\lambda,\mu)\in\R^2:$
 
 * *(close for addition)* $x + y \in V,$
 * *(close for scalar multiplaication)* $\lambda x  \in V,$
 * *(communtativity)*  $x + y = y + x,$
 * *(associativity of +)* $(x + y) + z  = x + (y + z),$
 * *(Null element for +)* $x + \mathbb{0} = x,$
 * *(Existence of additive inverse +)* There exists $-x \in V$, such that $ + (-x) = \mathbb{0},$
 * *(Associativity of scalar multiplication)* $\lambda (\mu x)   = (\lambda \mu ) x,$
 * *(Distributivity of scalar sums)* $(\lambda + \mu ) x   = \lambda x + \mu x,$
 * *(Distributivity of vector sums)* $\lambda (x + y )   = \lambda x + \lambda y,$
 * *(Scalar multiplication identity)* $1 x   = x.$
::::
 
It is enough for the course to think to the set of vectors in $\R^d$.



## Linear mapping

Let $f$ be a function from $\R^d$ to $\R^n$, $f$ is linear if and only if

 For all  $(\lambda,\mu) \in \R^2$ and for all $(x,y) (\R^d)^2,$
$$f(\lambda x + \mu y ) = \lambda  f(x) + \mu f( y )$$ 

### Example

\begin{align}
f : \R^2 & \to \R\cr
x & \mapsto x_1-x_2
\end{align}

\begin{align}
f(\lambda x + \mu y) & = f \left ( 
\begin{pmatrix}
\lambda x_1 + \mu y_1\cr
\lambda x_2 + \mu y_2\cr
\end{pmatrix}
\right) =  (\lambda x_1 + \mu y_1) - (\lambda x_2 + \mu y_2)=\lambda (x_1 - x_2) + \mu (y_1 -y_2)\cr
& = \lambda f( x )  + \mu f( y )\cr 
\end{align}

$$f(x) = A x, \quad A = \begin{pmatrix}
1 & -1
\end{pmatrix}$$

## Matrix as a linear map on vector space

A real matrix $A\in\R^{n\times d}$ is a linear function operating from one real vector space of dimension $d$ to real vector space of dimension $n$. 



\begin{align}
A: &\R^d \to  \R^n\\
  & x  \mapsto  y = Ax
\end{align}

::: {.panel-tabset} 

## Examples {.smaller}

::: {.columns}

::: {.column width="40%" }

$$A = \begin{pmatrix} 
\color{color 
}{1} & 2 \cr
1 & 3 \cr
-1 & -1 \end{pmatrix}$$

\begin{align}
A: &\R^2 \to  \R^3\\
  & x  \mapsto  y = \begin{pmatrix} 
\color{color 
}{1} & 2 \cr
1 & 3 \cr
-1 & -1 \end{pmatrix} \begin{pmatrix} 
x_1\cr
x_2 \end{pmatrix} =\begin{pmatrix}
                      x_1 + 2 x_2 \cr
                      x_1 + 3 x_2 \cr
                      -x_1-x_2
                    \end{pmatrix}
                    
\end{align}

In particular $A \begin{pmatrix}1\cr 0\end{pmatrix} = A_{.,1}$ and  $A \begin{pmatrix} 0\cr 1\end{pmatrix} = A_{.,2}.$  

:::

::: {.column width="10%" .smaller}

:::

::: {.column width="40%" .smaller}

$$M = \begin{pmatrix} 
\color{color 
}{1} & -1 \cr
1 & 2 
\end{pmatrix}$$



\begin{align}
  & x  \mapsto  y = \begin{pmatrix} 
                    1 & -1 \cr
                    1 & 2 
                    \end{pmatrix} \begin{pmatrix} 
x_1\cr
x_2 \end{pmatrix} =\begin{pmatrix}
                      x_1 -  x_2 \cr
                      x_1 + 2 x_2 
                    \end{pmatrix}
\end{align}

In particular $M \begin{pmatrix}1\cr 0\end{pmatrix} = M_{.,1}$ and  $M \begin{pmatrix} 0\cr 1\end{pmatrix} = M_{.,2}.$  

:::

::::

## Visualisation 

::: {.columns}

:::: {.column width="40%" }

\begin{align}
M: &\R^2 \to  \R^2\\
  & x  \mapsto  y = \begin{pmatrix} 
  1 & -1 \cr
1 & 2 
\end{pmatrix} x 
\end{align}
is a transformation of $\R^2$

::::

:::: {.column width="40%" }
```{r visu_Ax}
#| echo: false

Mat <- matrix(c(1,1,-1,2), ncol = 2) 

# Create a scatter plot
ggplot() + 
  xlim(c(-2,2)) + 
  ylim(c(-1,3)) +
  geom_segment(aes(x = c(0,0), y = c(0,0), xend = c(1,0), yend = c(0,1)), arrow = arrow(length = unit(0.5, "cm")), col = couleur[1:2]) +
 geom_segment(aes(x = c(0,0), y = c(0,0), xend = Mat[1,], yend = Mat[2,]), arrow = arrow(length = unit(0.5, "cm")), col = couleur[1:2]) +
  annotate("text", label =  c(expression(M (1,0)^T), expression(M (0,1)^T)), x=c(1.2,-0.6), y=c(0.8, 2),  col = couleur[1:2], size = 8) +
  xlab("") + ylab("")
```


::::


:::


## Exercices 

:::: {.columns}

::::: {.column width="45%" }

### Exercice 1  {.smaller}
Let's consider the application from $\R^2$ to $\R^2$ which projects on the $x$ axis. 

* Is it a linear transformation of  $\R^2$ ?
* If so, could you write the corresponding matrix $P$ ?


:::::

::::: {.column width="40%" }
### Exercice 2 {.smaller}
Let's consider the application from $\R^2$ to $\R$ which computes the square norm of the vector $|| x||^2 = x_1^2 + x_2^2$

* Is it a linear transformation of  $\R^2$  ?
* If so, could you write the corresponding matrix $N$ ?

:::::

::::




## Solutions 


:::: {.columns}

::::: {.column width="45%" }

### Exercice 1  {.smaller}
* Projection on $x$ axis consists in forgetting the $y$ axis component. 
\begin{align}
f(\lambda_1 x + \lambda_2 z) & = f\left ( \begin{pmatrix} \lambda_1 x_1 + \lambda_2 z_1 \cr
\lambda_1 x_1 + \lambda_2 z_2\cr
\end{pmatrix} \right) \cr
& =  \begin{pmatrix} \lambda_1 x_1 + \lambda_2 z_1 \cr
0\cr
\end{pmatrix}\cr
& = \lambda_1 f(x) + \lambda_2 f(z),
\end{align}



* $f$ is linear, its matrix form $P:$

$$P = \begin{pmatrix} 1 & 0 \cr 0  & 0 \end{pmatrix},$$

:::::

::::: {.column width="40%" }

$$Px = \begin{pmatrix} 1 & 0 \cr 0  & 0 \end{pmatrix} \begin{pmatrix} x_1  \cr x_2  \end{pmatrix} =  \begin{pmatrix} x_1  \cr 0  \end{pmatrix}$$


### Exercice 2  {.smaller}
* Square Norm $f$ of  $x$ equals $x_1^2 + x_2^2,$
As $f(\lambda_1 x ) = \lambda_1^2 f( x)$, the square norm is not a linear function and could not be expressed as a matrix. 


:::::

::::

:::

## Linear independance
A sequence of vectors $x_1, \cdots, x_k$ from $\R^n$ is said to be linearly independent, if the only solution to 
$X a = \begin{pmatrix}
& & & \cr
x_1 & x_2 & \cdots & x_k \cr
& & & \cr
\end{pmatrix} \begin{pmatrix} a_1 \cr a_2 \cr \vdots \cr a_k\end{pmatrix} =\mathbb{0}$ is
$a =\mathbb {0}$.

::: {.panel-tabset} 

### Example

$x_1 = \begin{pmatrix}
1 \cr
0 \cr
0 
\end{pmatrix}, \quad x_2 = \begin{pmatrix}
1 \cr
1 \cr
0
\end{pmatrix}, \quad X=\begin{pmatrix}
1 & 1  \cr
0 & 1  \cr
0 & 0 \cr
\end{pmatrix}, \quad Xa = \begin{pmatrix}a_1 + a_2\cr
a_2 \cr
0 \end{pmatrix}.$

$x_1$ and  $x_2$ are linearly independant.

### Exercice

$x_1 = \begin{pmatrix}
1 \cr
1 \cr
1 
\end{pmatrix}, \quad x_2 = \begin{pmatrix}
1 \cr
1 \cr
0
\end{pmatrix}, x_3 = \begin{pmatrix}
0 \cr
1 \cr
1
\end{pmatrix}, x_4 = \begin{pmatrix}
0 \cr
-1 \cr
1
\end{pmatrix}$

* Are $(x_1, x_2,x_3)$ linearly independant ?
* Are $(x_1, x_2,x_4)$ linearly independant ?

:::

## Matrix rank

The sequence of columns of a matrix $M\in \R^{n\times d}$ forms a sequence of $d$ vectors $(c_1, \cdots,c_d)$ in $\R^n$, while the rows of a matrix forms a sequence of $n$ vectors $(r_1, \cdots,r_n)$ in $\R^d$. 

The **rank** of a matrix is the length of the largest sequence of linearly independant columns or equivalently the length of the largest sequence of linearly independant rows.


::: {.panel-tabset} 

### Example

* $X_1 =\begin{pmatrix}
1 & 1  \cr
0 & 1  \cr
0 & 0 \cr
\end{pmatrix}, \quad rank(X_1) = 2; \quad \quad X_2=\begin{pmatrix}
1 & 1 & 0 \cr
1 & 1 & -1\cr
1 & 0 & 1
\end{pmatrix}, \quad rank(X_2) = 3;$
* $X_3=\begin{pmatrix}
1 & 1 & -1 \cr
1 & 1 & -1\cr
1 & 0 & 1
\end{pmatrix}, \quad rank(X_3) = 2;$



### Remarks

Let $A= (a_1, \cdots, a_d)^\intercal \in \R^d$, 
$A A^\intercal = \begin{pmatrix} 
& &  \cr
a_1 A & \cdots & a_d A\cr 
& &  \cr
\end{pmatrix}$ if of rank $1$.




### Example with R

:::: {.columns}

::: {.column width="45%" }

```{r rank1}
#| echo: true
#install.packages('Matrix')
library(Matrix)
X1 <- matrix(c(1,0,0,1,1,0), ncol = 2)
X2 <- matrix(c(1,1,1,1,1,0, 0, -1,1), ncol = 3)
X3 <- matrix(c(1,1,1,1,1,0, -1, -1,1), ncol = 3)
```

:::

::: {.column width="45%" }

```{r rank2}
#| echo: true
rankMatrix(X1)[1]
rankMatrix(X2)[1]
rankMatrix(X3)[1]
```

:::

::::

### Example with Python


```{python}
#| echo: true

import numpy as np
from numpy.linalg import matrix_rank

X1 = np.array([[ 1, 1], [ 0, 1], [ 0, 0]])
X2 = np.array([[ 1, 1, 0], [ 1, 1, -1], [ 1, 0, 1]])
X3 = np.array([[ 1, 1, -1], [ 1, 1, -1], [ 1, 0, 1]])

matrix_rank(X1) 
matrix_rank(X2) 
matrix_rank(X3) 
```

:::


## Column Space, rank 

If $A\in\R^{n\times d},$ and $c\in \R^d$, 
$$A c = \begin{pmatrix}
& & &  \cr
A_{.,1} & A_{.,2} & \cdots & A_{.,d} \cr
& & &  \cr
\end{pmatrix} \begin{pmatrix}
c_1  \cr
c_2 \cr
\vdots \cr
c_d  \cr
\end{pmatrix} = \begin{pmatrix}
c_1 a_{11} + c_2 a_{12} + \cdots + c_d a_{1d} \cr
\vdots \cr
c_1 a_{n1} + c_2 a_{n2} + \cdots + c_d a_{nd} \cr
\end{pmatrix} = c_1 A_{.,1} + c_2  A_{.,2} + \cdots + c_d  A_{.,d}$$

### Image of  $A$ or Column space of A 

$Im(A) = \left \lbrace y \in \R^d; \exists c\in R^d, y = \sum_{l=1}^d c_l A{., l} \right \rbrace$ 

[Remarks]{.rouge}  

* If $A_{.,1}$ and $A_{,2}$ are linearly dependant, $A_{,1} = \lambda A_{,2}$ and $Ac = ( c_1 \lambda + c_2)  A_{.,2} + \cdots + c_d  A_{.,d}$. The rank is the minimal number of vector to generate $Im(A)$, that is the dimension of $Im(A)$. 



## Remarkable matrices

### Identity matrix 

The square matrix $I$ such $I_{ij} =\delta_{ij}$ is named the **Identity matrix** and acts as the neutral element for matrix multiplication

Let $I_n$ be the identity matrix in $\R^{n\times n}, for any $A \in \R^{m\times n}$, $A I_n = A$ and $I_m A = A$.

### Diagonal matrix 

A **Diagonal matrix** is a matrix in which elements outside the main diagonal are all zeros. The term is mostly used for square matrix but the terminology might also be used otherwise.



::: {.panel-tabset} 

### Example
Let's $e_i = (\delta_{i1}, \cdots, \delta_{id})$ and 
$D = \begin{pmatrix}2 & 0 & 0 \cr
                    0 & 1 & 0 \cr
                    0 & 0 & -1 
                    \end{pmatrix}$
$$D \class{rouge}{e_1} = 2 \class{rouge}{e_1},\quad D \class{bleu}{e_2} =  \class{bleu}{e_2},\quad D \class{orange}{e_3} = - \class{orange}{e_3}.$$

$D$ acts as simple scalar multiplier on the basis vectors.

### Geometrical point of view 


:::: {.columns}

::: {.column width="35%" }
Consider 
$D = \begin{pmatrix}2  & 0 \cr
                    0  & 0.5
                    \end{pmatrix},\quad Dx = \begin{pmatrix} 2 x_1 \cr
                      0.5 x_2
                    \end{pmatrix}.$
                    
::: 

::: {.colmun width="40%"}

```{r visu_diag}

p1 <- ggplot() + 
  xlim(c(-0.5,2)) + 
  ylim(c(-0.5,1.5)) +
  geom_segment(aes(x = c(0,0), y = c(0, 0), xend = c(1, 2), yend = c(0, 0)), arrow = arrow(length = unit(0.5, "cm")), col = couleur[1], linetype = c(2,1), linewidth = c(1,2)  ) +
  annotate( "text", label = c("x", "Dx"), x=c(0.5, 1.3), y=c(-0.1, -0.2), col = couleur[1], size = 10) +
  geom_segment(aes(x = c(0,0), y = c(0, 0), xend = c(0, 0), yend = c(1, 0.5)), arrow = arrow(length = unit(0.5, "cm")), col = couleur[2], linetype = c(2,1), linewidth = c(1,2) ) +
  annotate( "text", label = c("y", "Dy"), x=c(-0.1, -0.2), y=c(0.5, 0.1), col = couleur[2], size = 10) +
  xlab('') + ylab('')  + coord_fixed()
p1

```

:::

::::

:::

## Orthogonal matrix


### Definition

An orthogonal matrix, $Q$ or orthonormal matrix, is a real square matrix whose columns and rows are orthonormal vectors and therefore verifies
$$Q^\intercal Q =  Q Q^\intercal = I.$$


::: {.panel-tabset} 

### Example
$Q = \begin{pmatrix}\frac{\sqrt{2}}{2} & 0 &  -\frac{\sqrt{2}}{2}\cr
                    \frac{\sqrt{2}}{2} & 0 &  \frac{\sqrt{2}}{2}\cr
                    0 & 1 & 0 
                    \end{pmatrix}$
$$Q  Q^\intercal = I_3$$

$D$ transform the vector basis in new orthonormal vectors.

### Geometrical point of view 


:::: {.columns}

::: {.column width="35%" }
Consider 
$Q = \begin{pmatrix}\frac{\sqrt{2}}{2} &   -\frac{\sqrt{2}}{2}\cr
                    \frac{\sqrt{2}}{2} &  \frac{\sqrt{2}}{2} 
                    \end{pmatrix},\quad Qx = \begin{pmatrix}  \frac{\sqrt{2}}{2}x_1 + \frac{\sqrt{2}}{2} x_2\cr
                  -\frac{\sqrt{2}}{2}x_1 + \frac{\sqrt{2}}{2} x_2
                    \end{pmatrix}.$
                    
::: 

::: {.colmun width="40%"}

```{r visu_ortho}

p1 <- ggplot() + 
 xlim(c(-1.5,1.5)) + 
ylim(c(-0.5,1.5)) +
  geom_segment(aes(x = c(0,0), y = c(0, 0), xend = c(1, sqrt(2)/2), yend = c(0, sqrt(2)/2)), arrow = arrow(length = unit(0.5, "cm")), col = couleur[1], linetype = c(2,1), linewidth = c(1,2)  ) +
  annotate( "text", label = c("x", "Qx"), x=c(0.5, 1.3), y=c(-0.1, -0.2), col = couleur[1], size = 10) +
  geom_segment(aes(x = c(0,0), y = c(0, 0), xend = c(0, -sqrt(2)/2), yend = c(1, sqrt(2)/2)), arrow = arrow(length = unit(0.5, "cm")), col = couleur[2], linetype = c(2,1), linewidth = c(1,2)  ) +
  annotate( "text", label = c("y", "Qy"), x=c(-0.1, -0.2), y=c(0.5, 0.1), col = couleur[2], size = 10) +
  xlab('') + ylab('')  + coord_fixed()
p1

```

:::

::::

:::


## Eigen values, Eigen vectors

Consider a square matrix, $A \in \R^{d\times d}$, and a nonzero vector, $v \in \R^d$, 
$v$ is an **eigen vector** for the **eigen value** $\lambda$ if 
$$Av = \lambda v.$$
If $v$ is an eigen vector for $A$, applying $A$ to $v$ just consists in multiplying by the corresponding eigen value.

::: {.panel-tabset} 

### Geometric interpretation

```{r visu_eigen}
#| fig-width: 3
#| fig-height: 3

p1 <- ggplot() + 
  xlim(c(-0.5, 3)) + 
  ylim(c(-0.5, 3)) +
  geom_segment(aes(x = c(0,0), y = c(0, 0), xend = c(0.5, 1.5), yend = c(1, 3)), arrow = arrow(length = unit(0.5, "cm")), col = couleur[1], linetype = c(2,1), linewidth = c(1,2), alpha = 0.5  ) +
  annotate( "text", label = c("v", "Av"), x=c(0.5, 1.3), y=c(0.4, 1.4), col = couleur[1], size = 10)  + coord_fixed() +
  xlab('') + ylab('')
p1
```

### Exercises

* Identify the eigen values and eigen vectors for $A_1 = \begin{pmatrix}1 & 0 \cr 0 & 2\end{pmatrix}$,  $A_2 = \begin{pmatrix}1 & 3 \cr 0 & 2\end{pmatrix},$  $A_3 = \begin{pmatrix} 2 & 2 \cr -2 & 2 \end{pmatrix}.$ 

:::


## Singular Value Decomposition (SVD)

Let's $A$ a $n\times d$ matrix of rank $r$, there exists a  $n\times n$ orthogonal matrix $P$,  a $d\times d$ orthogonal matrix $Q$, and $D_1$  a diagonal $r\times r$ matrix whose diagonal terms are in decreasing order such that: 
$$ A = P 
\begin{pmatrix} D_1 & 0 \cr
                0 & 0 \cr
    \end{pmatrix} Q^\intercal$$

For a nice and visual course on SVD see the [Steve Brunton](https://www.youtube.com/playlist?list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv) Youbube Channel on this subject!

::: {.panel-tabset} 

### Examples

$A = \frac{\sqrt{2}}{6} \begin{pmatrix}
0 & 4 \cr
6 & 2 \cr
-3 & -5
\end{pmatrix}$ then consider


$P = \frac{1}{3} \begin{pmatrix}
1 & -2 & 2 \cr
2 & 2 & 1 \cr
-2 & 1 & 2
\end{pmatrix}, \quad D = \begin{pmatrix} 2 & 0 \cr
  0 & -1 \cr 
  0 & 0
\end{pmatrix}, \quad and \ Q = \frac{\sqrt{2}}{2} \begin{pmatrix} 1 & -1 \cr
  1 & 1
\end{pmatrix}$



### Remarks

if $A$ is a $\^{d\timesd}$ symmetric matrix

$$A = P D P^\intercal.$$


### SVD in R 

```{r svd_r}
#| echo: true
A =matrix(c(0, sqrt(2), - sqrt(2)/2, 2* sqrt(2)/3, sqrt(2)/3, -5*sqrt(2)/6), ncol = 2);A
svd(A)
```

### SVD in Python


```{python svd_py}
from scipy.linalg import svd
import math

A = math.sqrt(2)/6 * np.array([[ 0, 4], [ 6, 2], [ -3, -5]]); A.view()
P, d, QT = svd(A)
```
:::



## And now

[It's time to use this concept for Data Analysis]{.rouge}


## Bibliography 


```{r refs, echo=FALSE, results="asis", eval = TRUE, cache = FALSE}
PrintBibliography(myBib)
```

