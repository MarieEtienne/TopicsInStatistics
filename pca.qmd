---
title: Principal Component Analysis
author: Marie-Pierre Etienne
date: "Last updated on `r format(Sys.time(), '%d %B, %Y')`"
institute: https://marieetienne.github.io/TopicsInStatistics/
execute: 
  freeze: false
editor: 
  markdown: 
    wrap: 72
css: mpe_pres_revealjs.css
format:
  revealjs: 
    theme: [default, custom.scss]
    width: 1050
    margin: 0.05
    slide-number: true
    smaller: true
    show-slide-number: print
    menu:
      useTextContentForMissingTitles: false
---


```{r setup, include=FALSE, eval = TRUE}
library(RefManageR)
library(tidyverse) ## to benefit from the tydiverse coding system
library(reticulate) ## to use python from R
library(wesanderson)
library(plotly)
```

```{r reference,  include=FALSE, cache=FALSE, eval = TRUE}
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "alphabetic",
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)
myBib <- ReadBib("./topics.bib", check = FALSE)
theme_set(theme_minimal())
options(ggplot2.discrete.colour=   scale_color_manual(values = wesanderson::wes_palette(name = "Darjeeling1")) )
couleur <-  wesanderson::wes_palette(name = "Darjeeling1")
```


::: hidden
$$
\newcommand\R{{\mathbb{R}}}
\newcommand\norm[1]{\lVert#1\rVert}
$$
\definecolor{wongBlue}{RGB}{0, 114, 178}

:::


```{r datapackage, eval = TRUE, echo = FALSE, warning = FALSE}
library(plotly)
ggplot <- function(...) ggplot2::ggplot(...) + scale_fill_manual(values = wesanderson::wes_palette(name = "Darjeeling1")) + scale_color_manual(values = wesanderson::wes_palette(name = "Darjeeling1")) +  scale_fill_manual(values = wesanderson::wes_palette(name = "Darjeeling1")) + theme_minimal()
#remotes::install_github('MarieEtienne/coursesdata', force = TRUE)
doubs.env <- read.csv ('https://raw.githubusercontent.com/zdealveindy/anadat-r/master/data/DoubsEnv.csv', row.names = 1) %>% as_tibble()

data(penguins, package = 'palmerpenguins')
penguins <- penguins %>% na.omit()

```
# Introduction



## The Example of Doubs River Characteristics

We measured the physico-chemical characteristics at 30 different sites along the [Doubs River](https://en.wikipedia.org/wiki/Doubs_(river)) .

:::: {.columns}

::: {.column width="53%" }

The first 4 rows (out of 30) of the `doubs.env` dataset
```{r extrait_doubs, echo = FALSE, eval = TRUE}
doubs.env %>% print(n=4)
```

[How can we best visualize these data to reveal the relationships between variables and identify similarities between sites?]{.question} 

:::

::: {.column width="45%" .smaller}

* das: distance to the source ($km$),
* alt: altitude ($m$),
* pen: slope (elevation change per 1000m),
* deb: flow rate ($m^3.s^{-1}$),
* pH: water pH,
* dur: calcium concentration ($mg.L^{-1}$),
* pho: phosphate concentration ($mg.L^{-1}$),
* nit: nitrate concentration ($mg.L^{-1}$),
* amn: ammonium concentration ($mg.L^{-1}$),
* oxy: dissolved oxygen concentration ($mg.L^{-1}$),
* dbo: Biological Oxygen Demand ($mg.L^{-1}$).

:::

::::




## The Example of Penguin Morphology

We measured the morphological characteristics of various penguins:

:::: {.columns}

::: {.column width="53%" }

The first 6 rows (out of 333) of the `penguins` dataset


```{r extrait_penguins}
#| echo: false
#| eval: true
 penguins %>% 
 select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%  print(n=6)
```

[How can we best visualize these data to reveal the relationships between variables and identify similarities between individuals?]{.question}

:::

::: {.column width="45%" }

* bill_length_mm: bill length,
* bill_depth_mm: bill depth,
* flipper_length_mm: flipper length,
* body_mass_g: body mass.

:::

::::

#  Introduction


## Formalization

* For each individual $i$, we measured $p$ different variables.
* For each variable $k$,  we measured $n$ individuals.


The data are arranged in a table with $n$ rows and $p$ columns.

:::: {.columns}

::: {.column width="45%"}


```{r tab_pres, eval =TRUE, out.width = "80%"}
knitr::include_graphics('img/acp_data_pres.png')
```

:::

::: {.column width="45%"}
We denote $x_{ik}$ as the value measured for variable $k$ on individual $i$,

and

* $x_{\bullet k} = \frac{1}{n} \sum_{i=1}^n x_{ik}$ as the mean value of variable $k$,
* $s_k  = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_{ik}-x_{\bullet k})^2}$ as the standard deviation of variable $k$.

:::

::::

## The Same Question in Various Fields

* Sensory analysis: score of descriptor $k$ for product $i$
* Economics: value of indicator $k$ for year $i$
* Genomics: gene expression $k$ for patient/sample $i$
* Marketing: satisfaction index $k$ for brand $i$
  
etc...

We have $p$ variables measured on $n$ individuals, and we want to visualize these data to understand the relationships between variables and the proximity between individuals.



## Seeing is Understanding: How to Represent the Information Contained in This Table?


### Idea 1: Represent the relationships between variables 2 by 2

```{r ggpairs, eval = TRUE, echo = FALSE, fig.show='asis', out.height = "80%", out.width = "80%"}
penguins %>% select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>% GGally::ggpairs()
```


## Seeing is Understanding: How to Represent the Information Contained in This Table?

### Idea 1: Represent the relationships between variables 2 by 2

```{r gg_prog, eval = TRUE, echo = FALSE, fig.show='asis', out.height = "80%", out.width = "80%"}
penguins  %>% ggplot() + aes(x= body_mass_g, y = flipper_length_mm)  + geom_point() + xlab("Body Mass") + ylab("Flipper Length")
```



We lose information on the other axes.



## Seeing is Understanding: How to Represent the Information Contained in This Table?

### Idea 1: Represent the relationships between variables 2 by 2


```{r gen_data,eval = TRUE, out.width = "100%"}
scene = list(camera = list(eye = list(x = -2, y = 0, z = 0)),
             center = list(x= mean(penguins$bill_depth_mm, y = mean(penguins$body_mass_g), z = mean(penguins$flipper_length_mm) )))
fig <- plot_ly(penguins, x = ~ bill_depth_mm, y = ~ -body_mass_g, z = ~flipper_length_mm, width = 500, height = 500) %>% 
  add_markers(size = 12)  %>% 
  layout(title = "3D Scatter plot", scene = scene)
fig
```


## Seeing is Understanding: How to Represent the Information Contained in This Table?

[Objective:]{.rouge}

* Represent without losing too much information
* Ideally, individuals far apart in the initial cloud remain far apart in the representation.



[What We Need:]{.rouge}

* Quantify the information loss in the representation
* Build the representation that loses the least amount of information



[Precautions:]{.rouge}

* Potentially, Make variables expressed in different units comparable

# Distance and Inertia

## Distance between individuals

Let $X_{i,.}^\intercal \in \R^d$ be the descriptions of individual $i$. To quantify the distance between indivuals we might used the Euclidian distance in $\R^d,$

$$d(i, j)^2 = \sum_{k=1}^d (x_{ik} - x_{jk})^2 = \norm{X_{i,.}^\intercal - X_{i,.}^\intercal}^2 = \left({X_{i,.}^\intercal - X_{i,.}^\intercal}\right)^\intercal  \left({X_{i,.}^\intercal - X_{i,.}^\intercal}\right). $$

This could be [misleading when dealing with different variables with different scale]{.rouge}. 

### Example in the penguins dataset

Body mass varies form `r min(penguins$body_mass_g)` to `r max(penguins$body_mass_g)`, while flipper length varies from `r min(penguins$flipper_length_mm)` to `r max(penguins$flipper_length_mm)`. 

Variable Body mass will contribute more to the distance between individuals.

## Specify a different metric


:::: {.callout-note icon="false"}
### Definition
A metric in $\R^d$, $M$ is a definite positive symmetric $\R^{d\times d}$ matrix which can be used to define distance between individuals


$$d_M(i, j)^2  = \norm{X_{i,.}^\intercal - X_{i,.}^\intercal}^2_M = \left({X_{i,.}^\intercal - X_{i,.}^\intercal}\right)^\intercal  M \left({X_{i,.}^\intercal - X_{i,.}^\intercal}\right).$$
::::

:::: {.columns}

::: {.column width="45%"}

* If $M=I_d$, 

$d_M$ is the classical Euclidian distance

* Get rid of the units used for the variables and attribute the same weight to all variables by choosing


$M = \begin{pmatrix}
\frac{1}{s_1^2} & &0 & \cr
 & \frac{1}{s_2^2}  & & \cr
 &0 & \ddots & \cr
&&& \frac{1}{s_d^2} \cr
\end{pmatrix} = D_{1/s^2}$ 

:::

::: {.column width="45%"}

with $s_k^2=\sum_{i=1}^n (x_{ik} - x_{.k})^2,$ $x_{.k} =\frac{1}{n}\sum_{i=1}^n x_{ik}$


:::: {.callout-note icon="false"}
### Remarks

The distance defined with $D_{1/s^2}$ is the same than the distance defined on centred and scaled variables with the identity matrix.
::::

In the following, we will assume that $X$ is the matrix of centred and scaled variables.

:::

::::


## Dispersion measure: Inertia with respect to a point 


:::: {.callout-note icon="false"}
### Definition
Inertia (denomination derived from moments of inertia in Physics) *with respect to a point $a \in R^{d},$* according to metric $M$:
$$I_a = \sum_{i=1}^n \norm{X_{i,.}^\intercal - a}_M^2$$

::::

* Inertia around the centro√Ød $G$ (center of gravity of the cloud) plays a central role in Principal Component Analysis:

$G = (x_{.,1}, \cdots, x_{.,d})^\intercal$ and $I_G = min_{a\in \R^d} I_a$


###  Remarks
Assume, we deal with centred scaled variables:

$I_G = \sum_{i=1}^n \norm{X_{i,.}^\intercal - G}^2 = \sum_{i=1}^n \class{rouge}{\sum_{k=1}^d} (x_{ik} - x_{.,k})^2 = \class{rouge}{\sum_{k=1}^d} \sum_{i=1}^n  (x_{ik} - x_{.,k})^2.$

As the variables are scaled, $\sum_{i=1}^n  (x_{ik} - x_{.,k})^2 = n s^2_k = n.$ and $I_G=nd.$


[Total inertia with scaled centred variables is $nd$]{.rouge}




## Dispersion measure: Inertia with respect to a affine subspace 

:::: {.callout-note icon="false"}
### Definition
Inertia *with respect to an affine subspace $S$* according to metric $M$: $I_S = \sum_{i=1}^n d_M(X_{i,.}, S)^2$

::::


Huygens theorem states that if $S^G$ stands for  the affine subspace containing $G$ and parallel to $S$ then
$$I_S = I_{S^G} + d_M^2(a, G),$$
where $a$ is the orthogonal projection of $G$ on $S$.

[The affine subspace $S$ which minimizes inertia is $S^G$.]{.rouge}





## Inertia Decomposition

Since, variables are centred $G=\mathbb{0}$,  $I=\sum_{i=1}^n d(X_{i,.},0)^2.$

Let $S$ be an affine subspace and $U=S^\intercal,$ $X^S_{i,.}$  (recip. $X^S_{i,.})$) the orthogonal projection on $S$ (recip. on $U$).

As $d(X_{i,.},0)^2 = d(X^S_{i,.},0)^2 + d(X^U_{i,.},0)^2$, $I=I_S + I_{S^\intercal}$

:::: {.columns}

::: {.column width="55%" }

```{r}

df <- data.frame(x=2, y= 2)
f <- data.frame(x=1,y=2)
proj <- as.matrix(df) %*% t(as.matrix(f)) / as.matrix(f) %*% t(as.matrix(f)) |> as.numeric()
df_proj <- proj*f
df_projorth <- df - df_proj


ggplot() +
  geom_point(data = df, aes(x=x, y=y), col = couleur[1]) +
  geom_abline(intercept  = 0,  slope = 2) +
  geom_abline(intercept  = 0,  slope = -0.5, col = couleur[2]) +
  xlim(c(-2, 3)) +
  ylim(c(-2, 4)) +
  geom_point(data = df_proj, aes(x=x, y=y), col = couleur[1], size= 2) +
  coord_fixed() +
  geom_segment(aes(x=df$x[1], xend = df_proj$x[1], y=df$y[1], yend = df_proj$y[1] ), col = couleur[1], linetype = 2) +
  geom_segment(aes(x=df$x[1], xend = 0, y=df$y[1], yend = 0 ), col = couleur[1], linetype = 2) +
  geom_segment(aes(x=df$x[1], xend = df_projorth$x[1], y=df$y[1], yend = df_projorth$y[1] ), col = couleur[2], linetype = 2) +
  annotate( "text", label = expression(X[1,.]), x=2, y=1.7, col = couleur[1], size = 8) +
  annotate( "text", label = "G", x=-0.4, y=-0.3, col = couleur[1], size = 8)  +
  annotate( "text", label = "S", x=2.5, y=3.9, size = 8)  +
  annotate( "text", label = "U", x=2.5, y=-1, size = 8)  
  
```
:::


::: {.column width="40%" }

### Interpretation

$I_S$ is the dispersion of the dataset **lost by projection on $S$**, 

while $I_{S^\intercal}$ is the **dispersion of the dataset projected on $S$**.



### PCA  

Identifying $(U_1, U_d)$ a sequence of orthogonal unitary vectors such that $I_{U_1}\leq I_{U_2}\leq \cdots \leq I_{U_d}$.

The projection on $U_1$ is the best projection of the dataset in one dimension, $U_1$ define the first Principal Component.


:::

::::


## Inertia: useful representation

Let $x_i= X_{i,.}^\intercal$, (recall that $X_{i,}$ is a row vector and $x_i$ is the corresponding column vector).

\begin{align}
I & = \sum_{i}^n d^2(X_{i,.}, 0) = \sum_{i}^n \norm{X_{i,.}}\\
& = \sum_{i}^n x_i^\intercal x_i \cr
& = tr(\sum_{i}^n x_i^\intercal x_i) \cr
& = tr(\sum_{i}^n x_i x_i^\intercal ) \cr
& = tr(X^\intercal  X) \cr
\end{align}



:::: {.columns}

::: {.column width="45%" }

### Remarks 

* $X^\intercal  X$ is the covariance matrix of the $d$ variables,

* $X^\intercal  X$ is a symmetric $\R^{d\times d}$ matrix, and the corresponding SVD

$$X^\intercal  X = \left ( P D Q^\intercal \right )^\intercal \left ( P D Q^\intercal \right ) =  Q D  P^\intercal  P D Q^\intercal = Q D^2 Q^\intercal.$$

:::

::: {.column width="45%" }

* $I= tr(X^\intercal X) = tr(Q D^2 Q^\intercal) = tr(Q^\intercal Q D  ) = tr( D ) = \sum_{k=1}^d \sigma^2_k,$  
where $\sigma^2_k$ stands for the k$^{th}$ eigen value.

:::

::::


# Principal Components Construction 

## Identify $\boldsymbol{u_1}$ - Formalization

:::::: columns
::: {.column width="45%"}
-   We seek $\boldsymbol{u_1}$ such that $I_{u_1}$ is minimal but
    $I_{u_1} + I_{u_1^\perp}=I$. [Minimizing $I_{u_1}$ is equivalent to
    maximizing $I_{u_1^\perp}$]{.rouge}, i.e. the inertia of the
    projected cloud.

    -   The first individual is projected as
        $\class{alea}{x_{1}}^\top \boldsymbol{u_1}$ onto
        $\boldsymbol{u_1}$
    -   The second individual is projected as
        $\class{alea}{x_{2}}^\top \boldsymbol{u_1}$ onto
        $\boldsymbol{u_1}$
    -   ...

-   Recall that
    $${\boldsymbol{X}}= \begin{pmatrix} \class{alea}{x_{1}}^\top \\ \vdots \\ \class{alea}{x_{n}}^\top \end{pmatrix}.$$
    The projection of individual $i$ is obtained by multiplying the row
    vector $\class{alea}{x_{i}}^\top$ by $\boldsymbol{u_1}$. The vector
    of coordinates on $\boldsymbol{u_1}$ for each individual is

$$\begin{pmatrix} \class{alea}{x_{1}}^\top \boldsymbol{u_1} \\ \vdots \\ \class{alea}{x_{n}}^\top \boldsymbol{u_1} \end{pmatrix} = X  \boldsymbol{u_1}.$$
:::

:::: {.column width="45%"}
### Maximize $I_{u_1^\perp}$

under the constraint $\lVert\boldsymbol{u_1}\rVert$ \begin{align}
 I_{u_1^\perp} & = \frac{1}{n}\lVert X  \boldsymbol{u_1}\rVert^2 \\
        & = \frac{1}{n} (X  \boldsymbol{u_1})^\top (X  \boldsymbol{u_1})\\
        & =   \boldsymbol{u_1}^\top \left (\frac{1}{n} X^\top X \right)  \boldsymbol{u_1}
\end{align}

::: panel-tabset
#### Remark

-   $\frac{1}{n} X^\top X$ is the covariance matrix, often written as
    $X^\top W X$ ($W$ is a diagonal matrix, with each diagonal entry
    equal to $1/n$, $W_{ii}$ is the weight assigned to observation $i$).

-   $X^\top W X$ is [symmetric]{.rouge} and [positive definite]{.rouge},
    therefore [diagonalizable]{.rouge}.

-   There exists a unitary matrix $P$ with $P^\top P = I_p,$ and a
    diagonal matrix $D$ such that $$X^\top W X = P D P^\top.$$

-   The diagonal entries of $D$ are the [eigenvalues, all
    positive,]{.rouge} of $X^\top W X$, arranged in decreasing order,
    $\lambda_1 > \lambda_2 \ldots >\lambda_p$, and $U$ is the matrix of
    the associated eigenvectors.

#### What this means

![](change_baseXtX.png)

$\boldsymbol{b}$ is an orthonormal basis, the basis formed by the
eigenvectors of $X^\top W X$.
:::
::::
::::::

[This is generally the implemented version.]{.rouge}






:::

::: {.column width="45%" }


### SVD of $X^\intercal X$ instead of $X$

As we are mainly interesting in $Q$, $d\times d$ matrix (not $P$ the $n\times n$ matrix) and the square of the eigen values, it is more efficient to consider the SVD of $\Sigma$ directly.



### Geometrically

* The matrix $Q^\intercal$ transform the original canonical basis $(e_1, \cdots, e_d)$ of $\R^d$ in the new ACP basis $(v_1, \cdots, v_d)$; .

* In the new basis variable $k$, 

$$ \sum_{j=1}^d \sqrt{\sigma_j^2} q_{kj} v_j = \begin{pmatrix} \sigma_1 q_{1j}, \cdots, \sigma_d q_{dj} \end{pmatrix}^\intercal$$

:::

::::

## Palmer Penguins example

```{r key_ingredients}
#| echo: true

## the core scale function normalizes by n-1 instead of n
scale2 <- function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm)) / ( sqrt((length(x)-1) / length(x)) *sd(x, na.rm) )

X <- penguins |> 
  mutate(year = as.factor(year))|>  ## year willnot be considered as numerical variable
  select(where(is.numeric)) |>      ## select all numeric columns
  mutate_all(list(scale2))          ## and scale them

n <- nrow(X) # number of individuals
d <- ncol(X)

X_mat <- X |>  as.matrix()          ## the data point matrix 
X_norm_mat <- 1/sqrt(n) * X_mat     ## the version considered to get rid of the number of individuals

X_mat_trace <- sum( diag(t(X_mat)%*% X_mat) )  # n d 
X_norm_mat_trace <- sum( diag( t( X_norm_mat)%*% X_norm_mat ) )  # d

penguins_svd <- svd( t(X_mat)%*% X_mat )
penguins_norm_svd <- svd( t(X_norm_mat)%*% X_norm_mat )

## eigenvalues
penguins_eigenvalue <- penguins_svd$d
sum(penguins_eigenvalue)
penguins_norm_eigenvalue <- penguins_norm_svd$d
sum(penguins_norm_eigenvalue)
```



## Palmer Penguins example


```{r representation}
#| echo: true
#| output-location: column
#| results: hold

penguins_Q <- penguins_svd$u
penguins_norm_Q <- penguins_norm_svd$u

## From orginal variable to new basis
## t(Q) %*% t(X[1,]) for 1st individual
## t(Q) %*% t(X) for all individuals, or in order to keep individual in lines
## X %*% Q 

coord_newbasis <- X_mat %*% (penguins_Q) 
coord_norm_newbasis <- X_norm_mat %*% (penguins_Q) 

coord_newbasis <- coord_newbasis |> 
  as_tibble() |> 
  rename(Dim1 = V1, Dim2 = V2, Dim3 = V3, Dim4 = V4)

ggplot(coord_newbasis) + aes(x= Dim1, y = Dim2) + geom_point()
#ggplot(coord_newbasis) + aes(x= Dim1, y = Dim2) + geom_point()
dist(X[1:5,])
dist(coord_newbasis[1:5,])
dist(coord_newbasis[1:5,])
dist(coord_newbasis[1:5,1:2])

#dist(coord_norm_newbasis[1:5,]) * sqrt(n)
X_C <- as.matrix(coord_newbasis)
```


## Global quality of the representation 

Let $C_{12}$ designs the plan defined by $C_1$ and $C_2$ and let $X^{C}$ be the coordinates of the individuals in the new basis.

The information preserved by projection is 
\begin{align}
I_{C_{12}^\perp} & = \sum_{i=1}^n \left ( \left(x^{C}_{i1)}\right)^2 + \left(x^{C}_{i2}\right)^2 \right) \cr
                  & = tr\left( \left(X^{C}_{,1:2}\right)^\intercal X^{C}_{,1:2}\right)\cr
                  & =  \lambda_1^2 + \lambda^2_2\cr
                  & = n (\sigma_1^2 + \sigma^2_2)\cr
\end{align}

```{r overall_penguins quality}
#| echo: true
#| results: hold

cat("On plan 12: ", sum(penguins_eigenvalue[1:2]), ".\n")
cat("Working with the correlation matrix (/n), sum of eigenvalues is ", sum(penguins_norm_eigenvalue[1:2]), ". \n This has to be multiply by the size of the dataset to get inertia:", sum(penguins_norm_eigenvalue[1:2]) * n, "\n")

cat("This is easier to appreciate when expressed  as a propotion of total inertia:",  round(sum(penguins_norm_eigenvalue[1:2])/sum(penguins_norm_eigenvalue)*100,2),"%. \n")
```


## Representing jointly initial variables and principal components

To understand the links between original and new variables, or between original variables themselve.

```{r}
#| echo: true
#| output-location: column
#| results: hold

new_var_coord <- diag(sqrt(penguins_norm_eigenvalue))%*%t(penguins_norm_Q)

## for graphical purpose
new_var_coord_df <- t(new_var_coord) |> as_tibble() |> rename(C1=V1, C2= V2, C3 =V3, C4 = V4)
circle_df <- data.frame(theta = seq(0, 2*pi, length.out = 501)) |> 
  mutate(x = cos(theta), y  = sin(theta))

ggplot(new_var_coord_df) + geom_point(aes(x=C1, y=C2)) + coord_fixed() + xlim(c(-1,1)) + ylim(c(-1,1)) + 
  geom_segment(aes(x=rep(0,4), y = rep(0,4), xend=C1, yend = C2),  arrow = arrow(length = unit(0.5, "cm"))) +
  geom_path(data = circle_df, aes(x=x, y=y)) +
  annotate("text", label = colnames(X), x = new_var_coord_df$C1  + 0.05, y = new_var_coord_df$C2- 0.05)
  
```


## Quality fo the representation of the variables

* The quality of the projection on a component $j$ depends on the angle $\theta_{ij}$ between the original variable and $C^j$.

$$cos(\theta_{ij})^2 = \frac{(C_j^T X^i)^2}{\norm{X^i}\norm{X^i}} =  \sigma^2_j q_{ij}^2$$


* The quality of the projection on a the plan $(C^1C^2)$ depends on the angle $\theta_{i,1-2}$ between the original variable  and the projection plan.


$$ cos(\theta_{i,1-2})^2 = \sum_{j=1}^2 \frac{(C_j^T X^i)^2}{\norm{X^i}\norm{C^j}} =  \sum_{j=1}^2 \sigma^2_j  q_{ij}^2$$


```{r}
#| echo: true
#| output-location: column
#| results: hold

## quality of the representation for variable 1 on teh different axis
new_var_coord[,1]^2 

## quality of the projection on plan C1-C2
sum((new_var_coord[,1]^2 )[1:2])

```




## Visualizing individuals

* $X$ is the matrix of individuals on the original basis, $XQ$ is the matrix of individual in the new basis (no information lost)

* To visualizing the dataset, we can project on the two first dimensions of the new basis

```{r projection}
#| echo: true
#| output-location: column
#| results: hold

XQ <- X_mat %*% penguins_norm_Q

XQ_df <- XQ |> 
  as_tibble() |> 
  rename(C1 = V1, C2 = V2, C3 = V3, C4 = V4)

ggplot(XQ_df) + geom_point(aes(x=C1, y=C2)) 

```


## Interpretation

### Quality of representation of individual $i$ on the plan

* The angle between the original individual vector $i$ and component $j$:

$$cos(\theta_{ij})^2 = \frac{( X_i Q^j)^2}{\norm{X_i}^2} $$

### Contribution to the axe $C_i$

The total inertia on component $1$ is $\sigma^2_i = \sum_{i=1}^n (XQ)_{i1}^2$.

Individual $i$ contributes for $(XQ)_{i1}^2/\sigma^2_i$


## Practically speaking

It is way simpler !!!

You can have a look at the book `r Citet(myBib, "husson2011exploratory")` and visit [Fran√ßois Husson MOOC](https://husson.github.io/MOOC_GB/index.html), author of the ` FactoMineR` package.



:::: {.columns}

::: {.column width="45%" }

```{r Factominer}
#| echo: true
#| eval: false
# install.packages('FactoMineR')
# install.packages('FactoShiny')
library(FactoMineR)

penguins_pca <- PCA(X, ncp = ncol(X), graph = FALSE)

penguins_pca$eig

penguins_pca$var

penguins_pca$ind

plot(penguins_pca, axes = c(1,2), choix = "ind")

plot(penguins_pca, axes = c(3,4) , choix = "var")



```

:::

::: {.column width="45%" }


```{r shiny}
#| echo: true
#| eval: false
#| 
library(Factoshiny)
PCAshiny(penguins)
```

:::

::::

## The Doubs River example

```{r doubs_shiny}
#| echo: true
#| eval: false
#| 
PCAshiny(doubs.env)
```

